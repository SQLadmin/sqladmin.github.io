{
    "version": "https://jsonfeed.org/version/1",
    "title": "The Data Guy",
    "home_page_url": "/",
    "feed_url": "/feed.json",
    "description": "Adventures of a DBA in Cloud, Data and DevOps",
    "icon": "/apple-touch-icon.png",
    "favicon": "/favicon.ico",
    "expired": false,
    
    "author":  {
        "name": "Bhuvanesh",
        "url": null,
        "avatar": null
    },
    
"items": [
    
        {
            "id": "/2020/04/26/redshift-copy-script-from-sct-agent-export-s3-part1",
            "title": "Redshift Copy Script From Sct Agent Export S3 Part1",
            "summary": "AWS SCT Extraction Agents will help to pull the data from the various sources. Generate RedShift copy command from SCT agent exported to S3 or Snowball with random string folders.",
            "content_text": "AWS SCT Extraction Agents will help to pull the data from the various data sources and push it into the targets. S3 and Snowball also can be a target for SCT Agent. But if we run the SCT Agent with multiple tasks and multiple agents they will export the data into S3 or Snowball with some string folder structure. If you want to push it into the RedShift then it is very difficult to import from the complex random string(UUID kind of) path. Recently we had an experience with this where we have to import around 20TB+ data from Snowball(later exported to S3) to RedShift. In this blog, Im going to share my experience and script to generate RedShift copy command from SCT agent exported to S3 or Snowball with random string folders. SCT Agent Terminology:Before explaining the solution, let’s understand how the SCT agent works and its terminology.  Once we created a migration task, it’ll split the data export between all the extraction agents.  One table’s data will be exported in multiple chunks. This chunk is based on the Virtual Partition.  The Virtual partition will be considered as Subtasks for the export.So the export folder structure will be formed based on the above terminology.  Top-level folder - The S3 prefix that you are going give while creating the task.  2nd level - Extraction Agent ID (Each agent has a name as well as a unique ID).  3rd level - The subtask ID. Each virtual partition will be exported in a subtask.  4th level - The main task ID. The job you created to export, it’ll get an ID.Lets see an example  S3 Prefix - april-my-large-table/  Extraction Agent ID (you can have multiple agents, but here I’ll select the agent 1’s ID) - e03e92407fcf4ede86e1f4630409c48c  Subtask ID - 19f6ce57ec254053ab5eb72245dc2641  Task ID - 4c0233f054c840359d93bdae8c175bf8Now if you start exporting the data then it’ll use the following folder structure.s3://bucketname/s3-prefix/agent-id/subtask-id/task-id/Lets compare this with our example.s3://migration-bucket/april-my-large-table/e03e92407fcf4ede86e1f4630409c48c/19f6ce57ec254053ab5eb72245dc2641/4c0233f054c840359d93bdae8c175bf8/S3 export Terminology:We understand the folder/directory structure, now let’s see what kind of files we’ll get inside these folders.  unit directory - Your actual data in CSV format will be pushed here. You’ll get up to 8 files per unit folder. If your data size is huge then you’ll get multiple unit folder like unit_1, unit_2, unit_n.  One good thing with this SCT is it’ll generate the COPY command for the RedShift using Manifest file. So inside your unit directory if you have 8 files, then there is a file called unit.manifest which is a manifest file that contains all the 8 files full path. Along with this, you’ll get the SQL file that already refers to the manifest file.  So using this SQL we can load all the 8 CSV files.  Statistics file - This is a JSON file that contains the information about the Taskid, agent ID, source, and target table information and the where condition used to export this data(I virtual partition or chunk). This statistics file will have the name that is the same as your subtask ID.Refer to the below image which shows the complete illustration of the SCT Agent directory structure. And if you notice the subtask ID and the statistic file’s name, both are the same.Challenges:Here can notice one thing, it’s already in good shape to load the CSV files to RedShift. Just get all the SQL files and run them, then what is the challenge here?  If you extract the SQL file, there we have an option for using Access and Secret keys. But we have to manually fill these values.  I mean download all the SQL files(all are having the same name, so while downloading rename them). And edit them with the right Access and Secret key values.  One SQL command refers to one manifest file that means a maximum of 8 files is there. If you have a pretty large cluster that has more than 8 slices then the performance will be slow.  If you have a Terabyte of data for a single table, you’ll get so many SQL files. Then you have to write a script to loop all these COPY SQL files one by one which may take a longer time.Optimized Approach:We can generate a custom SQL file for COPY command that refers to all the CSV files from the multiple subfolders. So in one SQL command, we can load all of our CSV files to RedShift. So how do we get that?  Get the list of all the unit.manifest files.  Download the manifest files to your system.  Extract the CSV files location from all the manifest files. For this Im going to use jq a command-line JSON parser for Linux. So just extract the entries[] array from the unit.manifest file.  Create a new manifest file that contains all the CSV file locations that are extracted from the previous step. And upload this to the new S3 path.  Generate a SQL command that refers to the newly generated manifest.  If you have many tables instead of repeating this step for all the tables, pass the table names, and the S3 prefix in a CSV file, let the script to the complete work for you.Remember how did you export?  From SCT we can create a local task to export a single table, so the S3 prefix contains the data only for that particular table.  Or you can upload a CSV file to SCT that contains a bunch of tables list. In this case, your S3 prefix location contains all the tables files. But luckily one subtask will export a table’s data. So it’ll not mix multiple tables data in one subtask.And make sure you install jq.apt install jqyum install jqReplace the following things in my script  migration-bucket - your bucket name  your_schema - RedShift schema name  1231231231 - AWS Account ID for the RedShift IAM role  s3-role - RedShift IAM role name to access S3  ap-south-1 - s3 bucket region  merged_manifest - New s3 prefix where the new Manifest file will get uploadedScript to generate SQL for individually exported tables:Prepare a CSV file that contains the list of table names and their s3 Prefix.Eg Filename: source_generate_copytable1,apr-04-table1table2,apr-04-table2table3,apr-04-table3Create necessary foldersmkdir /root/copy-sql  mkdir /root/manifest-list  mkdir /root/manifest_filesDownload the ManifestI want to download 20 files parallelly, so I used &amp; and ((++count % 20 == 0 )) &amp;&amp; wait. If you want to download more then use a higher number. But it better to keep less than 100.count=0while read -r s_tabledotable=$(echo $s_table | awk -F',' '{print $1}')s3_loc=$(echo $s_table | awk -F',' '{print $2}')aws s3 ls --human-readable --recursive s3://migration-bucket/$s3_loc/ | grep \"unit.manifest\" | awk -F ' ' '{print $5}' &gt; /root/manifest-list/$table.manifest.list      mkdir -p /root/manifest_files/$table/file_num=0while read -r r_manifestdoaws s3 cp s3://migration-bucket/$r_manifest /root/manifest_files/$table/unit.manifest.$file_num &amp;file_num=$(( $file_num + 1 ))((++count % 20 == 0 )) &amp;&amp; waitdone &lt; /root/manifest-list/$table.manifest.listdone &lt; source_generate_copyMerge Manifest into onewhile read -r s_tabledotable=$(echo $s_table | awk -F',' '{print $1}')files=$(ls /root/manifest_files/$table)for file in $filesdoecho $filecat /root/manifest_files/$table/$file | jq '.entries[]'  &gt;&gt; /root/manifest_files/$table/unit.mergedonecat /root/manifest_files/$table/unit.merge | jq -s '' &gt; /root/manifest_files/$table/$table.manifestsed -i '1c\\{\"entries\" : ['  /root/manifest_files/$table/$table.manifestsed -i -e '$a\\}'  /root/manifest_files/$table/$table.manifestdone &lt; source_generate_copyUpload Manifestwhile read -r s_tabledotable=$(echo $s_table | awk -F',' '{print $1}')aws s3 cp /root/manifest_files/$table/$table.manifest s3://migration-bucket/merged_manifest/done &lt; source_generate_copyGenerate COPYChange the options as per your need.while read -r s_tabledotable=$(echo $s_table | awk -F',' '{print $1}')echo \"COPY your_schema.$table from 's3://migration-bucket/merged_manifest/$table.manifest' MANIFEST iam_role 'arn:aws:iam::1231231231:role/s3-role' REGION 'ap-south-1' REMOVEQUOTES IGNOREHEADER 1 ESCAPE DATEFORMAT 'auto' TIMEFORMAT 'auto' GZIP DELIMITER '|' ACCEPTINVCHARS '?' COMPUPDATE FALSE STATUPDATE FALSE MAXERROR 0 BLANKSASNULL EMPTYASNULL  EXPLICIT_IDS\"  &gt; /root/copy-sql/copy-$table.sqldone &lt; source_generate_copyAll tables COPY command will be generated and located on /root/copy-sql/ location.I have written about how to generate the SQL command for bulk exported tables in Part 2.",
            "content_html": "<p>AWS SCT Extraction Agents will help to pull the data from the various data sources and push it into the targets. S3 and Snowball also can be a target for SCT Agent. But if we run the SCT Agent with multiple tasks and multiple agents they will export the data into S3 or Snowball with some string folder structure. If you want to push it into the RedShift then it is very difficult to import from the complex random string(UUID kind of) path. Recently we had an experience with this where we have to import around 20TB+ data from Snowball(later exported to S3) to RedShift. In this blog, Im going to share my experience and script to generate RedShift copy command from SCT agent exported to S3 or Snowball with random string folders. <img src=\"/assets/RedShift Copy Command From SCT Agent Export In S3.png\" alt=\"\" /></p><h2 id=\"sct-agent-terminology\">SCT Agent Terminology:</h2><p>Before explaining the solution, let’s understand how the SCT agent works and its terminology.</p><ol>  <li>Once we created a migration task, it’ll split the data export between all the extraction agents.</li>  <li>One table’s data will be exported in multiple chunks. This chunk is based on the Virtual Partition.</li>  <li>The Virtual partition will be considered as Subtasks for the export.</li></ol><p>So the export folder structure will be formed based on the above terminology.</p><ul>  <li>Top-level folder - The S3 prefix that you are going give while creating the task.</li>  <li>2nd level - Extraction Agent ID (Each agent has a name as well as a unique ID).</li>  <li>3rd level - The subtask ID. Each virtual partition will be exported in a subtask.</li>  <li>4th level - The main task ID. The job you created to export, it’ll get an ID.</li></ul><p><strong>Lets see an example</strong></p><ul>  <li><strong>S3 Prefix</strong> - april-my-large-table/</li>  <li><strong>Extraction Agent ID</strong> (you can have multiple agents, but here I’ll select the agent 1’s ID) - e03e92407fcf4ede86e1f4630409c48c</li>  <li><strong>Subtask ID</strong> - 19f6ce57ec254053ab5eb72245dc2641</li>  <li><strong>Task ID</strong> - 4c0233f054c840359d93bdae8c175bf8</li></ul><p>Now if you start exporting the data then it’ll use the following folder structure.</p><p><code class=\"language-html highlighter-rouge\">s3://bucketname/s3-prefix/agent-id/subtask-id/task-id/</code></p><p>Lets compare this with our example.</p><p><code class=\"language-html highlighter-rouge\">s3://migration-bucket/april-my-large-table/e03e92407fcf4ede86e1f4630409c48c/19f6ce57ec254053ab5eb72245dc2641/4c0233f054c840359d93bdae8c175bf8/</code></p><h2 id=\"s3-export-terminology\">S3 export Terminology:</h2><p>We understand the folder/directory structure, now let’s see what kind of files we’ll get inside these folders.</p><ul>  <li><strong>unit directory</strong> - Your actual data in CSV format will be pushed here. You’ll get up to 8 files per unit folder. If your data size is huge then you’ll get multiple unit folder like <code class=\"language-html highlighter-rouge\">unit_1, unit_2, unit_n</code>.</li>  <li>One good thing with this SCT is it’ll generate the COPY command for the RedShift using Manifest file. So inside your unit directory if you have 8 files, then there is a file called <code class=\"language-html highlighter-rouge\">unit.manifest</code> which is a manifest file that contains all the 8 files full path. Along with this, you’ll get the SQL file that already refers to the manifest file.  So using this SQL we can load all the 8 CSV files.</li>  <li><strong>Statistics file</strong> - This is a JSON file that contains the information about the Taskid, agent ID, source, and target table information and the where condition used to export this data(I virtual partition or chunk). This statistics file will have the name that is the same as your subtask ID.</li></ul><p>Refer to the below image which shows the complete illustration of the SCT Agent directory structure. And if you notice the subtask ID and the statistic file’s name, both are the same.</p><p><img src=\"/assets/RedShift Copy Command From SCT Agent Export In S3 snap.jpg\" alt=\"RedShift Copy Command From SCT Agent Export In S3\" title=\"RedShift Copy Command From SCT Agent Export In S3\" /><img src=\"/assets/RedShift Copy Command From SCT Agent Export In S3 snap1.jpg\" alt=\"RedShift Copy Command From SCT Agent Export In S3\" title=\"RedShift Copy Command From SCT Agent Export In S3\" /></p><h2 id=\"challenges\">Challenges:</h2><p>Here can notice one thing, it’s already in good shape to load the CSV files to RedShift. Just get all the SQL files and run them, then what is the challenge here?</p><ul>  <li>If you extract the SQL file, there we have an option for using Access and Secret keys. But we have to manually fill these values.</li>  <li>I mean download all the SQL files(all are having the same name, so while downloading rename them). And edit them with the right Access and Secret key values.</li>  <li>One SQL command refers to one manifest file that means a maximum of 8 files is there. If you have a pretty large cluster that has more than 8 slices then the performance will be slow.</li>  <li>If you have a Terabyte of data for a single table, you’ll get so many SQL files. Then you have to write a script to loop all these COPY SQL files one by one which may take a longer time.</li></ul><h2 id=\"optimized-approach\">Optimized Approach:</h2><p>We can generate a custom SQL file for COPY command that refers to all the CSV files from the multiple subfolders. So in one SQL command, we can load all of our CSV files to RedShift. So how do we get that?</p><ol>  <li>Get the list of all the <code class=\"language-html highlighter-rouge\">unit.manifest</code> files.</li>  <li>Download the manifest files to your system.</li>  <li>Extract the CSV files location from all the manifest files. For this Im going to use <code class=\"language-html highlighter-rouge\">jq</code> a command-line JSON parser for Linux. So just extract the <code class=\"language-html highlighter-rouge\">entries[]</code> array from the <code class=\"language-html highlighter-rouge\">unit.manifest</code> file.</li>  <li>Create a new manifest file that contains all the CSV file locations that are extracted from the previous step. And upload this to the new S3 path.</li>  <li>Generate a SQL command that refers to the newly generated manifest.</li>  <li>If you have many tables instead of repeating this step for all the tables, pass the table names, and the S3 prefix in a CSV file, let the script to the complete work for you.</li></ol><h2 id=\"remember-how-did-you-export\">Remember how did you export?</h2><ol>  <li>From SCT we can create a local task to export a single table, so the S3 prefix contains the data only for that particular table.</li>  <li>Or you can upload a CSV file to SCT that contains a bunch of tables list. In this case, your S3 prefix location contains all the tables files. But luckily one subtask will export a table’s data. So it’ll not mix multiple tables data in one subtask.</li></ol><p>And make sure you install <code class=\"language-html highlighter-rouge\">jq</code>.</p><figure class=\"highlight\"><pre><code class=\"language-sh\" data-lang=\"sh\">apt <span class=\"nb\">install </span>jqyum <span class=\"nb\">install </span>jq</code></pre></figure><p><strong>Replace the following things in my script</strong></p><ul>  <li><strong>migration-bucket</strong> - your bucket name</li>  <li><strong>your_schema</strong> - RedShift schema name</li>  <li><strong>1231231231</strong> - AWS Account ID for the RedShift IAM role</li>  <li><strong>s3-role</strong> - RedShift IAM role name to access S3</li>  <li><strong>ap-south-1</strong> - s3 bucket region</li>  <li><strong>merged_manifest</strong> - New s3 prefix where the new Manifest file will get uploaded</li></ul><h2 id=\"script-to-generate-sql-for-individually-exported-tables\">Script to generate SQL for individually exported tables:</h2><p>Prepare a CSV file that contains the list of table names and their s3 Prefix.</p><p><strong>Eg Filename: source_generate_copy</strong></p><figure class=\"highlight\"><pre><code class=\"language-sh\" data-lang=\"sh\">table1,apr-04-table1table2,apr-04-table2table3,apr-04-table3</code></pre></figure><p><strong>Create necessary folders</strong></p><figure class=\"highlight\"><pre><code class=\"language-sh\" data-lang=\"sh\"><span class=\"nb\">mkdir</span> /root/copy-sql  <span class=\"nb\">mkdir</span> /root/manifest-list  <span class=\"nb\">mkdir</span> /root/manifest_files</code></pre></figure><p><strong>Download the Manifest</strong></p><p>I want to download 20 files parallelly, so I used <code class=\"language-html highlighter-rouge\"><span class=\"err\">&amp;</span></code> and <code class=\"language-html highlighter-rouge\">((++count % 20 == 0 )) <span class=\"err\">&amp;&amp;</span> wait</code>. If you want to download more then use a higher number. But it better to keep less than 100.</p><figure class=\"highlight\"><pre><code class=\"language-sh\" data-lang=\"sh\"><span class=\"nv\">count</span><span class=\"o\">=</span>0<span class=\"k\">while </span><span class=\"nb\">read</span> <span class=\"nt\">-r</span> s_table<span class=\"k\">do</span><span class=\"nv\">table</span><span class=\"o\">=</span><span class=\"si\">$(</span><span class=\"nb\">echo</span> <span class=\"nv\">$s_table</span> | <span class=\"nb\">awk</span> <span class=\"nt\">-F</span><span class=\"s1\">','</span> <span class=\"s1\">'{print $1}'</span><span class=\"si\">)</span><span class=\"nv\">s3_loc</span><span class=\"o\">=</span><span class=\"si\">$(</span><span class=\"nb\">echo</span> <span class=\"nv\">$s_table</span> | <span class=\"nb\">awk</span> <span class=\"nt\">-F</span><span class=\"s1\">','</span> <span class=\"s1\">'{print $2}'</span><span class=\"si\">)</span>aws s3 <span class=\"nb\">ls</span> <span class=\"nt\">--human-readable</span> <span class=\"nt\">--recursive</span> s3://migration-bucket/<span class=\"nv\">$s3_loc</span>/ | <span class=\"nb\">grep</span> <span class=\"s2\">\"unit.manifest\"</span> | <span class=\"nb\">awk</span> <span class=\"nt\">-F</span> <span class=\"s1\">' '</span> <span class=\"s1\">'{print $5}'</span> <span class=\"o\">&gt;</span> /root/manifest-list/<span class=\"nv\">$table</span>.manifest.list      <span class=\"nb\">mkdir</span> <span class=\"nt\">-p</span> /root/manifest_files/<span class=\"nv\">$table</span>/<span class=\"nv\">file_num</span><span class=\"o\">=</span>0<span class=\"k\">while </span><span class=\"nb\">read</span> <span class=\"nt\">-r</span> r_manifest<span class=\"k\">do</span>aws s3 <span class=\"nb\">cp </span>s3://migration-bucket/<span class=\"nv\">$r_manifest</span> /root/manifest_files/<span class=\"nv\">$table</span>/unit.manifest.<span class=\"nv\">$file_num</span> &amp;<span class=\"nv\">file_num</span><span class=\"o\">=</span><span class=\"k\">$((</span> <span class=\"nv\">$file_num</span> <span class=\"o\">+</span> <span class=\"m\">1</span> <span class=\"k\">))</span><span class=\"o\">((</span>++count % 20 <span class=\"o\">==</span> 0 <span class=\"o\">))</span> <span class=\"o\">&amp;&amp;</span> <span class=\"nb\">wait</span><span class=\"k\">done</span> &lt; /root/manifest-list/<span class=\"nv\">$table</span>.manifest.list<span class=\"k\">done</span> &lt; source_generate_copy</code></pre></figure><p><strong>Merge Manifest into one</strong></p><figure class=\"highlight\"><pre><code class=\"language-sh\" data-lang=\"sh\"><span class=\"k\">while </span><span class=\"nb\">read</span> <span class=\"nt\">-r</span> s_table<span class=\"k\">do</span><span class=\"nv\">table</span><span class=\"o\">=</span><span class=\"si\">$(</span><span class=\"nb\">echo</span> <span class=\"nv\">$s_table</span> | <span class=\"nb\">awk</span> <span class=\"nt\">-F</span><span class=\"s1\">','</span> <span class=\"s1\">'{print $1}'</span><span class=\"si\">)</span><span class=\"nv\">files</span><span class=\"o\">=</span><span class=\"si\">$(</span><span class=\"nb\">ls</span> /root/manifest_files/<span class=\"nv\">$table</span><span class=\"si\">)</span><span class=\"k\">for </span>file <span class=\"k\">in</span> <span class=\"nv\">$files</span><span class=\"k\">do</span><span class=\"nb\">echo</span> <span class=\"nv\">$file</span><span class=\"nb\">cat</span> /root/manifest_files/<span class=\"nv\">$table</span>/<span class=\"nv\">$file</span> | jq <span class=\"s1\">'.entries[]'</span>  <span class=\"o\">&gt;&gt;</span> /root/manifest_files/<span class=\"nv\">$table</span>/unit.merge<span class=\"k\">done</span><span class=\"nb\">cat</span> /root/manifest_files/<span class=\"nv\">$table</span>/unit.merge | jq <span class=\"nt\">-s</span> <span class=\"s1\">''</span> <span class=\"o\">&gt;</span> /root/manifest_files/<span class=\"nv\">$table</span>/<span class=\"nv\">$table</span>.manifest<span class=\"nb\">sed</span> <span class=\"nt\">-i</span> <span class=\"s1\">'1c\\{\"entries\" : ['</span>  /root/manifest_files/<span class=\"nv\">$table</span>/<span class=\"nv\">$table</span>.manifest<span class=\"nb\">sed</span> <span class=\"nt\">-i</span> <span class=\"nt\">-e</span> <span class=\"s1\">'$a\\}'</span>  /root/manifest_files/<span class=\"nv\">$table</span>/<span class=\"nv\">$table</span>.manifest<span class=\"k\">done</span> &lt; source_generate_copy</code></pre></figure><p><strong>Upload Manifest</strong></p><figure class=\"highlight\"><pre><code class=\"language-sh\" data-lang=\"sh\"><span class=\"k\">while </span><span class=\"nb\">read</span> <span class=\"nt\">-r</span> s_table<span class=\"k\">do</span><span class=\"nv\">table</span><span class=\"o\">=</span><span class=\"si\">$(</span><span class=\"nb\">echo</span> <span class=\"nv\">$s_table</span> | <span class=\"nb\">awk</span> <span class=\"nt\">-F</span><span class=\"s1\">','</span> <span class=\"s1\">'{print $1}'</span><span class=\"si\">)</span>aws s3 <span class=\"nb\">cp</span> /root/manifest_files/<span class=\"nv\">$table</span>/<span class=\"nv\">$table</span>.manifest s3://migration-bucket/merged_manifest/<span class=\"k\">done</span> &lt; source_generate_copy</code></pre></figure><p><strong>Generate COPY</strong></p><p>Change the options as per your need.</p><figure class=\"highlight\"><pre><code class=\"language-sh\" data-lang=\"sh\"><span class=\"k\">while </span><span class=\"nb\">read</span> <span class=\"nt\">-r</span> s_table<span class=\"k\">do</span><span class=\"nv\">table</span><span class=\"o\">=</span><span class=\"si\">$(</span><span class=\"nb\">echo</span> <span class=\"nv\">$s_table</span> | <span class=\"nb\">awk</span> <span class=\"nt\">-F</span><span class=\"s1\">','</span> <span class=\"s1\">'{print $1}'</span><span class=\"si\">)</span><span class=\"nb\">echo</span> <span class=\"s2\">\"COPY your_schema.</span><span class=\"nv\">$table</span><span class=\"s2\"> from 's3://migration-bucket/merged_manifest/</span><span class=\"nv\">$table</span><span class=\"s2\">.manifest' MANIFEST iam_role 'arn:aws:iam::1231231231:role/s3-role' REGION 'ap-south-1' REMOVEQUOTES IGNOREHEADER 1 ESCAPE DATEFORMAT 'auto' TIMEFORMAT 'auto' GZIP DELIMITER '|' ACCEPTINVCHARS '?' COMPUPDATE FALSE STATUPDATE FALSE MAXERROR 0 BLANKSASNULL EMPTYASNULL  EXPLICIT_IDS\"</span>  <span class=\"o\">&gt;</span> /root/copy-sql/copy-<span class=\"nv\">$table</span>.sql<span class=\"k\">done</span> &lt; source_generate_copy</code></pre></figure><p>All tables COPY command will be generated and located on <code class=\"language-html highlighter-rouge\">/root/copy-sql/</code> location.</p><p>I have written about how to generate the SQL command for bulk exported tables in <strong><a href=\"https://thedataguy.in/redshift-copy-script-from-sct-agent-export-s3-part1/\">Part 2</a></strong>.</p>",
            "url": "/2020/04/26/redshift-copy-script-from-sct-agent-export-s3-part1",
            
            
            
            "tags": ["aws","redshift","shellscript","automation"],
            
            "date_published": "2020-04-26T14:30:00+00:00",
            "date_modified": "2020-04-26T14:30:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2020/04/14/automate-redshift-vacuum-analyze-using-shell-script-utility",
            "title": "Automate RedShift Vacuum And Analyze with Script",
            "summary": "Automate the RedShift vacuum and analyze using the shell script utility",
            "content_text": "Vacuum and Analyze process in AWS Redshift is a pain point to everyone, most of us trying to automate with their favorite scripting languge. AWS RedShift is an enterprise data warehouse solution to handle petabyte-scale data for you. AWS also improving its quality by adding a lot more features like Concurrency scaling, Spectrum, Auto WLM, etc. But for a DBA or a RedShift admin its always a headache to vacuum the cluster and do analyze to update the statistics. Since its build on top of the PostgreSQL database. But RedShift will do the Full vacuum without locking the tables. And they can trigger the auto vacuum at any time whenever the cluster load is less. But for a busy Cluster where everyday 200GB+ data will be added and modified some decent amount of data will not get benefit from the native auto vacuum feature. You know your workload, so you have to set a scheduled vacuum for your cluster and even we had such a situation where we need to build some more handy utility for my workload.Vacuum Analyze Utility:We all know that AWS has an awesome repository for community contributed utilities. We can see a utility for Vacuum as well. But due to some errors and python related dependencies (also this one module is referring modules from other utilities as well). So we wanted to have a utility with the flexibility that we are looking for. And that’s why you are here. We developed(replicated) a shell-based vacuum analyze utility which almost converted all the features from the existing utility also some additional features like DRY RUN and etc. Lets see how it works.You can get the script from my github repo.Script Arguments:To trigger the vacuum you need to provide three mandatory things.  RedShift Endpoint  User Name  Database NameThis utility will not support cross database vacuum, it’s the PostgreSQL limitation. There are some other parameters that will get generated automatically if you didn’t pass them as an argument. Please refer to the below table.            Argument      Details      Default                  -h      RedShift Endpoint                     -u      User name (super admin user)                     -P      password for the redshift user      use pgpass file              -p      RedShift Port      5439              -d      Database name                     -s      Schema name to vacuum/analyze, for multiple schemas then use comma (eg: ‘schema1,schema2’)      ALL              -t      Table name to vacuum/analyze, for multiple tables then use comma (eg: ‘table1,table2’)      ALL              -b      Blacklisted tables, these tables will be ignored from the vacuum/analyze      Nothing              -k      Blacklisted schemas, these schemas will be ignored from the vacuum/analyze      Nothing              -w      WLM slot count to allocate limited memory      1              -q      querygroup for the vacuum/analyze, Default=default (for now I didn’t use this in script)      default              -a      Perform analyze or not [Binary value, if 1 then Perform 0 means don’t Perform]      1              -r      set analyze_threshold_percent      5              -v      Perform vacuum or not [Binary value, if 1 then Perform 0 means don’t Perform]      1              -o      vacuum options [FULL, SORT ONLY, DELETE ONLY, REINDEX ]      SORT ONLY              -c      vacuum threshold percentage      80              -x      Filter the tables based on unsorted rows from svv_table_info      10              -f      Filter the tables based on stats_off from svv_table_info      10              -z      DRY RUN - just print the vacuum and analyze queries on the screen [1 Yes, 0 No]      0      Installation:For this, you just need psql client only, no need to install any other tools/software.Example Commands:Run vacuum and Analyze on all the tables../vacuum-analyze-utility.sh -h endpoint -u bhuvi -d dev Run vacuum and Analyze on the schema sc1, sc2../vacuum-analyze-utility.sh -h endpoint -u bhuvi -d dev -s 'sc1,sc2'Run vacuum FULL on all the tables in all the schema except the schema sc1. But don’t want Analyze./vacuum-analyze-utility.sh -h endpoint -u bhuvi -d dev -k sc1 -o FULL -a 0 -v 1or./vacuum-analyze-utility.sh -h endpoint -u bhuvi -d dev -k sc1 -o FULL -a 0Run Analyze only on all the tables except the tables tb1,tbl3../vacuum-analyze-utility.sh -h endpoint -u bhuvi -d dev -b 'tbl1,tbl3' -a 1 -v 0or ./vacuum-analyze-utility.sh -h endpoint -u bhuvi -d dev -b 'tbl1,tbl3' -v 0Use a password on the command line../vacuum-analyze-utility.sh -h endpoint -u bhuvi -d dev -P bhuvipasswordRun vacuum and analyze on the tables where unsorted rows are greater than 10%../vacuum-analyze-utility.sh -h endpoint -u bhuvi -d dev -v 1 -a 1 -x 10or./vacuum-analyze-utility.sh -h endpoint -u bhuvi -d dev -x 10Run the Analyze on all the tables in schema sc1 where stats_off is greater than 5../vacuum-analyze-utility.sh -h endpoint -u bhuvi -d dev -v 0 -a 1 -f 5Run the vacuum only on the table tbl1 which is in the schema sc1 with the Vacuum threshold 90%../vacuum-analyze-utility.sh -h endpoint -u bhuvi -d dev -s sc1 -t tbl1 -a 0 -c 90Run analyze only the schema sc1 but set the analyze_threshold_percent=0.01./vacuum-analyze-utility.sh -h endpoint -u bhuvi -d dev -s sc1 -t tbl1 -a 1 -v 0 -r 0.01Do a dry run (generate SQL queries) for analyze all the tables on the schema sc2../vacuum-analyze-utility.sh -h endpoint -u bhuvi -d dev -s sc2 -z 1Do a dry run (generate SQL queries) for both vacuum and analyze for the table tbl3 on all the schema../vacuum-analyze-utility.sh -h endpoint -u bhuvi -d dev -t tbl3 -z 1Schedule different vacuum options based on the dayWe’ll not full the Vacuum full on daily basis, so If you want to run vacumm only on Sunday and do vacuum SORT ONLY on the other day’s without creating a new cron job you can handle this from the script.Just remove this piece of the code.if [[ $vacuumoption == 'unset' ]]\tthen vacuumoption='SORT ONLY'else\tvacuumoption=$vacuumoptionfiAnd add this lines.## Eg: run vacuum FULL on Sunday and SORT ONLY on other daysif [[ `date '+%a'` == 'Sun' ]]\tthen  vacuumoption='FULL'else \tvacuumoption=\"SORT ONLY\"fiSample output:For vacumm and Analyze:./vacuum-analyze-utility.sh -h endpoint -u bhuvi -d dev -b tbl1 -k sc1 -a 1  -v 1 -x 0 -f 0    2020-04-13 18:35:25 Starting the process.....2020-04-13 18:35:25 Validating the Host,User,Database arguments2020-04-13 18:35:25 Perfect all the mandatory arguments are set2020-04-13 18:35:25 Validating the Vacuum and Analyze arguments2020-04-13 18:35:25 Vacuum Arguemnt is 12020-04-13 18:35:25 Analyze Arguemnt is 12020-04-13 18:35:25 Password will be taken from pgpass2020-04-13 18:35:25 Getting the list of schema2020-04-13 18:35:26 Getting the list of Tables2020-04-13 18:35:26 Setting the other arguments2020-04-13 18:35:27 Vacuum is Starting now, stay tune !!!2020-04-13 18:35:28 Vacuum done2020-04-13 18:35:29 Analyze is Starting now, Please wait...2020-04-13 18:35:29 Analyze doneFor Dry Run:./vacuum-analyze-utility.sh -h endpoint -u bhuvi -d dev -s sc3 -a 1  -v 1 -x 80 -f 0 -z 12020-04-13 18:33:53 Starting the process.....2020-04-13 18:33:53 Validating the Host,User,Database arguments2020-04-13 18:33:53 Perfect all the mandatory arguments are set2020-04-13 18:33:53 Validating the Vacuum and Analyze arguments2020-04-13 18:33:53 Vacuum Arguemnt is 12020-04-13 18:33:53 Analyze Arguemnt is 12020-04-13 18:33:53 Password will be taken from pgpass2020-04-13 18:33:53 Getting the list of schema2020-04-13 18:33:54 Getting the list of Tables2020-04-13 18:33:54 Setting the other argumentsDRY RUN for vacuum------------------vacuum SORT ONLY public.nyc_data to 80 percent;vacuum SORT ONLY sc3.tbl3 to 80 percent;DRY RUN for Analyze-------------------analyze public.nyc_data;analyze sc3.tbl3;Conclusion:If you found any issues or looking for a feature please feel free to open an issue on the github page, also if you want to contribute for this utility please comment below.Click here to get the Code ",
            "content_html": "<p>Vacuum and Analyze process in AWS Redshift is a pain point to everyone, most of us trying to automate with their favorite scripting languge. AWS RedShift is an enterprise data warehouse solution to handle petabyte-scale data for you. AWS also improving its quality by adding a lot more features like Concurrency scaling, Spectrum, Auto WLM, etc. But for a DBA or a RedShift admin its always a headache to vacuum the cluster and do analyze to update the statistics. Since its build on top of the PostgreSQL database. But RedShift will do the Full vacuum without locking the tables. And they can trigger the auto vacuum at any time whenever the cluster load is less. But for a busy Cluster where everyday 200GB+ data will be added and modified some decent amount of data will not get benefit from the native auto vacuum feature. You know your workload, so you have to set a scheduled vacuum for your cluster and even we had such a situation where we need to build some more handy utility for my workload.</p><h2 id=\"vacuum-analyze-utility\">Vacuum Analyze Utility:</h2><p>We all know that AWS has an awesome repository for community contributed utilities. We can see a utility for Vacuum as well. But due to some errors and python related dependencies (also this one module is referring modules from other utilities as well). So we wanted to have a utility with the flexibility that we are looking for. And that’s why you are here. We developed(replicated) a shell-based vacuum analyze utility which almost converted all the features from the existing utility also some additional features like DRY RUN and etc. Lets see how it works.</p><p>You can get the script from my <a href=\"https://github.com/BhuviTheDataGuy/RedShift-ToolKit/tree/master/VacuumAnalyzeUtility\">github repo</a>.</p><h2 id=\"script-arguments\">Script Arguments:</h2><p>To trigger the vacuum you need to provide three mandatory things.</p><ol>  <li>RedShift Endpoint</li>  <li>User Name</li>  <li>Database Name</li></ol><p>This utility will not support cross database vacuum, it’s the PostgreSQL limitation. There are some other parameters that will get generated automatically if you didn’t pass them as an argument. Please refer to the below table.</p><table>  <thead>    <tr>      <th>Argument</th>      <th>Details</th>      <th>Default</th>    </tr>  </thead>  <tbody>    <tr>      <td>-h</td>      <td>RedShift Endpoint</td>      <td> </td>    </tr>    <tr>      <td>-u</td>      <td>User name (super admin user)</td>      <td> </td>    </tr>    <tr>      <td>-P</td>      <td>password for the redshift user</td>      <td>use pgpass file</td>    </tr>    <tr>      <td>-p</td>      <td>RedShift Port</td>      <td>5439</td>    </tr>    <tr>      <td>-d</td>      <td>Database name</td>      <td> </td>    </tr>    <tr>      <td>-s</td>      <td>Schema name to vacuum/analyze, for multiple schemas then use comma (eg: ‘schema1,schema2’)</td>      <td>ALL</td>    </tr>    <tr>      <td>-t</td>      <td>Table name to vacuum/analyze, for multiple tables then use comma (eg: ‘table1,table2’)</td>      <td>ALL</td>    </tr>    <tr>      <td>-b</td>      <td>Blacklisted tables, these tables will be ignored from the vacuum/analyze</td>      <td>Nothing</td>    </tr>    <tr>      <td>-k</td>      <td>Blacklisted schemas, these schemas will be ignored from the vacuum/analyze</td>      <td>Nothing</td>    </tr>    <tr>      <td>-w</td>      <td>WLM slot count to allocate limited memory</td>      <td>1</td>    </tr>    <tr>      <td>-q</td>      <td>querygroup for the vacuum/analyze, Default=default (for now I didn’t use this in script)</td>      <td>default</td>    </tr>    <tr>      <td>-a</td>      <td>Perform analyze or not [Binary value, if 1 then Perform 0 means don’t Perform]</td>      <td>1</td>    </tr>    <tr>      <td>-r</td>      <td>set analyze_threshold_percent</td>      <td>5</td>    </tr>    <tr>      <td>-v</td>      <td>Perform vacuum or not [Binary value, if 1 then Perform 0 means don’t Perform]</td>      <td>1</td>    </tr>    <tr>      <td>-o</td>      <td>vacuum options [FULL, SORT ONLY, DELETE ONLY, REINDEX ]</td>      <td>SORT ONLY</td>    </tr>    <tr>      <td>-c</td>      <td>vacuum threshold percentage</td>      <td>80</td>    </tr>    <tr>      <td>-x</td>      <td>Filter the tables based on unsorted rows from svv_table_info</td>      <td>10</td>    </tr>    <tr>      <td>-f</td>      <td>Filter the tables based on stats_off from svv_table_info</td>      <td>10</td>    </tr>    <tr>      <td>-z</td>      <td>DRY RUN - just print the vacuum and analyze queries on the screen [1 Yes, 0 No]</td>      <td>0</td>    </tr>  </tbody></table><h2 id=\"installation\">Installation:</h2><p>For this, you just need <code class=\"language-html highlighter-rouge\">psql client</code> only, no need to install any other tools/software.</p><h2 id=\"example-commands\">Example Commands:</h2><p>Run vacuum and Analyze on all the tables.</p><figure class=\"highlight\"><pre><code class=\"language-sh\" data-lang=\"sh\">./vacuum-analyze-utility.sh <span class=\"nt\">-h</span> endpoint <span class=\"nt\">-u</span> bhuvi <span class=\"nt\">-d</span> dev </code></pre></figure><p>Run vacuum and Analyze on the schema sc1, sc2.</p><figure class=\"highlight\"><pre><code class=\"language-sh\" data-lang=\"sh\">./vacuum-analyze-utility.sh <span class=\"nt\">-h</span> endpoint <span class=\"nt\">-u</span> bhuvi <span class=\"nt\">-d</span> dev <span class=\"nt\">-s</span> <span class=\"s1\">'sc1,sc2'</span></code></pre></figure><p>Run vacuum FULL on all the tables in all the schema except the schema sc1. But don’t want Analyze</p><figure class=\"highlight\"><pre><code class=\"language-sh\" data-lang=\"sh\">./vacuum-analyze-utility.sh <span class=\"nt\">-h</span> endpoint <span class=\"nt\">-u</span> bhuvi <span class=\"nt\">-d</span> dev <span class=\"nt\">-k</span> sc1 <span class=\"nt\">-o</span> FULL <span class=\"nt\">-a</span> 0 <span class=\"nt\">-v</span> 1or./vacuum-analyze-utility.sh <span class=\"nt\">-h</span> endpoint <span class=\"nt\">-u</span> bhuvi <span class=\"nt\">-d</span> dev <span class=\"nt\">-k</span> sc1 <span class=\"nt\">-o</span> FULL <span class=\"nt\">-a</span> 0</code></pre></figure><p>Run Analyze only on all the tables except the tables tb1,tbl3.</p><figure class=\"highlight\"><pre><code class=\"language-sh\" data-lang=\"sh\">./vacuum-analyze-utility.sh <span class=\"nt\">-h</span> endpoint <span class=\"nt\">-u</span> bhuvi <span class=\"nt\">-d</span> dev <span class=\"nt\">-b</span> <span class=\"s1\">'tbl1,tbl3'</span> <span class=\"nt\">-a</span> 1 <span class=\"nt\">-v</span> 0or ./vacuum-analyze-utility.sh <span class=\"nt\">-h</span> endpoint <span class=\"nt\">-u</span> bhuvi <span class=\"nt\">-d</span> dev <span class=\"nt\">-b</span> <span class=\"s1\">'tbl1,tbl3'</span> <span class=\"nt\">-v</span> 0</code></pre></figure><p>Use a password on the command line.</p><figure class=\"highlight\"><pre><code class=\"language-sh\" data-lang=\"sh\">./vacuum-analyze-utility.sh <span class=\"nt\">-h</span> endpoint <span class=\"nt\">-u</span> bhuvi <span class=\"nt\">-d</span> dev <span class=\"nt\">-P</span> bhuvipassword</code></pre></figure><p>Run vacuum and analyze on the tables where unsorted rows are greater than 10%.</p><figure class=\"highlight\"><pre><code class=\"language-sh\" data-lang=\"sh\">./vacuum-analyze-utility.sh <span class=\"nt\">-h</span> endpoint <span class=\"nt\">-u</span> bhuvi <span class=\"nt\">-d</span> dev <span class=\"nt\">-v</span> 1 <span class=\"nt\">-a</span> 1 <span class=\"nt\">-x</span> 10or./vacuum-analyze-utility.sh <span class=\"nt\">-h</span> endpoint <span class=\"nt\">-u</span> bhuvi <span class=\"nt\">-d</span> dev <span class=\"nt\">-x</span> 10</code></pre></figure><p>Run the Analyze on all the tables in schema sc1 where stats_off is greater than 5.</p><figure class=\"highlight\"><pre><code class=\"language-sh\" data-lang=\"sh\">./vacuum-analyze-utility.sh <span class=\"nt\">-h</span> endpoint <span class=\"nt\">-u</span> bhuvi <span class=\"nt\">-d</span> dev <span class=\"nt\">-v</span> 0 <span class=\"nt\">-a</span> 1 <span class=\"nt\">-f</span> 5</code></pre></figure><p>Run the vacuum only on the table tbl1 which is in the schema sc1 with the Vacuum threshold 90%.</p><figure class=\"highlight\"><pre><code class=\"language-sh\" data-lang=\"sh\">./vacuum-analyze-utility.sh <span class=\"nt\">-h</span> endpoint <span class=\"nt\">-u</span> bhuvi <span class=\"nt\">-d</span> dev <span class=\"nt\">-s</span> sc1 <span class=\"nt\">-t</span> tbl1 <span class=\"nt\">-a</span> 0 <span class=\"nt\">-c</span> 90</code></pre></figure><p>Run analyze only the schema sc1 but set the <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/r_analyze_threshold_percent.html\">analyze_threshold_percent=0.01</a></p><figure class=\"highlight\"><pre><code class=\"language-sh\" data-lang=\"sh\">./vacuum-analyze-utility.sh <span class=\"nt\">-h</span> endpoint <span class=\"nt\">-u</span> bhuvi <span class=\"nt\">-d</span> dev <span class=\"nt\">-s</span> sc1 <span class=\"nt\">-t</span> tbl1 <span class=\"nt\">-a</span> 1 <span class=\"nt\">-v</span> 0 <span class=\"nt\">-r</span> 0.01</code></pre></figure><p>Do a dry run (generate SQL queries) for analyze all the tables on the schema sc2.</p><figure class=\"highlight\"><pre><code class=\"language-sh\" data-lang=\"sh\">./vacuum-analyze-utility.sh <span class=\"nt\">-h</span> endpoint <span class=\"nt\">-u</span> bhuvi <span class=\"nt\">-d</span> dev <span class=\"nt\">-s</span> sc2 <span class=\"nt\">-z</span> 1</code></pre></figure><p>Do a dry run (generate SQL queries) for both vacuum and analyze for the table tbl3 on all the schema.</p><figure class=\"highlight\"><pre><code class=\"language-sh\" data-lang=\"sh\">./vacuum-analyze-utility.sh <span class=\"nt\">-h</span> endpoint <span class=\"nt\">-u</span> bhuvi <span class=\"nt\">-d</span> dev <span class=\"nt\">-t</span> tbl3 <span class=\"nt\">-z</span> 1</code></pre></figure><h2 id=\"schedule-different-vacuum-options-based-on-the-day\">Schedule different vacuum options based on the day</h2><p>We’ll not full the Vacuum full on daily basis, so If you want to run vacumm only on Sunday and do vacuum <code class=\"language-html highlighter-rouge\">SORT ONLY</code> on the other day’s without creating a new cron job you can handle this from the script.</p><p>Just remove this piece of the code.</p><figure class=\"highlight\"><pre><code class=\"language-sh\" data-lang=\"sh\"><span class=\"k\">if</span> <span class=\"o\">[[</span> <span class=\"nv\">$vacuumoption</span> <span class=\"o\">==</span> <span class=\"s1\">'unset'</span> <span class=\"o\">]]</span>\t<span class=\"k\">then </span><span class=\"nv\">vacuumoption</span><span class=\"o\">=</span><span class=\"s1\">'SORT ONLY'</span><span class=\"k\">else\t</span><span class=\"nv\">vacuumoption</span><span class=\"o\">=</span><span class=\"nv\">$vacuumoption</span><span class=\"k\">fi</span></code></pre></figure><p>And add this lines.</p><figure class=\"highlight\"><pre><code class=\"language-sh\" data-lang=\"sh\"><span class=\"c\">## Eg: run vacuum FULL on Sunday and SORT ONLY on other days</span><span class=\"k\">if</span> <span class=\"o\">[[</span> <span class=\"sb\">`</span><span class=\"nb\">date</span> <span class=\"s1\">'+%a'</span><span class=\"sb\">`</span> <span class=\"o\">==</span> <span class=\"s1\">'Sun'</span> <span class=\"o\">]]</span>\t<span class=\"k\">then  </span><span class=\"nv\">vacuumoption</span><span class=\"o\">=</span><span class=\"s1\">'FULL'</span><span class=\"k\">else \t</span><span class=\"nv\">vacuumoption</span><span class=\"o\">=</span><span class=\"s2\">\"SORT ONLY\"</span><span class=\"k\">fi</span></code></pre></figure><h2 id=\"sample-output\">Sample output:</h2><p><strong>For vacumm and Analyze:</strong></p><figure class=\"highlight\"><pre><code class=\"language-sh\" data-lang=\"sh\">./vacuum-analyze-utility.sh <span class=\"nt\">-h</span> endpoint <span class=\"nt\">-u</span> bhuvi <span class=\"nt\">-d</span> dev <span class=\"nt\">-b</span> tbl1 <span class=\"nt\">-k</span> sc1 <span class=\"nt\">-a</span> 1  <span class=\"nt\">-v</span> 1 <span class=\"nt\">-x</span> 0 <span class=\"nt\">-f</span> 0    2020-04-13 18:35:25 Starting the process.....2020-04-13 18:35:25 Validating the Host,User,Database arguments2020-04-13 18:35:25 Perfect all the mandatory arguments are <span class=\"nb\">set</span>2020-04-13 18:35:25 Validating the Vacuum and Analyze arguments2020-04-13 18:35:25 Vacuum Arguemnt is 12020-04-13 18:35:25 Analyze Arguemnt is 12020-04-13 18:35:25 Password will be taken from pgpass2020-04-13 18:35:25 Getting the list of schema2020-04-13 18:35:26 Getting the list of Tables2020-04-13 18:35:26 Setting the other arguments2020-04-13 18:35:27 Vacuum is Starting now, stay tune <span class=\"o\">!!!</span>2020-04-13 18:35:28 Vacuum <span class=\"k\">done</span>2020-04-13 18:35:29 Analyze is Starting now, Please wait...2020-04-13 18:35:29 Analyze <span class=\"k\">done</span></code></pre></figure><p><strong>For Dry Run:</strong></p><figure class=\"highlight\"><pre><code class=\"language-sh\" data-lang=\"sh\">./vacuum-analyze-utility.sh <span class=\"nt\">-h</span> endpoint <span class=\"nt\">-u</span> bhuvi <span class=\"nt\">-d</span> dev <span class=\"nt\">-s</span> sc3 <span class=\"nt\">-a</span> 1  <span class=\"nt\">-v</span> 1 <span class=\"nt\">-x</span> 80 <span class=\"nt\">-f</span> 0 <span class=\"nt\">-z</span> 12020-04-13 18:33:53 Starting the process.....2020-04-13 18:33:53 Validating the Host,User,Database arguments2020-04-13 18:33:53 Perfect all the mandatory arguments are <span class=\"nb\">set</span>2020-04-13 18:33:53 Validating the Vacuum and Analyze arguments2020-04-13 18:33:53 Vacuum Arguemnt is 12020-04-13 18:33:53 Analyze Arguemnt is 12020-04-13 18:33:53 Password will be taken from pgpass2020-04-13 18:33:53 Getting the list of schema2020-04-13 18:33:54 Getting the list of Tables2020-04-13 18:33:54 Setting the other argumentsDRY RUN <span class=\"k\">for </span>vacuum<span class=\"nt\">------------------</span>vacuum SORT ONLY public.nyc_data to 80 percent<span class=\"p\">;</span>vacuum SORT ONLY sc3.tbl3 to 80 percent<span class=\"p\">;</span>DRY RUN <span class=\"k\">for </span>Analyze<span class=\"nt\">-------------------</span>analyze public.nyc_data<span class=\"p\">;</span>analyze sc3.tbl3<span class=\"p\">;</span></code></pre></figure><h2 id=\"conclusion\">Conclusion:</h2><p>If you found any issues or looking for a feature please feel free to open an issue on the github page, also if you want to contribute for this utility please comment below.</p><h2 id=\"click-here-to-get-the-code-\"><a href=\"https://github.com/BhuviTheDataGuy/RedShift-ToolKit/tree/master/VacuumAnalyzeUtility\">Click here to get the Code </a></h2>",
            "url": "/2020/04/14/automate-redshift-vacuum-analyze-using-shell-script-utility",
            "image": "/assets/Automate RedShift Vacuum And Analyze Like a Boss.jpg",
            
            
            "tags": ["aws","redshift","shellscript","automation"],
            
            "date_published": "2020-04-14T01:15:00+00:00",
            "date_modified": "2020-04-14T01:15:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2020/04/06/export-redshift-system-tables-views-to-s3",
            "title": "Export RedShift System Tables And Views To S3",
            "summary": "Export the ResShift system tables and views (STL tables) to S3 for persistent storage with incremental backup",
            "content_text": "RedShift’s system tables and views are haveing more depth information about the queries, Its highly important to export the RedShift system tables and views (STL tables) to S3 for persistent. These tables contains the information like query history, plan, query summary, etc. But these informations only available for very shot period of time. RedShift documentation says that from 2 days to 5 days we can retrive the data. It’ll automatically flush the older data to keep the disk space utilization under the control. This is fine for ad-hoc performance tuning but if you want to keep the complete history of the data from these system tables and views then we have to export/Unload them to S3. I did google for implementing this on my infra, I found 2 awesome links from official AWS blog and github repo. But I wanted to try something easy with cost benifical. So I have written a stored procedure to do this.Exsiting Options:  #1 Export via Glue - We have to spend money for Glue and DynamoDB. Lambda will be the trigger to export this.  #2 Export via Lambda - This is just a stright farword unload command, but this repo is having a lot of other utilities. If someone is new to Python, they might get stuck in understand this. But anyhow we only need the system table export option.  #3 Export via Stored Procedure - I decided to use stored procedures to do this. Few tables like stl_querytext and stl_query_metrics or not having any timestamp column, but we have to export them incrementally.List of tables and views:For now I have created this procedure to export only the following tables.  stl_query  stl_dlltext  stl_querytext  stl_utilitytext  stl_wlm_query  stl_explain  svl_query_summary  stl_query_metrics  stl_alert_event_logIncremental approach:From the above tables, 4 tables are having the timestamp column. So we can easily find the incremental data. First time it’ll upload all of the rows and mark the maximum value for the timestamp into this history table. Next time it’ll go and look at the max value of the timestamp column and export the incremental data.The tables that doesn’t have the timestamp column, we can join them on the userid, query id, pid, xid columns with stl_query table. So we can get the incremetnal data.Stored Procedure:The following items are hardcoded, you need to replace them.  s3_path - s3://prod-bucket/log-testing  iam_role - arn:aws:iam::XXXXXXXXXX:role/Red-Shift-Access-S3  unload_options - ADDQUOTES ALLOWOVERWRITE HEADER                              delimiter -                      NOTE: stl_querytxt and stl_ddltext tables are having the new line charactors like \\n and \\r, so we have to replace them.Create the history table:CREATE TABLE PUBLIC.history_sysobjects   (      id               INT IDENTITY(1, 1),      export_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,      object_name      VARCHAR(100),      start_time       VARCHAR(100),      unload_query     VARCHAR(65000)   ); RedShift procedure:CREATE OR REPLACE PROCEDURE public.persist_sysobjects()LANGUAGE plpgsql AS$$DECLARE-- Store the max Timestampstlq_starttime timestamp;ddlt_starttime timestamp;utlt_starttime timestamp;wq_starttime timestamp;-- S3 path and credentialss3_path varchar(300);iam_role varchar(300);unload_options varchar(500);delimiter varchar(10);un_year varchar(10);un_month varchar(10);un_day varchar(10);-- Declare queries as variablestl_query_unload varchar(65000);stl_ddltext_unload varchar(65000);stl_utilitytext_unload varchar(65000);stl_querytext_unload varchar(65000);stl_wlm_query_unload varchar(65000);stl_explain_unload varchar(65000);svl_query_summary_unload varchar(65000);stl_query_metrics_unload varchar(65000);stl_alert_event_log_unload varchar(65000);BEGIN-- Get the yyyy/mm/dd for paritions in S3select to_char(GETDATE(),'YYYY') into un_year;select to_char(GETDATE(),'MM') into un_month;select to_char(GETDATE(),'DD') into un_day;-- S3 Optionss3_path:='s3://prod-bucket/log-testing';iam_role:='arn:aws:iam::XXXXXXXXXX:role/Red-Shift-Access-S3';unload_options:='ADDQUOTES ALLOWOVERWRITE HEADER';delimiter:='|';-- Find the current max timestamp select max(starttime) from stl_query into stlq_starttime;select max(starttime) from stl_ddltext into ddlt_starttime;select max(starttime) from stl_utilitytext into utlt_starttime;select max(service_class_start_time) from stl_wlm_query into wq_starttime;-- Start exporting the dataRAISE info 'Starting to unload stl_query';stl_query_unload='unload(\\'SELECT stlq.* FROM stl_query  stlq,   (SELECT NVL(MAX(start_time),\\\\''1902-01-01\\\\''::TIMESTAMP) AS max_starttime FROM public.history_sysobjects where object_name=\\\\''stl_query\\\\'') h WHERE stlq.starttime &gt; h.max_starttime\\') to \\''||s3_path||'/stl_query/'||un_year||'/'||un_month||'/'||un_day||'/stl_query_\\' iam_role \\''||iam_role||'\\' '||unload_options||' delimiter \\''||delimiter||'\\'';execute stl_query_unload;     insert into public.history_sysobjects (object_name,start_time,unload_query) values ('stl_query',stlq_starttime,stl_query_unload);RAISE info 'Starting to unload stl_ddltext';stl_ddltext_unload='unload(\\'SELECT ddlt.userid,ddlt.pid,ddlt.label,ddlt.starttime, ddlt.endtime,ddlt.sequence,replace(replace(ddlt.text,\\\\''\\\\\\\\\\\\\\\\n\\\\'',\\\\''\\\\''),\\\\''\\\\\\\\\\\\\\\\r\\\\'',\\\\''\\\\'') as text FROM stl_ddltext ddlt,   (SELECT NVL(MAX(start_time),\\\\''1902-01-01\\\\''::TIMESTAMP) AS max_starttime FROM public.history_sysobjects where object_name=\\\\''stl_ddltext\\\\'') h WHERE ddlt.starttime &gt; h.max_starttime\\') to \\''||s3_path||'/stl_ddltext/'||un_year||'/'||un_month||'/'||un_day||'/stl_ddltext_\\' iam_role \\''||iam_role||'\\' '||unload_options||' delimiter \\''||delimiter||'\\'';execute stl_ddltext_unload;     insert into public.history_sysobjects (object_name,start_time,unload_query) values ('stl_ddltext',ddlt_starttime,stl_ddltext_unload);RAISE info 'Starting to unload stl_utilitytext';stl_utilitytext_unload='unload(\\'SELECT utlt.* FROM stl_utilitytext utlt, (SELECT NVL(MAX(start_time),\\\\''1902-01-01\\\\''::TIMESTAMP) AS max_starttime FROM public.history_sysobjects where object_name=\\\\''stl_utilitytext\\\\'') h WHERE utlt.starttime &gt; h.max_starttime\\') to \\''||s3_path||'/stl_utilitytext/'||un_year||'/'||un_month||'/'||un_day||'/stl_utilitytext_\\' iam_role \\''||iam_role||'\\' '||unload_options||' delimiter \\''||delimiter||'\\'';execute stl_utilitytext_unload;     insert into public.history_sysobjects (object_name,start_time,unload_query) values ('stl_utilitytext',utlt_starttime,stl_utilitytext_unload);RAISE info 'Starting to unload stl_querytext';stl_querytext_unload='unload(\\'select qtxt.userid,qtxt.xid,qtxt.pid,qtxt.query,qtxt.sequence,replace(replace(qtxt.text,\\\\''\\\\\\\\\\\\\\\\n\\\\'',\\\\''\\\\''),\\\\''\\\\\\\\\\\\\\\\r\\\\'',\\\\''\\\\'') as text from stl_querytext qtxt join stl_query qt on qt.userid=qtxt.userid and qt.query=qtxt.query and qt.xid=qtxt.xid and qt.pid=qtxt.pid where qt.starttime&gt;(SELECT NVL(MAX(start_time),\\\\''1902-01-01\\\\''::TIMESTAMP) AS max_starttime FROM public.history_sysobjects where object_name=\\\\''stl_query\\\\'')\\') to \\''||s3_path||'/stl_querytext/'||un_year||'/'||un_month||'/'||un_day||'/stl_querytext_\\' iam_role \\''||iam_role||'\\' '||unload_options||' delimiter \\''||delimiter||'\\'';execute stl_querytext_unload;     insert into public.history_sysobjects (object_name,unload_query) values ('stl_querytext',stl_querytext_unload);RAISE info 'Starting to unload stl_wlm_query';stl_wlm_query_unload='unload(\\'SELECT wq.* FROM stl_wlm_query wq, (SELECT NVL(MAX(start_time),\\\\''1902-01-01\\\\''::TIMESTAMP) AS max_service_class_start_time FROM public.history_sysobjects where object_name=\\\\''stl_wlm_query\\\\'') h WHERE wq.service_class_start_time &gt; h.max_service_class_start_time\\') to \\''||s3_path||'/stl_wlm_query/'||un_year||'/'||un_month||'/'||un_day||'/stl_wlm_query_\\' iam_role \\''||iam_role||'\\' '||unload_options||' delimiter \\''||delimiter||'\\'';execute stl_wlm_query_unload;     insert into public.history_sysobjects (object_name,start_time,unload_query) values ('stl_wlm_query',stlq_starttime,stl_wlm_query_unload);RAISE info 'Starting to unload stl_explain';stl_explain_unload='unload(\\'select exp.* from stl_explain exp join stl_query stlq on stlq.userid=exp.userid and stlq.query=exp.query where stlq.starttime&gt;(SELECT NVL(MAX(start_time),\\\\''1902-01-01\\\\''::TIMESTAMP) AS max_starttime FROM public.history_sysobjects where object_name=\\\\''stl_query\\\\'')\\') to \\''||s3_path||'/stl_explain/'||un_year||'/'||un_month||'/'||un_day||'/stl_explain_\\' iam_role \\''||iam_role||'\\' '||unload_options||' delimiter \\''||delimiter||'\\'';execute stl_explain_unload;     insert into public.history_sysobjects (object_name,unload_query) values ('stl_explain',stl_explain_unload);RAISE info 'Starting to unload svl_query_summary';svl_query_summary_unload='unload(\\'select qs.* from svl_query_summary qs join stl_query stlq on stlq.userid=qs.userid and stlq.query=qs.query  where stlq.starttime&gt;(SELECT NVL(MAX(start_time),\\\\''1902-01-01\\\\''::TIMESTAMP) AS max_starttime FROM public.history_sysobjects where object_name=\\\\''stl_query\\\\'')\\')  to \\''||s3_path||'/svl_query_summary/'||un_year||'/'||un_month||'/'||un_day||'/svl_query_summary_\\' iam_role \\''||iam_role||'\\' '||unload_options||' delimiter \\''||delimiter||'\\'';execute svl_query_summary_unload;     insert into public.history_sysobjects (object_name,unload_query) values ('svl_query_summary',svl_query_summary_unload);RAISE info 'Starting to unload stl_query_metrics';stl_query_metrics_unload='unload(\\'select qm.* from stl_query_metrics qm join stl_query stlq on stlq.userid=qm.userid and stlq.query=qm.query  where stlq.starttime&gt;(SELECT NVL(MAX(start_time),\\\\''1902-01-01\\\\''::TIMESTAMP) AS max_starttime FROM public.history_sysobjects where object_name=\\\\''stl_query\\\\'')\\') to \\''||s3_path||'/stl_query_metrics/'||un_year||'/'||un_month||'/'||un_day||'/stl_query_metrics_\\' iam_role \\''||iam_role||'\\' '||unload_options||' delimiter \\''||delimiter||'\\'';execute stl_query_metrics_unload;     insert into public.history_sysobjects (object_name,unload_query) values ('stl_query_metrics',stl_query_metrics_unload);RAISE info 'Starting to unload stl_alert_event_log';stl_alert_event_log_unload='unload(\\'select evnt.* from stl_alert_event_log evnt join stl_query qt on qt.userid=evnt.userid and qt.query=evnt.query and qt.xid=evnt.xid and qt.pid=evnt.pid  where qt.starttime&gt;(SELECT NVL(MAX(start_time),\\\\''1902-01-01\\\\''::TIMESTAMP) AS max_starttime FROM public.history_sysobjects where object_name=\\\\''stl_query\\\\'')\\') to \\''||s3_path||'/stl_alert_event_log/'||un_year||'/'||un_month||'/'||un_day||'/stl_alert_event_log_\\' iam_role \\''||iam_role||'\\' '||unload_options||' delimiter \\''||delimiter||'\\'';execute stl_alert_event_log_unload;     insert into public.history_sysobjects (object_name,unload_query) values ('stl_alert_event_log',stl_alert_event_log_unload);END$$; ",
            "content_html": "<p>RedShift’s system tables and views are haveing more depth information about the queries, Its highly important to export the RedShift system tables and views (STL tables) to S3 for persistent. These tables contains the information like query history, plan, query summary, etc. But these informations only available for very shot period of time. RedShift documentation says that from 2 days to 5 days we can retrive the data. It’ll automatically flush the older data to keep the disk space utilization under the control. This is fine for ad-hoc performance tuning but if you want to keep the complete history of the data from these system tables and views then we have to export/Unload them to S3. I did google for implementing this on my infra, I found 2 awesome links from official AWS blog and github repo. But I wanted to try something easy with cost benifical. So I have written a stored procedure to do this.</p><h2 id=\"exsiting-options\">Exsiting Options:</h2><ul>  <li><strong><a href=\"https://aws.amazon.com/blogs/big-data/how-to-retain-system-tables-data-spanning-multiple-amazon-redshift-clusters-and-run-cross-cluster-diagnostic-queries/\">#1 Export via Glue</a></strong> - We have to spend money for Glue and DynamoDB. Lambda will be the trigger to export this.</li>  <li><strong><a href=\"https://github.com/awslabs/amazon-redshift-utils/tree/master/src/SystemTablePersistence\">#2 Export via Lambda</a></strong> - This is just a stright farword unload command, but this repo is having a lot of other utilities. If someone is new to Python, they might get stuck in understand this. But anyhow we only need the system table export option.</li>  <li><strong>#3 Export via Stored Procedure</strong> - I decided to use stored procedures to do this. Few tables like <code class=\"language-html highlighter-rouge\">stl_querytext</code> and <code class=\"language-html highlighter-rouge\">stl_query_metrics</code> or not having any timestamp column, but we have to export them incrementally.</li></ul><h2 id=\"list-of-tables-and-views\">List of tables and views:</h2><p>For now I have created this procedure to export only the following tables.</p><ol>  <li>stl_query</li>  <li>stl_dlltext</li>  <li>stl_querytext</li>  <li>stl_utilitytext</li>  <li>stl_wlm_query</li>  <li>stl_explain</li>  <li>svl_query_summary</li>  <li>stl_query_metrics</li>  <li>stl_alert_event_log</li></ol><h2 id=\"incremental-approach\">Incremental approach:</h2><p>From the above tables, 4 tables are having the timestamp column. So we can easily find the incremental data. First time it’ll upload all of the rows and mark the maximum value for the timestamp into this history table. Next time it’ll go and look at the max value of the timestamp column and export the incremental data.The tables that doesn’t have the timestamp column, we can join them on the <code class=\"language-html highlighter-rouge\">userid</code>, <code class=\"language-html highlighter-rouge\">query id</code>, <code class=\"language-html highlighter-rouge\">pid</code>, <code class=\"language-html highlighter-rouge\">xid</code> columns with <code class=\"language-html highlighter-rouge\">stl_query</code> table. So we can get the incremetnal data.</p><h2 id=\"stored-procedure\">Stored Procedure:</h2><p>The following items are hardcoded, you need to replace them.</p><ul>  <li><strong>s3_path</strong> - s3://prod-bucket/log-testing</li>  <li><strong>iam_role</strong> - arn:aws:iam::XXXXXXXXXX:role/Red-Shift-Access-S3</li>  <li><strong>unload_options</strong> - ADDQUOTES ALLOWOVERWRITE HEADER</li>  <li>    <table>      <tbody>        <tr>          <td><strong>delimiter</strong> -</td>        </tr>      </tbody>    </table>  </li></ul><blockquote>  <p><strong>NOTE</strong>: <code class=\"language-html highlighter-rouge\">stl_querytxt</code> and <code class=\"language-html highlighter-rouge\">stl_ddltext</code> tables are having the new line charactors like <code class=\"language-html highlighter-rouge\">\\n</code> and <code class=\"language-html highlighter-rouge\">\\r</code>, so we have to replace them.</p></blockquote><h3 id=\"create-the-history-table\">Create the history table:</h3><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">CREATE</span> <span class=\"k\">TABLE</span> <span class=\"k\">PUBLIC</span><span class=\"p\">.</span><span class=\"n\">history_sysobjects</span>   <span class=\"p\">(</span>      <span class=\"n\">id</span>               <span class=\"nb\">INT</span> <span class=\"k\">IDENTITY</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span>      <span class=\"n\">export_timestamp</span> <span class=\"nb\">TIMESTAMP</span> <span class=\"k\">DEFAULT</span> <span class=\"k\">CURRENT_TIMESTAMP</span><span class=\"p\">,</span>      <span class=\"n\">object_name</span>      <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">),</span>      <span class=\"n\">start_time</span>       <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">),</span>      <span class=\"n\">unload_query</span>     <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">65000</span><span class=\"p\">)</span>   <span class=\"p\">);</span> </code></pre></figure><h3 id=\"redshift-procedure\">RedShift procedure:</h3><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">CREATE</span> <span class=\"k\">OR</span> <span class=\"k\">REPLACE</span> <span class=\"k\">PROCEDURE</span> <span class=\"k\">public</span><span class=\"p\">.</span><span class=\"n\">persist_sysobjects</span><span class=\"p\">()</span><span class=\"k\">LANGUAGE</span> <span class=\"n\">plpgsql</span> <span class=\"k\">AS</span><span class=\"err\">$$</span><span class=\"k\">DECLARE</span><span class=\"c1\">-- Store the max Timestamp</span><span class=\"n\">stlq_starttime</span> <span class=\"nb\">timestamp</span><span class=\"p\">;</span><span class=\"n\">ddlt_starttime</span> <span class=\"nb\">timestamp</span><span class=\"p\">;</span><span class=\"n\">utlt_starttime</span> <span class=\"nb\">timestamp</span><span class=\"p\">;</span><span class=\"n\">wq_starttime</span> <span class=\"nb\">timestamp</span><span class=\"p\">;</span><span class=\"c1\">-- S3 path and credentials</span><span class=\"n\">s3_path</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">300</span><span class=\"p\">);</span><span class=\"n\">iam_role</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">300</span><span class=\"p\">);</span><span class=\"n\">unload_options</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">500</span><span class=\"p\">);</span><span class=\"k\">delimiter</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">);</span><span class=\"n\">un_year</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">);</span><span class=\"n\">un_month</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">);</span><span class=\"n\">un_day</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">);</span><span class=\"c1\">-- Declare queries as variable</span><span class=\"n\">stl_query_unload</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">65000</span><span class=\"p\">);</span><span class=\"n\">stl_ddltext_unload</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">65000</span><span class=\"p\">);</span><span class=\"n\">stl_utilitytext_unload</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">65000</span><span class=\"p\">);</span><span class=\"n\">stl_querytext_unload</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">65000</span><span class=\"p\">);</span><span class=\"n\">stl_wlm_query_unload</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">65000</span><span class=\"p\">);</span><span class=\"n\">stl_explain_unload</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">65000</span><span class=\"p\">);</span><span class=\"n\">svl_query_summary_unload</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">65000</span><span class=\"p\">);</span><span class=\"n\">stl_query_metrics_unload</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">65000</span><span class=\"p\">);</span><span class=\"n\">stl_alert_event_log_unload</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">65000</span><span class=\"p\">);</span><span class=\"k\">BEGIN</span><span class=\"c1\">-- Get the yyyy/mm/dd for paritions in S3</span><span class=\"k\">select</span> <span class=\"n\">to_char</span><span class=\"p\">(</span><span class=\"n\">GETDATE</span><span class=\"p\">(),</span><span class=\"s1\">'YYYY'</span><span class=\"p\">)</span> <span class=\"k\">into</span> <span class=\"n\">un_year</span><span class=\"p\">;</span><span class=\"k\">select</span> <span class=\"n\">to_char</span><span class=\"p\">(</span><span class=\"n\">GETDATE</span><span class=\"p\">(),</span><span class=\"s1\">'MM'</span><span class=\"p\">)</span> <span class=\"k\">into</span> <span class=\"n\">un_month</span><span class=\"p\">;</span><span class=\"k\">select</span> <span class=\"n\">to_char</span><span class=\"p\">(</span><span class=\"n\">GETDATE</span><span class=\"p\">(),</span><span class=\"s1\">'DD'</span><span class=\"p\">)</span> <span class=\"k\">into</span> <span class=\"n\">un_day</span><span class=\"p\">;</span><span class=\"c1\">-- S3 Options</span><span class=\"n\">s3_path</span><span class=\"p\">:</span><span class=\"o\">=</span><span class=\"s1\">'s3://prod-bucket/log-testing'</span><span class=\"p\">;</span><span class=\"n\">iam_role</span><span class=\"p\">:</span><span class=\"o\">=</span><span class=\"s1\">'arn:aws:iam::XXXXXXXXXX:role/Red-Shift-Access-S3'</span><span class=\"p\">;</span><span class=\"n\">unload_options</span><span class=\"p\">:</span><span class=\"o\">=</span><span class=\"s1\">'ADDQUOTES ALLOWOVERWRITE HEADER'</span><span class=\"p\">;</span><span class=\"k\">delimiter</span><span class=\"p\">:</span><span class=\"o\">=</span><span class=\"s1\">'|'</span><span class=\"p\">;</span><span class=\"c1\">-- Find the current max timestamp </span><span class=\"k\">select</span> <span class=\"k\">max</span><span class=\"p\">(</span><span class=\"n\">starttime</span><span class=\"p\">)</span> <span class=\"k\">from</span> <span class=\"n\">stl_query</span> <span class=\"k\">into</span> <span class=\"n\">stlq_starttime</span><span class=\"p\">;</span><span class=\"k\">select</span> <span class=\"k\">max</span><span class=\"p\">(</span><span class=\"n\">starttime</span><span class=\"p\">)</span> <span class=\"k\">from</span> <span class=\"n\">stl_ddltext</span> <span class=\"k\">into</span> <span class=\"n\">ddlt_starttime</span><span class=\"p\">;</span><span class=\"k\">select</span> <span class=\"k\">max</span><span class=\"p\">(</span><span class=\"n\">starttime</span><span class=\"p\">)</span> <span class=\"k\">from</span> <span class=\"n\">stl_utilitytext</span> <span class=\"k\">into</span> <span class=\"n\">utlt_starttime</span><span class=\"p\">;</span><span class=\"k\">select</span> <span class=\"k\">max</span><span class=\"p\">(</span><span class=\"n\">service_class_start_time</span><span class=\"p\">)</span> <span class=\"k\">from</span> <span class=\"n\">stl_wlm_query</span> <span class=\"k\">into</span> <span class=\"n\">wq_starttime</span><span class=\"p\">;</span><span class=\"c1\">-- Start exporting the data</span><span class=\"n\">RAISE</span> <span class=\"n\">info</span> <span class=\"s1\">'Starting to unload stl_query'</span><span class=\"p\">;</span><span class=\"n\">stl_query_unload</span><span class=\"o\">=</span><span class=\"s1\">'unload(</span><span class=\"se\">\\'</span><span class=\"s1\">SELECT stlq.* FROM stl_query  stlq,   (SELECT NVL(MAX(start_time),</span><span class=\"se\">\\\\''</span><span class=\"s1\">1902-01-01</span><span class=\"se\">\\\\''</span><span class=\"s1\">::TIMESTAMP) AS max_starttime FROM public.history_sysobjects where object_name=</span><span class=\"se\">\\\\''</span><span class=\"s1\">stl_query</span><span class=\"se\">\\\\''</span><span class=\"s1\">) h WHERE stlq.starttime &gt; h.max_starttime</span><span class=\"se\">\\'</span><span class=\"s1\">) to </span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"n\">s3_path</span><span class=\"o\">||</span><span class=\"s1\">'/stl_query/'</span><span class=\"o\">||</span><span class=\"n\">un_year</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">un_month</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">un_day</span><span class=\"o\">||</span><span class=\"s1\">'/stl_query_</span><span class=\"se\">\\'</span><span class=\"s1\"> iam_role </span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"n\">iam_role</span><span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">\\'</span><span class=\"s1\"> '</span><span class=\"o\">||</span><span class=\"n\">unload_options</span><span class=\"o\">||</span><span class=\"s1\">' delimiter </span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"k\">delimiter</span><span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"p\">;</span><span class=\"k\">execute</span> <span class=\"n\">stl_query_unload</span><span class=\"p\">;</span>     <span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"k\">public</span><span class=\"p\">.</span><span class=\"n\">history_sysobjects</span> <span class=\"p\">(</span><span class=\"n\">object_name</span><span class=\"p\">,</span><span class=\"n\">start_time</span><span class=\"p\">,</span><span class=\"n\">unload_query</span><span class=\"p\">)</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"s1\">'stl_query'</span><span class=\"p\">,</span><span class=\"n\">stlq_starttime</span><span class=\"p\">,</span><span class=\"n\">stl_query_unload</span><span class=\"p\">);</span><span class=\"n\">RAISE</span> <span class=\"n\">info</span> <span class=\"s1\">'Starting to unload stl_ddltext'</span><span class=\"p\">;</span><span class=\"n\">stl_ddltext_unload</span><span class=\"o\">=</span><span class=\"s1\">'unload(</span><span class=\"se\">\\'</span><span class=\"s1\">SELECT ddlt.userid,ddlt.pid,ddlt.label,ddlt.starttime, ddlt.endtime,ddlt.sequence,replace(replace(ddlt.text,</span><span class=\"se\">\\\\''\\\\\\\\\\\\\\\\</span><span class=\"s1\">n</span><span class=\"se\">\\\\''</span><span class=\"s1\">,</span><span class=\"se\">\\\\''\\\\''</span><span class=\"s1\">),</span><span class=\"se\">\\\\''\\\\\\\\\\\\\\\\</span><span class=\"s1\">r</span><span class=\"se\">\\\\''</span><span class=\"s1\">,</span><span class=\"se\">\\\\''\\\\''</span><span class=\"s1\">) as text FROM stl_ddltext ddlt,   (SELECT NVL(MAX(start_time),</span><span class=\"se\">\\\\''</span><span class=\"s1\">1902-01-01</span><span class=\"se\">\\\\''</span><span class=\"s1\">::TIMESTAMP) AS max_starttime FROM public.history_sysobjects where object_name=</span><span class=\"se\">\\\\''</span><span class=\"s1\">stl_ddltext</span><span class=\"se\">\\\\''</span><span class=\"s1\">) h WHERE ddlt.starttime &gt; h.max_starttime</span><span class=\"se\">\\'</span><span class=\"s1\">) to </span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"n\">s3_path</span><span class=\"o\">||</span><span class=\"s1\">'/stl_ddltext/'</span><span class=\"o\">||</span><span class=\"n\">un_year</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">un_month</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">un_day</span><span class=\"o\">||</span><span class=\"s1\">'/stl_ddltext_</span><span class=\"se\">\\'</span><span class=\"s1\"> iam_role </span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"n\">iam_role</span><span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">\\'</span><span class=\"s1\"> '</span><span class=\"o\">||</span><span class=\"n\">unload_options</span><span class=\"o\">||</span><span class=\"s1\">' delimiter </span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"k\">delimiter</span><span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"p\">;</span><span class=\"k\">execute</span> <span class=\"n\">stl_ddltext_unload</span><span class=\"p\">;</span>     <span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"k\">public</span><span class=\"p\">.</span><span class=\"n\">history_sysobjects</span> <span class=\"p\">(</span><span class=\"n\">object_name</span><span class=\"p\">,</span><span class=\"n\">start_time</span><span class=\"p\">,</span><span class=\"n\">unload_query</span><span class=\"p\">)</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"s1\">'stl_ddltext'</span><span class=\"p\">,</span><span class=\"n\">ddlt_starttime</span><span class=\"p\">,</span><span class=\"n\">stl_ddltext_unload</span><span class=\"p\">);</span><span class=\"n\">RAISE</span> <span class=\"n\">info</span> <span class=\"s1\">'Starting to unload stl_utilitytext'</span><span class=\"p\">;</span><span class=\"n\">stl_utilitytext_unload</span><span class=\"o\">=</span><span class=\"s1\">'unload(</span><span class=\"se\">\\'</span><span class=\"s1\">SELECT utlt.* FROM stl_utilitytext utlt, (SELECT NVL(MAX(start_time),</span><span class=\"se\">\\\\''</span><span class=\"s1\">1902-01-01</span><span class=\"se\">\\\\''</span><span class=\"s1\">::TIMESTAMP) AS max_starttime FROM public.history_sysobjects where object_name=</span><span class=\"se\">\\\\''</span><span class=\"s1\">stl_utilitytext</span><span class=\"se\">\\\\''</span><span class=\"s1\">) h WHERE utlt.starttime &gt; h.max_starttime</span><span class=\"se\">\\'</span><span class=\"s1\">) to </span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"n\">s3_path</span><span class=\"o\">||</span><span class=\"s1\">'/stl_utilitytext/'</span><span class=\"o\">||</span><span class=\"n\">un_year</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">un_month</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">un_day</span><span class=\"o\">||</span><span class=\"s1\">'/stl_utilitytext_</span><span class=\"se\">\\'</span><span class=\"s1\"> iam_role </span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"n\">iam_role</span><span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">\\'</span><span class=\"s1\"> '</span><span class=\"o\">||</span><span class=\"n\">unload_options</span><span class=\"o\">||</span><span class=\"s1\">' delimiter </span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"k\">delimiter</span><span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"p\">;</span><span class=\"k\">execute</span> <span class=\"n\">stl_utilitytext_unload</span><span class=\"p\">;</span>     <span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"k\">public</span><span class=\"p\">.</span><span class=\"n\">history_sysobjects</span> <span class=\"p\">(</span><span class=\"n\">object_name</span><span class=\"p\">,</span><span class=\"n\">start_time</span><span class=\"p\">,</span><span class=\"n\">unload_query</span><span class=\"p\">)</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"s1\">'stl_utilitytext'</span><span class=\"p\">,</span><span class=\"n\">utlt_starttime</span><span class=\"p\">,</span><span class=\"n\">stl_utilitytext_unload</span><span class=\"p\">);</span><span class=\"n\">RAISE</span> <span class=\"n\">info</span> <span class=\"s1\">'Starting to unload stl_querytext'</span><span class=\"p\">;</span><span class=\"n\">stl_querytext_unload</span><span class=\"o\">=</span><span class=\"s1\">'unload(</span><span class=\"se\">\\'</span><span class=\"s1\">select qtxt.userid,qtxt.xid,qtxt.pid,qtxt.query,qtxt.sequence,replace(replace(qtxt.text,</span><span class=\"se\">\\\\''\\\\\\\\\\\\\\\\</span><span class=\"s1\">n</span><span class=\"se\">\\\\''</span><span class=\"s1\">,</span><span class=\"se\">\\\\''\\\\''</span><span class=\"s1\">),</span><span class=\"se\">\\\\''\\\\\\\\\\\\\\\\</span><span class=\"s1\">r</span><span class=\"se\">\\\\''</span><span class=\"s1\">,</span><span class=\"se\">\\\\''\\\\''</span><span class=\"s1\">) as text from stl_querytext qtxt join stl_query qt on qt.userid=qtxt.userid and qt.query=qtxt.query and qt.xid=qtxt.xid and qt.pid=qtxt.pid where qt.starttime&gt;(SELECT NVL(MAX(start_time),</span><span class=\"se\">\\\\''</span><span class=\"s1\">1902-01-01</span><span class=\"se\">\\\\''</span><span class=\"s1\">::TIMESTAMP) AS max_starttime FROM public.history_sysobjects where object_name=</span><span class=\"se\">\\\\''</span><span class=\"s1\">stl_query</span><span class=\"se\">\\\\''</span><span class=\"s1\">)</span><span class=\"se\">\\'</span><span class=\"s1\">) to </span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"n\">s3_path</span><span class=\"o\">||</span><span class=\"s1\">'/stl_querytext/'</span><span class=\"o\">||</span><span class=\"n\">un_year</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">un_month</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">un_day</span><span class=\"o\">||</span><span class=\"s1\">'/stl_querytext_</span><span class=\"se\">\\'</span><span class=\"s1\"> iam_role </span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"n\">iam_role</span><span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">\\'</span><span class=\"s1\"> '</span><span class=\"o\">||</span><span class=\"n\">unload_options</span><span class=\"o\">||</span><span class=\"s1\">' delimiter </span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"k\">delimiter</span><span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"p\">;</span><span class=\"k\">execute</span> <span class=\"n\">stl_querytext_unload</span><span class=\"p\">;</span>     <span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"k\">public</span><span class=\"p\">.</span><span class=\"n\">history_sysobjects</span> <span class=\"p\">(</span><span class=\"n\">object_name</span><span class=\"p\">,</span><span class=\"n\">unload_query</span><span class=\"p\">)</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"s1\">'stl_querytext'</span><span class=\"p\">,</span><span class=\"n\">stl_querytext_unload</span><span class=\"p\">);</span><span class=\"n\">RAISE</span> <span class=\"n\">info</span> <span class=\"s1\">'Starting to unload stl_wlm_query'</span><span class=\"p\">;</span><span class=\"n\">stl_wlm_query_unload</span><span class=\"o\">=</span><span class=\"s1\">'unload(</span><span class=\"se\">\\'</span><span class=\"s1\">SELECT wq.* FROM stl_wlm_query wq, (SELECT NVL(MAX(start_time),</span><span class=\"se\">\\\\''</span><span class=\"s1\">1902-01-01</span><span class=\"se\">\\\\''</span><span class=\"s1\">::TIMESTAMP) AS max_service_class_start_time FROM public.history_sysobjects where object_name=</span><span class=\"se\">\\\\''</span><span class=\"s1\">stl_wlm_query</span><span class=\"se\">\\\\''</span><span class=\"s1\">) h WHERE wq.service_class_start_time &gt; h.max_service_class_start_time</span><span class=\"se\">\\'</span><span class=\"s1\">) to </span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"n\">s3_path</span><span class=\"o\">||</span><span class=\"s1\">'/stl_wlm_query/'</span><span class=\"o\">||</span><span class=\"n\">un_year</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">un_month</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">un_day</span><span class=\"o\">||</span><span class=\"s1\">'/stl_wlm_query_</span><span class=\"se\">\\'</span><span class=\"s1\"> iam_role </span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"n\">iam_role</span><span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">\\'</span><span class=\"s1\"> '</span><span class=\"o\">||</span><span class=\"n\">unload_options</span><span class=\"o\">||</span><span class=\"s1\">' delimiter </span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"k\">delimiter</span><span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"p\">;</span><span class=\"k\">execute</span> <span class=\"n\">stl_wlm_query_unload</span><span class=\"p\">;</span>     <span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"k\">public</span><span class=\"p\">.</span><span class=\"n\">history_sysobjects</span> <span class=\"p\">(</span><span class=\"n\">object_name</span><span class=\"p\">,</span><span class=\"n\">start_time</span><span class=\"p\">,</span><span class=\"n\">unload_query</span><span class=\"p\">)</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"s1\">'stl_wlm_query'</span><span class=\"p\">,</span><span class=\"n\">stlq_starttime</span><span class=\"p\">,</span><span class=\"n\">stl_wlm_query_unload</span><span class=\"p\">);</span><span class=\"n\">RAISE</span> <span class=\"n\">info</span> <span class=\"s1\">'Starting to unload stl_explain'</span><span class=\"p\">;</span><span class=\"n\">stl_explain_unload</span><span class=\"o\">=</span><span class=\"s1\">'unload(</span><span class=\"se\">\\'</span><span class=\"s1\">select exp.* from stl_explain exp join stl_query stlq on stlq.userid=exp.userid and stlq.query=exp.query where stlq.starttime&gt;(SELECT NVL(MAX(start_time),</span><span class=\"se\">\\\\''</span><span class=\"s1\">1902-01-01</span><span class=\"se\">\\\\''</span><span class=\"s1\">::TIMESTAMP) AS max_starttime FROM public.history_sysobjects where object_name=</span><span class=\"se\">\\\\''</span><span class=\"s1\">stl_query</span><span class=\"se\">\\\\''</span><span class=\"s1\">)</span><span class=\"se\">\\'</span><span class=\"s1\">) to </span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"n\">s3_path</span><span class=\"o\">||</span><span class=\"s1\">'/stl_explain/'</span><span class=\"o\">||</span><span class=\"n\">un_year</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">un_month</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">un_day</span><span class=\"o\">||</span><span class=\"s1\">'/stl_explain_</span><span class=\"se\">\\'</span><span class=\"s1\"> iam_role </span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"n\">iam_role</span><span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">\\'</span><span class=\"s1\"> '</span><span class=\"o\">||</span><span class=\"n\">unload_options</span><span class=\"o\">||</span><span class=\"s1\">' delimiter </span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"k\">delimiter</span><span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"p\">;</span><span class=\"k\">execute</span> <span class=\"n\">stl_explain_unload</span><span class=\"p\">;</span>     <span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"k\">public</span><span class=\"p\">.</span><span class=\"n\">history_sysobjects</span> <span class=\"p\">(</span><span class=\"n\">object_name</span><span class=\"p\">,</span><span class=\"n\">unload_query</span><span class=\"p\">)</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"s1\">'stl_explain'</span><span class=\"p\">,</span><span class=\"n\">stl_explain_unload</span><span class=\"p\">);</span><span class=\"n\">RAISE</span> <span class=\"n\">info</span> <span class=\"s1\">'Starting to unload svl_query_summary'</span><span class=\"p\">;</span><span class=\"n\">svl_query_summary_unload</span><span class=\"o\">=</span><span class=\"s1\">'unload(</span><span class=\"se\">\\'</span><span class=\"s1\">select qs.* from svl_query_summary qs join stl_query stlq on stlq.userid=qs.userid and stlq.query=qs.query  where stlq.starttime&gt;(SELECT NVL(MAX(start_time),</span><span class=\"se\">\\\\''</span><span class=\"s1\">1902-01-01</span><span class=\"se\">\\\\''</span><span class=\"s1\">::TIMESTAMP) AS max_starttime FROM public.history_sysobjects where object_name=</span><span class=\"se\">\\\\''</span><span class=\"s1\">stl_query</span><span class=\"se\">\\\\''</span><span class=\"s1\">)</span><span class=\"se\">\\'</span><span class=\"s1\">)  to </span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"n\">s3_path</span><span class=\"o\">||</span><span class=\"s1\">'/svl_query_summary/'</span><span class=\"o\">||</span><span class=\"n\">un_year</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">un_month</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">un_day</span><span class=\"o\">||</span><span class=\"s1\">'/svl_query_summary_</span><span class=\"se\">\\'</span><span class=\"s1\"> iam_role </span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"n\">iam_role</span><span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">\\'</span><span class=\"s1\"> '</span><span class=\"o\">||</span><span class=\"n\">unload_options</span><span class=\"o\">||</span><span class=\"s1\">' delimiter </span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"k\">delimiter</span><span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"p\">;</span><span class=\"k\">execute</span> <span class=\"n\">svl_query_summary_unload</span><span class=\"p\">;</span>     <span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"k\">public</span><span class=\"p\">.</span><span class=\"n\">history_sysobjects</span> <span class=\"p\">(</span><span class=\"n\">object_name</span><span class=\"p\">,</span><span class=\"n\">unload_query</span><span class=\"p\">)</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"s1\">'svl_query_summary'</span><span class=\"p\">,</span><span class=\"n\">svl_query_summary_unload</span><span class=\"p\">);</span><span class=\"n\">RAISE</span> <span class=\"n\">info</span> <span class=\"s1\">'Starting to unload stl_query_metrics'</span><span class=\"p\">;</span><span class=\"n\">stl_query_metrics_unload</span><span class=\"o\">=</span><span class=\"s1\">'unload(</span><span class=\"se\">\\'</span><span class=\"s1\">select qm.* from stl_query_metrics qm join stl_query stlq on stlq.userid=qm.userid and stlq.query=qm.query  where stlq.starttime&gt;(SELECT NVL(MAX(start_time),</span><span class=\"se\">\\\\''</span><span class=\"s1\">1902-01-01</span><span class=\"se\">\\\\''</span><span class=\"s1\">::TIMESTAMP) AS max_starttime FROM public.history_sysobjects where object_name=</span><span class=\"se\">\\\\''</span><span class=\"s1\">stl_query</span><span class=\"se\">\\\\''</span><span class=\"s1\">)</span><span class=\"se\">\\'</span><span class=\"s1\">) to </span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"n\">s3_path</span><span class=\"o\">||</span><span class=\"s1\">'/stl_query_metrics/'</span><span class=\"o\">||</span><span class=\"n\">un_year</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">un_month</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">un_day</span><span class=\"o\">||</span><span class=\"s1\">'/stl_query_metrics_</span><span class=\"se\">\\'</span><span class=\"s1\"> iam_role </span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"n\">iam_role</span><span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">\\'</span><span class=\"s1\"> '</span><span class=\"o\">||</span><span class=\"n\">unload_options</span><span class=\"o\">||</span><span class=\"s1\">' delimiter </span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"k\">delimiter</span><span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"p\">;</span><span class=\"k\">execute</span> <span class=\"n\">stl_query_metrics_unload</span><span class=\"p\">;</span>     <span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"k\">public</span><span class=\"p\">.</span><span class=\"n\">history_sysobjects</span> <span class=\"p\">(</span><span class=\"n\">object_name</span><span class=\"p\">,</span><span class=\"n\">unload_query</span><span class=\"p\">)</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"s1\">'stl_query_metrics'</span><span class=\"p\">,</span><span class=\"n\">stl_query_metrics_unload</span><span class=\"p\">);</span><span class=\"n\">RAISE</span> <span class=\"n\">info</span> <span class=\"s1\">'Starting to unload stl_alert_event_log'</span><span class=\"p\">;</span><span class=\"n\">stl_alert_event_log_unload</span><span class=\"o\">=</span><span class=\"s1\">'unload(</span><span class=\"se\">\\'</span><span class=\"s1\">select evnt.* from stl_alert_event_log evnt join stl_query qt on qt.userid=evnt.userid and qt.query=evnt.query and qt.xid=evnt.xid and qt.pid=evnt.pid  where qt.starttime&gt;(SELECT NVL(MAX(start_time),</span><span class=\"se\">\\\\''</span><span class=\"s1\">1902-01-01</span><span class=\"se\">\\\\''</span><span class=\"s1\">::TIMESTAMP) AS max_starttime FROM public.history_sysobjects where object_name=</span><span class=\"se\">\\\\''</span><span class=\"s1\">stl_query</span><span class=\"se\">\\\\''</span><span class=\"s1\">)</span><span class=\"se\">\\'</span><span class=\"s1\">) to </span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"n\">s3_path</span><span class=\"o\">||</span><span class=\"s1\">'/stl_alert_event_log/'</span><span class=\"o\">||</span><span class=\"n\">un_year</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">un_month</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">un_day</span><span class=\"o\">||</span><span class=\"s1\">'/stl_alert_event_log_</span><span class=\"se\">\\'</span><span class=\"s1\"> iam_role </span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"n\">iam_role</span><span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">\\'</span><span class=\"s1\"> '</span><span class=\"o\">||</span><span class=\"n\">unload_options</span><span class=\"o\">||</span><span class=\"s1\">' delimiter </span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"k\">delimiter</span><span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">\\'</span><span class=\"s1\">'</span><span class=\"p\">;</span><span class=\"k\">execute</span> <span class=\"n\">stl_alert_event_log_unload</span><span class=\"p\">;</span>     <span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"k\">public</span><span class=\"p\">.</span><span class=\"n\">history_sysobjects</span> <span class=\"p\">(</span><span class=\"n\">object_name</span><span class=\"p\">,</span><span class=\"n\">unload_query</span><span class=\"p\">)</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"s1\">'stl_alert_event_log'</span><span class=\"p\">,</span><span class=\"n\">stl_alert_event_log_unload</span><span class=\"p\">);</span><span class=\"k\">END</span><span class=\"err\">$$</span><span class=\"p\">;</span> </code></pre></figure>",
            "url": "/2020/04/06/export-redshift-system-tables-views-to-s3",
            "image": "/assets/Export RedShift System Tables And Views To S3.jpg",
            
            
            "tags": ["aws","redshift","sql","s3"],
            
            "date_published": "2020-04-06T01:15:00+00:00",
            "date_modified": "2020-04-06T01:15:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2020/04/05/analyze-redshift-useractivitylog-with-athena",
            "title": "Analyze RedShift user activity logs With Athena",
            "summary": "Analuze the RedShift User activity log(useractivitylog) files with Athena, You can remove the newline charactors with Lambda",
            "content_text": "A few of my recent blogs are concentrating on Analyzing RedShift queries. It seems its not a production critical issue or business challenge, but keeping your historical queries are very important for auditing. RedShift providing us 3 ways to see the query logging. But all are having some restrictions, so its very difficult to manage the right framework for analyzing the RedShift queries. We can keep the historical queries in S3, its a default feature. We can get all of our queries in a file named as User activity log(useractivitylogs). But its a plain text file, in other words, it’s an unstructured data. Now you understand where the problem is. Lets see the challenges with all these 3 ways.  useractivitylog in s3 - Completly unstructured, we can’t directly use this.  STL_QUERY - Great table, but if your query is huge in size, then it’ll truncate your query, so you’ll not get the complete query.  STL_QUERYTEXT - This table contains the full query, but unfortunately one single query split into multiple rows, so we need to concat all these rows into a single row. Again in RedShift concat is not available, instead, we can use LIST_AGG, but it’ll support up to  65k charactors in a single group.From the above three options, we can’t solve this issue with the help of RedShift, we need a different engine to solve this. So I picked AWS Athena which is cheaper. Now if you think which method will give you a complete query analyzing feature?  STL_QUERYTEXT - Need to perform CONCAT but the data is structured.  useractivitylog file - Unstructured, need some effort and customization to process it.Some references:useractivitylog files can we easily analyzed with pgbadger an opensource tool to analyze the PostgreSQL logs. But it’ll not give you all the metrics like query execution, etc. But applying more filters is not possible. To read about this approach click this likSTL_QUERYTEXT CONCAT process in RedShift with LIST_AGG also CONCAT process in Athena with ARRAY_AGG. But both methods are not full fledged solutions.That’s why I want to bring another solution where I can see the complete queries and play around with many filters like username, update queries, alter queries, etc.Solution Overview:  This file is also having many queries that will go more than a line, so you may see multiple new lines for a single query.We need to remove all of these new line charactors from all the log files.  Athena can’t directly scan these files from its default S3 location, because RedShift will export 3 different files at every 1hr, so Athena will fail to query only on the useractivitylog files.  Automate the whole steps for upcoming files as well.I read a blog from PMG where they did some customization on these log files and built their dashboard, but it helped me to understand the parsing the files and so many python codes, and more filter, but I don’t want to do all those things. I just took a piece of code to remove the newline characters from the log file.Approach:  Whenever the RedShift puts the log files to S3, use Lambda + S3 trigger to get the file and do the cleansing.  Upload the cleansed file to a new location.  Create the Athena table on the new location.  Create a view on top of the Athena table to split the single raw line to structured rows.Create the lambda function:Create a new lambda function with S3 Read permission to download the files and write permission to upload the cleansed file. No need to run this under a VPC. You have to change the following things as per your setup.  redshift-bucket - S3 bucket name where the RedShift is uploading the logs.  log_folder - S3 prefix where the log files are stored. (you need this while creating the S3 trigger)  custom-log-path - S3 prefix where the new cleaned will be uploaded.  file_name = key.split('/')[8] - In my s3 bucket the files are located with the following format.    s3://bucketname/log_prefix/AWSLogs/Account-ID/Cluster-name/region/YYYY/MM/DD/        From the the Prefix to DD folder I need to jump 8 Folders to reach my files, so I have given 8, if you use more than one folder as a RedShift Prefix, please count the folder and replace 8 with your value.import jsonimport urllibimport boto3import reimport gzip#S3 clients3 = boto3.client('s3')def lambda_handler(event, context):    bucket = event['Records'][0]['s3']['bucket']['name']    key = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'])        #get the file name from the key    file_pattern='_useractivitylog_'    file_name = key.split('/')[8]    key_path=key.replace(file_name,'')    if file_pattern in key:        with open('/tmp/'+file_name, 'wb') as f:                        #download the file            s3.download_fileobj('redshift-bucket', key, f)                        #extract the content from gzip and write to a new file            with gzip.open('/tmp/'+file_name, 'rb') as f,open('/tmp/custom'+file_name.replace('.gz',''), 'w') as fout:                file_content = str(f.read().decode('utf-8'))                fout.write(file_content)                                #read lines from the new file and repalce all new lines                 #Credits for this piece PMG.COM                with open('/tmp/custom'+file_name.replace('.gz',''), 'r', encoding='utf-8') as log_file:                    log_data = log_file.read().replace('\\n', ' ')                log_data = re.sub(r'(\\'\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z UTC)', '\\n \\\\1', log_data)                newlog = re.sub(r'^\\s*$', '', log_data)                                #write the formatter lines to a file                with open('/tmp/cleansed_'+file_name.replace('.gz','')+'.txt', 'w') as fout:                    fout.writelines(newlog)                                #upload the new file to S3                s3.upload_file('/tmp/cleansed_'+file_name.replace('.gz','')+'.txt', 'redshift-bucket', 'custom-log-path/'+key_path+file_name.replace('.gz','')+'.txt')    else:        print(\"Skipping\")Create the Athena Table:CREATE EXTERNAL TABLE `activitylog`(  `logtext` varchar(20000))ROW FORMAT DELIMITED   LINES TERMINATED BY '\\n' STORED AS INPUTFORMAT   'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT   'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'LOCATION  's3://redshift-bucket/custom-log-path/'Create View to split the row into columns:CREATE OR replace VIEW v_activitylog AS   SELECT Cast(\"Replace\"(\"Replace\"(\"Substring\"(\"logtext\", 3, (                                             \"Strpos\"(\"logtext\", 'UTC') - 3 )                                                  ), 'T',                                                  ' '), 'Z', '') AS TIMESTAMP)             \"timestamp\",          \"Replace\"(\"Substring\"(\"Substring\"(\"logtext\", (                                \"Strpos\"(\"logtext\", 'db=')                                + 3 ))                    , 1,                              \"Strpos\"(\"Substring\"(\"logtext\", (                                       \"Strpos\"(\"logtext\", 'db=')                                       + 3 )),                              ' ')), ' ', '')          \"db\"             ,          \"Replace\"(\"Substring\"(\"Substring\"(\"logtext\",                                ( \"Strpos\"(\"logtext\", 'user=') + 5                                )), 1,                              \"Strpos\"(\"Substring\"(\"logtext\", (                                       \"Strpos\"(\"logtext\", 'user=') + 5 )),                              ' ')), ' ', '')             \"user\",          Cast(\"Replace\"(\"Substring\"(\"Substring\"(\"logtext\", (                                     \"Strpos\"(\"logtext\", 'pid=')                                     + 4 )                                          ), 1,                                        \"Strpos\"(\"Substring\"(\"logtext\", (                                                 \"Strpos\"(\"logtext\", 'pid=') + 4                                                                        )), ' '))               ,               ' ', ''               ) AS INTEGER)             \"pid\",          Cast(\"Replace\"(\"Substring\"(\"Substring\"(\"logtext\", (                                     \"Strpos\"(\"logtext\", 'userid=') +                                          7 )), 1,                                        \"Strpos\"(\"Substring\"(\"logtext\", (                                                 \"Strpos\"(\"logtext\", 'userid=')                                                 + 7 )), ' ')), ' ', '') AS               INTEGER             )             \"userid\",          Cast(\"Replace\"(\"Substring\"(\"Substring\"(\"logtext\", (                                     \"Strpos\"(\"logtext\", 'xid=')                                     + 4 )                                          ), 1,                                        \"Strpos\"(\"Substring\"(\"logtext\", (                                                 \"Strpos\"(\"logtext\", 'xid=') + 4                                                                        )), ' '))               ,               ' ', ''               ) AS INTEGER)             \"xid\",          \"Substring\"(\"logtext\", ( \"Strpos\"(\"logtext\", 'LOG:') + 5 ))             \"query\"   FROM   activitylog   WHERE  logtext != ''; Query the Data:Everything is ready for analysis. Let’s run some sample queries.select * from v_activitylog limit 10;select * from v_activitylog where user!='rdsdb';select user,count(*)as count from v_activitylog group by user;Next Announcement:Here we are extracting the user, query, pid and everything with SQL operations which is a bit costly operation, but to leverge the Bigdata’s features we can use Gork pattern in Glue to crawl the data and create the table. Unfortunatly Im facing an issue with the Grok patten, may be I’ll publish that as a new blog, that will save your execution time.How about Spectrum?Yes, you can use the same DDL query to create your external table and (I hope everything will work fine there as well).",
            "content_html": "<p>A few of my recent blogs are concentrating on Analyzing RedShift queries. It seems its not a production critical issue or business challenge, but keeping your historical queries are very important for auditing. RedShift providing us 3 ways to see the query logging. But all are having some restrictions, so its very difficult to manage the right framework for analyzing the RedShift queries. We can keep the historical queries in S3, its a default feature. We can get all of our queries in a file named as User activity log(<code class=\"language-html highlighter-rouge\">useractivitylogs</code>). But its a plain text file, in other words, it’s an unstructured data. Now you understand where the problem is. Lets see the challenges with all these 3 ways.</p><ol>  <li><strong>useractivitylog in s3</strong> - Completly unstructured, we can’t directly use this.</li>  <li><strong>STL_QUERY</strong> - Great table, but if your query is huge in size, then it’ll truncate your query, so you’ll not get the complete query.</li>  <li><strong>STL_QUERYTEXT</strong> - This table contains the full query, but unfortunately one single query split into multiple rows, so we need to concat all these rows into a single row. Again in RedShift concat is not available, instead, we can use LIST_AGG, but it’ll support up to  65k charactors in a single group.</li></ol><p>From the above three options, we can’t solve this issue with the help of RedShift, we need a different engine to solve this. So I picked AWS Athena which is cheaper. Now if you think which method will give you a complete query analyzing feature?</p><ul>  <li>STL_QUERYTEXT - Need to perform CONCAT but the data is structured.</li>  <li>useractivitylog file - Unstructured, need some effort and customization to process it.</li></ul><h2 id=\"some-references\">Some references:</h2><p><code class=\"language-html highlighter-rouge\">useractivitylog</code> files can we easily analyzed with pgbadger an opensource tool to analyze the PostgreSQL logs. But it’ll not give you all the metrics like query execution, etc. But applying more filters is not possible. To read about this approach <a href=\"https://medium.com/searce/audit-redshift-historical-queries-with-pgbadger-619f7f43fbd0\">click this lik</a></p><p><code class=\"language-html highlighter-rouge\">STL_QUERYTEXT</code> CONCAT process in RedShift with <a href=\"https://thedataguy.in/redshift-reconstructing-sql-from-sql-querytext/\">LIST_AGG</a> also CONCAT process in Athena with <a href=\"https://thedataguy.in/reconstruct-redshift-stl-querytext-using-aws-athena/\">ARRAY_AGG</a>. But both methods are not full fledged solutions.</p><p>That’s why I want to bring another solution where I can see the complete queries and play around with many filters like username, update queries, alter queries, etc.</p><h2 id=\"solution-overview\">Solution Overview:</h2><ol>  <li>This file is also having many queries that will go more than a line, so you may see multiple new lines for a single query.We need to remove all of these new line charactors from all the log files.</li>  <li>Athena can’t directly scan these files from its default S3 location, because RedShift will export 3 different files at every 1hr, so Athena will fail to query only on the useractivitylog files.</li>  <li>Automate the whole steps for upcoming files as well.</li></ol><p>I read a blog from <a href=\"https://www.pmg.com/blog/parsing-redshift-logs-to-understand-data-usage/\">PMG</a> where they did some customization on these log files and built their dashboard, but it helped me to understand the parsing the files and so many python codes, and more filter, but I don’t want to do all those things. I just took a piece of code to remove the newline characters from the log file.</p><h1 id=\"approach\">Approach:</h1><ul>  <li>Whenever the RedShift puts the log files to S3, use <code class=\"language-html highlighter-rouge\">Lambda + S3</code> trigger to get the file and do the cleansing.</li>  <li>Upload the cleansed file to a new location.</li>  <li>Create the Athena table on the new location.</li>  <li>Create a view on top of the Athena table to split the single raw line to structured rows.</li></ul><h2 id=\"create-the-lambda-function\">Create the lambda function:</h2><p>Create a new lambda function with S3 Read permission to download the files and write permission to upload the cleansed file. No need to run this under a VPC. You have to change the following things as per your setup.</p><ul>  <li>redshift-bucket - S3 bucket name where the RedShift is uploading the logs.</li>  <li>log_folder - S3 prefix where the log files are stored. (you need this while creating the S3 trigger)</li>  <li>custom-log-path - S3 prefix where the new cleaned will be uploaded.</li>  <li><code class=\"language-html highlighter-rouge\">file_name = key.split('/')[8]</code> - In my s3 bucket the files are located with the following format.    <div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>s3://bucketname/log_prefix/AWSLogs/Account-ID/Cluster-name/region/YYYY/MM/DD/</code></pre></div>    </div>  </li>  <li>From the the Prefix to DD folder I need to jump 8 Folders to reach my files, so I have given 8, if you use more than one folder as a RedShift Prefix, please count the folder and replace 8 with your value.</li></ul><figure class=\"highlight\"><pre><code class=\"language-python\" data-lang=\"python\"><span class=\"kn\">import</span> <span class=\"nn\">json</span><span class=\"kn\">import</span> <span class=\"nn\">urllib</span><span class=\"kn\">import</span> <span class=\"nn\">boto3</span><span class=\"kn\">import</span> <span class=\"nn\">re</span><span class=\"kn\">import</span> <span class=\"nn\">gzip</span><span class=\"c1\">#S3 client</span><span class=\"n\">s3</span> <span class=\"o\">=</span> <span class=\"n\">boto3</span><span class=\"p\">.</span><span class=\"n\">client</span><span class=\"p\">(</span><span class=\"s\">'s3'</span><span class=\"p\">)</span><span class=\"k\">def</span> <span class=\"nf\">lambda_handler</span><span class=\"p\">(</span><span class=\"n\">event</span><span class=\"p\">,</span> <span class=\"n\">context</span><span class=\"p\">):</span>    <span class=\"n\">bucket</span> <span class=\"o\">=</span> <span class=\"n\">event</span><span class=\"p\">[</span><span class=\"s\">'Records'</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"s\">'s3'</span><span class=\"p\">][</span><span class=\"s\">'bucket'</span><span class=\"p\">][</span><span class=\"s\">'name'</span><span class=\"p\">]</span>    <span class=\"n\">key</span> <span class=\"o\">=</span> <span class=\"n\">urllib</span><span class=\"p\">.</span><span class=\"n\">parse</span><span class=\"p\">.</span><span class=\"n\">unquote_plus</span><span class=\"p\">(</span><span class=\"n\">event</span><span class=\"p\">[</span><span class=\"s\">'Records'</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"s\">'s3'</span><span class=\"p\">][</span><span class=\"s\">'object'</span><span class=\"p\">][</span><span class=\"s\">'key'</span><span class=\"p\">])</span>        <span class=\"c1\">#get the file name from the key</span>    <span class=\"n\">file_pattern</span><span class=\"o\">=</span><span class=\"s\">'_useractivitylog_'</span>    <span class=\"n\">file_name</span> <span class=\"o\">=</span> <span class=\"n\">key</span><span class=\"p\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"s\">'/'</span><span class=\"p\">)[</span><span class=\"mi\">8</span><span class=\"p\">]</span>    <span class=\"n\">key_path</span><span class=\"o\">=</span><span class=\"n\">key</span><span class=\"p\">.</span><span class=\"n\">replace</span><span class=\"p\">(</span><span class=\"n\">file_name</span><span class=\"p\">,</span><span class=\"s\">''</span><span class=\"p\">)</span>    <span class=\"k\">if</span> <span class=\"n\">file_pattern</span> <span class=\"ow\">in</span> <span class=\"n\">key</span><span class=\"p\">:</span>        <span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s\">'/tmp/'</span><span class=\"o\">+</span><span class=\"n\">file_name</span><span class=\"p\">,</span> <span class=\"s\">'wb'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>                        <span class=\"c1\">#download the file</span>            <span class=\"n\">s3</span><span class=\"p\">.</span><span class=\"n\">download_fileobj</span><span class=\"p\">(</span><span class=\"s\">'redshift-bucket'</span><span class=\"p\">,</span> <span class=\"n\">key</span><span class=\"p\">,</span> <span class=\"n\">f</span><span class=\"p\">)</span>                        <span class=\"c1\">#extract the content from gzip and write to a new file</span>            <span class=\"k\">with</span> <span class=\"n\">gzip</span><span class=\"p\">.</span><span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s\">'/tmp/'</span><span class=\"o\">+</span><span class=\"n\">file_name</span><span class=\"p\">,</span> <span class=\"s\">'rb'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">,</span><span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s\">'/tmp/custom'</span><span class=\"o\">+</span><span class=\"n\">file_name</span><span class=\"p\">.</span><span class=\"n\">replace</span><span class=\"p\">(</span><span class=\"s\">'.gz'</span><span class=\"p\">,</span><span class=\"s\">''</span><span class=\"p\">),</span> <span class=\"s\">'w'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">fout</span><span class=\"p\">:</span>                <span class=\"n\">file_content</span> <span class=\"o\">=</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">.</span><span class=\"n\">read</span><span class=\"p\">().</span><span class=\"n\">decode</span><span class=\"p\">(</span><span class=\"s\">'utf-8'</span><span class=\"p\">))</span>                <span class=\"n\">fout</span><span class=\"p\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">file_content</span><span class=\"p\">)</span>                                <span class=\"c1\">#read lines from the new file and repalce all new lines </span>                <span class=\"c1\">#Credits for this piece PMG.COM</span>                <span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s\">'/tmp/custom'</span><span class=\"o\">+</span><span class=\"n\">file_name</span><span class=\"p\">.</span><span class=\"n\">replace</span><span class=\"p\">(</span><span class=\"s\">'.gz'</span><span class=\"p\">,</span><span class=\"s\">''</span><span class=\"p\">),</span> <span class=\"s\">'r'</span><span class=\"p\">,</span> <span class=\"n\">encoding</span><span class=\"o\">=</span><span class=\"s\">'utf-8'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">log_file</span><span class=\"p\">:</span>                    <span class=\"n\">log_data</span> <span class=\"o\">=</span> <span class=\"n\">log_file</span><span class=\"p\">.</span><span class=\"n\">read</span><span class=\"p\">().</span><span class=\"n\">replace</span><span class=\"p\">(</span><span class=\"s\">'</span><span class=\"si\">\\</span><span class=\"se\">n</span><span class=\"s\">'</span><span class=\"p\">,</span> <span class=\"s\">' '</span><span class=\"p\">)</span>                <span class=\"n\">log_data</span> <span class=\"o\">=</span> <span class=\"n\">re</span><span class=\"p\">.</span><span class=\"n\">sub</span><span class=\"p\">(</span><span class=\"s\">r'(\\'\\</span><span class=\"err\">d{</span><span class=\"s\">4}-\\</span><span class=\"err\">d{</span><span class=\"s\">2}-\\</span><span class=\"err\">d{</span><span class=\"s\">2}T\\</span><span class=\"err\">d{</span><span class=\"s\">2}:\\</span><span class=\"err\">d{</span><span class=\"s\">2}:\\</span><span class=\"err\">d{</span><span class=\"s\">2}Z UTC)'</span><span class=\"p\">,</span> <span class=\"s\">'</span><span class=\"si\">\\</span><span class=\"se\">n</span><span class=\"s\"> </span><span class=\"si\">\\</span><span class=\"se\">\\</span><span class=\"s\">1'</span><span class=\"p\">,</span> <span class=\"n\">log_data</span><span class=\"p\">)</span>                <span class=\"n\">newlog</span> <span class=\"o\">=</span> <span class=\"n\">re</span><span class=\"p\">.</span><span class=\"n\">sub</span><span class=\"p\">(</span><span class=\"s\">r'^\\</span><span class=\"err\">s*$</span><span class=\"s\">', '', log_data)                                #write the formatter lines to a file                with open('</span><span class=\"o\">/</span><span class=\"n\">tmp</span><span class=\"o\">/</span><span class=\"n\">cleansed_</span><span class=\"s\">'+file_name.replace('</span><span class=\"p\">.</span><span class=\"n\">gz</span><span class=\"s\">','')+'</span><span class=\"p\">.</span><span class=\"n\">txt</span><span class=\"s\">', '</span><span class=\"n\">w</span><span class=\"s\">') as fout:                    fout.writelines(newlog)                                #upload the new file to S3                s3.upload_file('</span><span class=\"o\">/</span><span class=\"n\">tmp</span><span class=\"o\">/</span><span class=\"n\">cleansed_</span><span class=\"s\">'+file_name.replace('</span><span class=\"p\">.</span><span class=\"n\">gz</span><span class=\"s\">','')+'</span><span class=\"p\">.</span><span class=\"n\">txt</span><span class=\"s\">', '</span><span class=\"n\">redshift</span><span class=\"o\">-</span><span class=\"n\">bucket</span><span class=\"s\">', '</span><span class=\"n\">custom</span><span class=\"o\">-</span><span class=\"n\">log</span><span class=\"o\">-</span><span class=\"n\">path</span><span class=\"o\">/</span><span class=\"s\">'+key_path+file_name.replace('</span><span class=\"p\">.</span><span class=\"n\">gz</span><span class=\"s\">','')+'</span><span class=\"p\">.</span><span class=\"n\">txt</span><span class=\"s\">')    else:        print(\"Skipping\")</span></code></pre></figure><h2 id=\"create-the-athena-table\">Create the Athena Table:</h2><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">CREATE</span> <span class=\"k\">EXTERNAL</span> <span class=\"k\">TABLE</span> <span class=\"nv\">`activitylog`</span><span class=\"p\">(</span>  <span class=\"nv\">`logtext`</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">20000</span><span class=\"p\">))</span><span class=\"k\">ROW</span> <span class=\"n\">FORMAT</span> <span class=\"n\">DELIMITED</span>   <span class=\"n\">LINES</span> <span class=\"n\">TERMINATED</span> <span class=\"k\">BY</span> <span class=\"s1\">'</span><span class=\"se\">\\n</span><span class=\"s1\">'</span> <span class=\"n\">STORED</span> <span class=\"k\">AS</span> <span class=\"n\">INPUTFORMAT</span>   <span class=\"s1\">'org.apache.hadoop.mapred.TextInputFormat'</span> <span class=\"n\">OUTPUTFORMAT</span>   <span class=\"s1\">'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'</span><span class=\"k\">LOCATION</span>  <span class=\"s1\">'s3://redshift-bucket/custom-log-path/'</span></code></pre></figure><h2 id=\"create-view-to-split-the-row-into-columns\">Create View to split the row into columns:</h2><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">CREATE</span> <span class=\"k\">OR</span> <span class=\"k\">replace</span> <span class=\"k\">VIEW</span> <span class=\"n\">v_activitylog</span> <span class=\"k\">AS</span>   <span class=\"k\">SELECT</span> <span class=\"k\">Cast</span><span class=\"p\">(</span><span class=\"nv\">\"Replace\"</span><span class=\"p\">(</span><span class=\"nv\">\"Replace\"</span><span class=\"p\">(</span><span class=\"nv\">\"Substring\"</span><span class=\"p\">(</span><span class=\"nv\">\"logtext\"</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"p\">(</span>                                             <span class=\"nv\">\"Strpos\"</span><span class=\"p\">(</span><span class=\"nv\">\"logtext\"</span><span class=\"p\">,</span> <span class=\"s1\">'UTC'</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"mi\">3</span> <span class=\"p\">)</span>                                                  <span class=\"p\">),</span> <span class=\"s1\">'T'</span><span class=\"p\">,</span>                                                  <span class=\"s1\">' '</span><span class=\"p\">),</span> <span class=\"s1\">'Z'</span><span class=\"p\">,</span> <span class=\"s1\">''</span><span class=\"p\">)</span> <span class=\"k\">AS</span> <span class=\"nb\">TIMESTAMP</span><span class=\"p\">)</span>             <span class=\"nv\">\"timestamp\"</span><span class=\"p\">,</span>          <span class=\"nv\">\"Replace\"</span><span class=\"p\">(</span><span class=\"nv\">\"Substring\"</span><span class=\"p\">(</span><span class=\"nv\">\"Substring\"</span><span class=\"p\">(</span><span class=\"nv\">\"logtext\"</span><span class=\"p\">,</span> <span class=\"p\">(</span>                                <span class=\"nv\">\"Strpos\"</span><span class=\"p\">(</span><span class=\"nv\">\"logtext\"</span><span class=\"p\">,</span> <span class=\"s1\">'db='</span><span class=\"p\">)</span>                                <span class=\"o\">+</span> <span class=\"mi\">3</span> <span class=\"p\">))</span>                    <span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span>                              <span class=\"nv\">\"Strpos\"</span><span class=\"p\">(</span><span class=\"nv\">\"Substring\"</span><span class=\"p\">(</span><span class=\"nv\">\"logtext\"</span><span class=\"p\">,</span> <span class=\"p\">(</span>                                       <span class=\"nv\">\"Strpos\"</span><span class=\"p\">(</span><span class=\"nv\">\"logtext\"</span><span class=\"p\">,</span> <span class=\"s1\">'db='</span><span class=\"p\">)</span>                                       <span class=\"o\">+</span> <span class=\"mi\">3</span> <span class=\"p\">)),</span>                              <span class=\"s1\">' '</span><span class=\"p\">)),</span> <span class=\"s1\">' '</span><span class=\"p\">,</span> <span class=\"s1\">''</span><span class=\"p\">)</span>          <span class=\"nv\">\"db\"</span>             <span class=\"p\">,</span>          <span class=\"nv\">\"Replace\"</span><span class=\"p\">(</span><span class=\"nv\">\"Substring\"</span><span class=\"p\">(</span><span class=\"nv\">\"Substring\"</span><span class=\"p\">(</span><span class=\"nv\">\"logtext\"</span><span class=\"p\">,</span>                                <span class=\"p\">(</span> <span class=\"nv\">\"Strpos\"</span><span class=\"p\">(</span><span class=\"nv\">\"logtext\"</span><span class=\"p\">,</span> <span class=\"s1\">'user='</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"mi\">5</span>                                <span class=\"p\">)),</span> <span class=\"mi\">1</span><span class=\"p\">,</span>                              <span class=\"nv\">\"Strpos\"</span><span class=\"p\">(</span><span class=\"nv\">\"Substring\"</span><span class=\"p\">(</span><span class=\"nv\">\"logtext\"</span><span class=\"p\">,</span> <span class=\"p\">(</span>                                       <span class=\"nv\">\"Strpos\"</span><span class=\"p\">(</span><span class=\"nv\">\"logtext\"</span><span class=\"p\">,</span> <span class=\"s1\">'user='</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"mi\">5</span> <span class=\"p\">)),</span>                              <span class=\"s1\">' '</span><span class=\"p\">)),</span> <span class=\"s1\">' '</span><span class=\"p\">,</span> <span class=\"s1\">''</span><span class=\"p\">)</span>             <span class=\"nv\">\"user\"</span><span class=\"p\">,</span>          <span class=\"k\">Cast</span><span class=\"p\">(</span><span class=\"nv\">\"Replace\"</span><span class=\"p\">(</span><span class=\"nv\">\"Substring\"</span><span class=\"p\">(</span><span class=\"nv\">\"Substring\"</span><span class=\"p\">(</span><span class=\"nv\">\"logtext\"</span><span class=\"p\">,</span> <span class=\"p\">(</span>                                     <span class=\"nv\">\"Strpos\"</span><span class=\"p\">(</span><span class=\"nv\">\"logtext\"</span><span class=\"p\">,</span> <span class=\"s1\">'pid='</span><span class=\"p\">)</span>                                     <span class=\"o\">+</span> <span class=\"mi\">4</span> <span class=\"p\">)</span>                                          <span class=\"p\">),</span> <span class=\"mi\">1</span><span class=\"p\">,</span>                                        <span class=\"nv\">\"Strpos\"</span><span class=\"p\">(</span><span class=\"nv\">\"Substring\"</span><span class=\"p\">(</span><span class=\"nv\">\"logtext\"</span><span class=\"p\">,</span> <span class=\"p\">(</span>                                                 <span class=\"nv\">\"Strpos\"</span><span class=\"p\">(</span><span class=\"nv\">\"logtext\"</span><span class=\"p\">,</span> <span class=\"s1\">'pid='</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"mi\">4</span>                                                                        <span class=\"p\">)),</span> <span class=\"s1\">' '</span><span class=\"p\">))</span>               <span class=\"p\">,</span>               <span class=\"s1\">' '</span><span class=\"p\">,</span> <span class=\"s1\">''</span>               <span class=\"p\">)</span> <span class=\"k\">AS</span> <span class=\"nb\">INTEGER</span><span class=\"p\">)</span>             <span class=\"nv\">\"pid\"</span><span class=\"p\">,</span>          <span class=\"k\">Cast</span><span class=\"p\">(</span><span class=\"nv\">\"Replace\"</span><span class=\"p\">(</span><span class=\"nv\">\"Substring\"</span><span class=\"p\">(</span><span class=\"nv\">\"Substring\"</span><span class=\"p\">(</span><span class=\"nv\">\"logtext\"</span><span class=\"p\">,</span> <span class=\"p\">(</span>                                     <span class=\"nv\">\"Strpos\"</span><span class=\"p\">(</span><span class=\"nv\">\"logtext\"</span><span class=\"p\">,</span> <span class=\"s1\">'userid='</span><span class=\"p\">)</span> <span class=\"o\">+</span>                                          <span class=\"mi\">7</span> <span class=\"p\">)),</span> <span class=\"mi\">1</span><span class=\"p\">,</span>                                        <span class=\"nv\">\"Strpos\"</span><span class=\"p\">(</span><span class=\"nv\">\"Substring\"</span><span class=\"p\">(</span><span class=\"nv\">\"logtext\"</span><span class=\"p\">,</span> <span class=\"p\">(</span>                                                 <span class=\"nv\">\"Strpos\"</span><span class=\"p\">(</span><span class=\"nv\">\"logtext\"</span><span class=\"p\">,</span> <span class=\"s1\">'userid='</span><span class=\"p\">)</span>                                                 <span class=\"o\">+</span> <span class=\"mi\">7</span> <span class=\"p\">)),</span> <span class=\"s1\">' '</span><span class=\"p\">)),</span> <span class=\"s1\">' '</span><span class=\"p\">,</span> <span class=\"s1\">''</span><span class=\"p\">)</span> <span class=\"k\">AS</span>               <span class=\"nb\">INTEGER</span>             <span class=\"p\">)</span>             <span class=\"nv\">\"userid\"</span><span class=\"p\">,</span>          <span class=\"k\">Cast</span><span class=\"p\">(</span><span class=\"nv\">\"Replace\"</span><span class=\"p\">(</span><span class=\"nv\">\"Substring\"</span><span class=\"p\">(</span><span class=\"nv\">\"Substring\"</span><span class=\"p\">(</span><span class=\"nv\">\"logtext\"</span><span class=\"p\">,</span> <span class=\"p\">(</span>                                     <span class=\"nv\">\"Strpos\"</span><span class=\"p\">(</span><span class=\"nv\">\"logtext\"</span><span class=\"p\">,</span> <span class=\"s1\">'xid='</span><span class=\"p\">)</span>                                     <span class=\"o\">+</span> <span class=\"mi\">4</span> <span class=\"p\">)</span>                                          <span class=\"p\">),</span> <span class=\"mi\">1</span><span class=\"p\">,</span>                                        <span class=\"nv\">\"Strpos\"</span><span class=\"p\">(</span><span class=\"nv\">\"Substring\"</span><span class=\"p\">(</span><span class=\"nv\">\"logtext\"</span><span class=\"p\">,</span> <span class=\"p\">(</span>                                                 <span class=\"nv\">\"Strpos\"</span><span class=\"p\">(</span><span class=\"nv\">\"logtext\"</span><span class=\"p\">,</span> <span class=\"s1\">'xid='</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"mi\">4</span>                                                                        <span class=\"p\">)),</span> <span class=\"s1\">' '</span><span class=\"p\">))</span>               <span class=\"p\">,</span>               <span class=\"s1\">' '</span><span class=\"p\">,</span> <span class=\"s1\">''</span>               <span class=\"p\">)</span> <span class=\"k\">AS</span> <span class=\"nb\">INTEGER</span><span class=\"p\">)</span>             <span class=\"nv\">\"xid\"</span><span class=\"p\">,</span>          <span class=\"nv\">\"Substring\"</span><span class=\"p\">(</span><span class=\"nv\">\"logtext\"</span><span class=\"p\">,</span> <span class=\"p\">(</span> <span class=\"nv\">\"Strpos\"</span><span class=\"p\">(</span><span class=\"nv\">\"logtext\"</span><span class=\"p\">,</span> <span class=\"s1\">'LOG:'</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"mi\">5</span> <span class=\"p\">))</span>             <span class=\"nv\">\"query\"</span>   <span class=\"k\">FROM</span>   <span class=\"n\">activitylog</span>   <span class=\"k\">WHERE</span>  <span class=\"n\">logtext</span> <span class=\"o\">!=</span> <span class=\"s1\">''</span><span class=\"p\">;</span> </code></pre></figure><h2 id=\"query-the-data\">Query the Data:</h2><p>Everything is ready for analysis. Let’s run some sample queries.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">select</span> <span class=\"o\">*</span> <span class=\"k\">from</span> <span class=\"n\">v_activitylog</span> <span class=\"k\">limit</span> <span class=\"mi\">10</span><span class=\"p\">;</span></code></pre></figure><p><img src=\"/assets/Analyze RedShift user activity logs With Athena1.jpg\" alt=\"\" /></p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">select</span> <span class=\"o\">*</span> <span class=\"k\">from</span> <span class=\"n\">v_activitylog</span> <span class=\"k\">where</span> <span class=\"k\">user</span><span class=\"o\">!=</span><span class=\"s1\">'rdsdb'</span><span class=\"p\">;</span></code></pre></figure><p><img src=\"/assets/Analyze RedShift user activity logs With Athena2.jpg\" alt=\"\" /></p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">select</span> <span class=\"k\">user</span><span class=\"p\">,</span><span class=\"k\">count</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"p\">)</span><span class=\"k\">as</span> <span class=\"k\">count</span> <span class=\"k\">from</span> <span class=\"n\">v_activitylog</span> <span class=\"k\">group</span> <span class=\"k\">by</span> <span class=\"k\">user</span><span class=\"p\">;</span></code></pre></figure><p><img src=\"/assets/Analyze RedShift user activity logs With Athena3.jpg\" alt=\"\" /></p><h2 id=\"next-announcement\">Next Announcement:</h2><p>Here we are extracting the user, query, pid and everything with SQL operations which is a bit costly operation, but to leverge the Bigdata’s features we can use Gork pattern in Glue to crawl the data and create the table. Unfortunatly Im facing an issue with the Grok patten, may be I’ll publish that as a new blog, that will save your execution time.</p><h2 id=\"how-about-spectrum\">How about Spectrum?</h2><p>Yes, you can use the same DDL query to create your external table and (I hope everything will work fine there as well).</p>",
            "url": "/2020/04/05/analyze-redshift-useractivitylog-with-athena",
            "image": "/assets/Analyze RedShift user activity logs With Athena.jpg",
            
            
            "tags": ["aws","redshift","sql","athena","lambda"],
            
            "date_published": "2020-04-05T01:15:00+00:00",
            "date_modified": "2020-04-05T01:15:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2020/03/25/reconstruct-redshift-stl-querytext-using-aws-athena",
            "title": "Reconstruct RedShift STL_QUERYTEXT using AWS Athena",
            "summary": "Reconstruct all SQL queries from STL_QUERYTEXT with AWS Athena",
            "content_text": "In my last post, I have experimented to reconstruct the complete stl_querytext table in RedShift using LISTAGG() function. But there is a limitation in that. If you read AWS Doc for LISTAGG, you can’t group the rows which are more than 65535 characters. What happens if we have tens of thousands of lines of SQL queries? It’ll throw the limitation error for sure. I didn’t find a much better solution in RedShift to solve this. So I decided to use other services which are cheaper to do this work. And that’s why you are here. Yes, this time I used AWS Athena an interactive analytics platform where we can query the data in an S3 bucket directly. Lets see how to reconstruct all the queries in stl_querytext table using Athena.Unload the data:Athena can’t use the RedShift directly to query the data, we have to export the data into S3 bucket. Here we are going to do the same. But a minor change, the data in stl_querytext table has some special characters like \\r and \\n t what ill break our process in Athena. So while exporting the data, we have to replace then with whitespace.Replace(Replace(text, '\\r', ''), '\\\\n', '') AS text But unfortunately, I was not able to use this SQL syntax in unload command, it didn’t replace anything.  (I have request AWS Support to solve this, I’ll this blog once I got the solution), but we can create a view on top of the stl_querytext to replace these characters.CREATE VIEW v_stl_querytext AS   (SELECT userid,           xid,           pid,           query,           SEQUENCE,           Replace(Replace(text, '\\r', ''), '\\\\n', '') AS text    FROM   stl_querytext) Now lest unload the data to S3 bucket, make sure you have a proper IAM role to access the S3 bucket and change the bucket name and path as per your need.unload ('select * from  v_stl_querytext') to 's3://mybucket/querytxt/querytxt.csv' iam_role 'arn:aws:iam::1111111111:role/Access-S3' REGION 'ap-south-1' HEADER delimiter '|' ADDQUOTES;Update: 2020-03-30Instead of creating the view, you can use the following query to unload the data.-- Credit: AWS teamunload ('SELECT userid,        xid,        pid,        query,        sequence,        replace(replace(text,        \\'\\\\\\\\n\\',\\'\\'),\\'\\\\\\\\r\\',\\'\\') AS txtFROM stl_querytext') to 's3://mybucket/querytxt/querytxt.csv' iam_role 'arn:aws:iam::1111111111:role/Access-S3' REGION 'ap-south-1' HEADER delimiter '|' ADDQUOTES;If you want to add compress, please enable it, which will improve your Athena query performance.Athena Table:Now lets go and create a table in Athena to query the data in S3.CREATE EXTERNAL TABLE `stl_querytext`(  `userid` string COMMENT 'from deserializer',   `xid` string COMMENT 'from deserializer',   `pid` string COMMENT 'from deserializer',   `query` string COMMENT 'from deserializer',   `sequence` string COMMENT 'from deserializer',   `text` string COMMENT 'from deserializer')ROW FORMAT SERDE   'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES (   'escapeChar'='\\\\',   'quoteChar'='\\\"',   'separatorChar'='|') STORED AS INPUTFORMAT   'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT   'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'LOCATION  's3://mybucket/querytxt/'TBLPROPERTIES (  'skip.header.line.count'='1') We are almost done, now let’s write some SQL queries to reconstruct the stl_querytext table.Reconstruct the SQL Queries:with cte as    (SELECT userid,         xid,        pid,        query,         sequence,         text,        row_number()        OVER (PARTITION BY userid, xid,pid,query    ORDER BY  sequence) AS rank    FROM stl_querytext    GROUP BY  userid, xid,pid,query,sequence,text    ORDER BY  rank)SELECT userid,         xid,        pid,        query,        array_join(array_agg(text),         '') as textFROM cteGROUP BY  userid, xid,pid,queryORDER BY  queryOhhh!!! Im impressed by its performance, Last time, when  I perform the same activity on RedShift some times it took a min to complete, but Athena did a great job. Try it out and comment below if you face any issues.Update: 2020-03-30I faced an issue with Athena recently, what happened is the array_agg() is not maintaining the query order. So some queries may be incorrect in the result reset. We can solve them by using array_join(array_agg(text ORDER BY  sequence), '') AS text and again this syntax will not support in Athena because it 1.7x version. Im not sure when AWS is going to update the Athena’s presto verison. But if you want to use presto in EMR, it’ll work. Use the following query.with cte AS     (SELECT userid,         xid,         pid,         query,         sequence,         text,         row_number()        OVER (PARTITION BY userid, xid,pid,query    ORDER BY  sequence) AS rank    FROM stl_querytext    GROUP BY  userid, xid,pid,query,sequence,text    ORDER BY  rank)SELECT userid,         xid,         pid,         query,         array_join(array_agg(text ORDER BY  sequence), '') AS textFROM cteGROUP BY  userid, xid,pid,queryORDER BY  query;",
            "content_html": "<p><a href=\"https://thedataguy.in/redshift-reconstructing-sql-from-sql-querytext/\">In my last post</a>, I have experimented to reconstruct the complete <code class=\"language-html highlighter-rouge\">stl_querytext</code> table in RedShift using <code class=\"language-html highlighter-rouge\">LISTAGG()</code> function. But there is a limitation in that. If you read AWS Doc for LISTAGG, you can’t group the rows which are more than 65535 characters. What happens if we have tens of thousands of lines of SQL queries? It’ll throw the limitation error for sure. I didn’t find a much better solution in RedShift to solve this. So I decided to use other services which are cheaper to do this work. And that’s why you are here. Yes, this time I used AWS Athena an interactive analytics platform where we can query the data in an S3 bucket directly. Lets see how to reconstruct all the queries in <code class=\"language-html highlighter-rouge\">stl_querytext</code> table using Athena.</p><h2 id=\"unload-the-data\">Unload the data:</h2><p>Athena can’t use the RedShift directly to query the data, we have to export the data into S3 bucket. Here we are going to do the same. But a minor change, the data in stl_querytext table has some special characters like <code class=\"language-html highlighter-rouge\">\\r</code> and <code class=\"language-html highlighter-rouge\">\\n</code> t what ill break our process in Athena. So while exporting the data, we have to replace then with whitespace.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">Replace</span><span class=\"p\">(</span><span class=\"k\">Replace</span><span class=\"p\">(</span><span class=\"nb\">text</span><span class=\"p\">,</span> <span class=\"s1\">'</span><span class=\"se\">\\r</span><span class=\"s1\">'</span><span class=\"p\">,</span> <span class=\"s1\">''</span><span class=\"p\">),</span> <span class=\"s1\">'</span><span class=\"se\">\\\\</span><span class=\"s1\">n'</span><span class=\"p\">,</span> <span class=\"s1\">''</span><span class=\"p\">)</span> <span class=\"k\">AS</span> <span class=\"nb\">text</span> </code></pre></figure><p>But unfortunately, I was not able to use this SQL syntax in unload command, it didn’t replace anything.  (I have request AWS Support to solve this, I’ll this blog once I got the solution), but we can create a view on top of the <code class=\"language-html highlighter-rouge\">stl_querytext</code> to replace these characters.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">CREATE</span> <span class=\"k\">VIEW</span> <span class=\"n\">v_stl_querytext</span> <span class=\"k\">AS</span>   <span class=\"p\">(</span><span class=\"k\">SELECT</span> <span class=\"n\">userid</span><span class=\"p\">,</span>           <span class=\"n\">xid</span><span class=\"p\">,</span>           <span class=\"n\">pid</span><span class=\"p\">,</span>           <span class=\"n\">query</span><span class=\"p\">,</span>           <span class=\"n\">SEQUENCE</span><span class=\"p\">,</span>           <span class=\"k\">Replace</span><span class=\"p\">(</span><span class=\"k\">Replace</span><span class=\"p\">(</span><span class=\"nb\">text</span><span class=\"p\">,</span> <span class=\"s1\">'</span><span class=\"se\">\\r</span><span class=\"s1\">'</span><span class=\"p\">,</span> <span class=\"s1\">''</span><span class=\"p\">),</span> <span class=\"s1\">'</span><span class=\"se\">\\\\</span><span class=\"s1\">n'</span><span class=\"p\">,</span> <span class=\"s1\">''</span><span class=\"p\">)</span> <span class=\"k\">AS</span> <span class=\"nb\">text</span>    <span class=\"k\">FROM</span>   <span class=\"n\">stl_querytext</span><span class=\"p\">)</span> </code></pre></figure><p>Now lest unload the data to S3 bucket, make sure you have a proper IAM role to access the S3 bucket and change the bucket name and path as per your need.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"n\">unload</span> <span class=\"p\">(</span><span class=\"s1\">'select * from  v_stl_querytext'</span><span class=\"p\">)</span> <span class=\"k\">to</span> <span class=\"s1\">'s3://mybucket/querytxt/querytxt.csv'</span> <span class=\"n\">iam_role</span> <span class=\"s1\">'arn:aws:iam::1111111111:role/Access-S3'</span> <span class=\"n\">REGION</span> <span class=\"s1\">'ap-south-1'</span> <span class=\"n\">HEADER</span> <span class=\"k\">delimiter</span> <span class=\"s1\">'|'</span> <span class=\"n\">ADDQUOTES</span><span class=\"p\">;</span></code></pre></figure><h2 id=\"update-2020-03-30\">Update: 2020-03-30</h2><p>Instead of creating the view, you can use the following query to unload the data.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"c1\">-- Credit: AWS team</span><span class=\"n\">unload</span> <span class=\"p\">(</span><span class=\"s1\">'SELECT userid,        xid,        pid,        query,        sequence,        replace(replace(text,        </span><span class=\"se\">\\'\\\\\\\\</span><span class=\"s1\">n</span><span class=\"se\">\\'</span><span class=\"s1\">,</span><span class=\"se\">\\'\\'</span><span class=\"s1\">),</span><span class=\"se\">\\'\\\\\\\\</span><span class=\"s1\">r</span><span class=\"se\">\\'</span><span class=\"s1\">,</span><span class=\"se\">\\'\\'</span><span class=\"s1\">) AS txtFROM stl_querytext'</span><span class=\"p\">)</span> <span class=\"k\">to</span> <span class=\"s1\">'s3://mybucket/querytxt/querytxt.csv'</span> <span class=\"n\">iam_role</span> <span class=\"s1\">'arn:aws:iam::1111111111:role/Access-S3'</span> <span class=\"n\">REGION</span> <span class=\"s1\">'ap-south-1'</span> <span class=\"n\">HEADER</span> <span class=\"k\">delimiter</span> <span class=\"s1\">'|'</span> <span class=\"n\">ADDQUOTES</span><span class=\"p\">;</span></code></pre></figure><p>If you want to add compress, please enable it, which will improve your Athena query performance.</p><h2 id=\"athena-table\">Athena Table:</h2><p>Now lets go and create a table in Athena to query the data in S3.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">CREATE</span> <span class=\"k\">EXTERNAL</span> <span class=\"k\">TABLE</span> <span class=\"nv\">`stl_querytext`</span><span class=\"p\">(</span>  <span class=\"nv\">`userid`</span> <span class=\"n\">string</span> <span class=\"k\">COMMENT</span> <span class=\"s1\">'from deserializer'</span><span class=\"p\">,</span>   <span class=\"nv\">`xid`</span> <span class=\"n\">string</span> <span class=\"k\">COMMENT</span> <span class=\"s1\">'from deserializer'</span><span class=\"p\">,</span>   <span class=\"nv\">`pid`</span> <span class=\"n\">string</span> <span class=\"k\">COMMENT</span> <span class=\"s1\">'from deserializer'</span><span class=\"p\">,</span>   <span class=\"nv\">`query`</span> <span class=\"n\">string</span> <span class=\"k\">COMMENT</span> <span class=\"s1\">'from deserializer'</span><span class=\"p\">,</span>   <span class=\"nv\">`sequence`</span> <span class=\"n\">string</span> <span class=\"k\">COMMENT</span> <span class=\"s1\">'from deserializer'</span><span class=\"p\">,</span>   <span class=\"nv\">`text`</span> <span class=\"n\">string</span> <span class=\"k\">COMMENT</span> <span class=\"s1\">'from deserializer'</span><span class=\"p\">)</span><span class=\"k\">ROW</span> <span class=\"n\">FORMAT</span> <span class=\"n\">SERDE</span>   <span class=\"s1\">'org.apache.hadoop.hive.serde2.OpenCSVSerde'</span> <span class=\"k\">WITH</span> <span class=\"n\">SERDEPROPERTIES</span> <span class=\"p\">(</span>   <span class=\"s1\">'escapeChar'</span><span class=\"o\">=</span><span class=\"s1\">'</span><span class=\"se\">\\\\</span><span class=\"s1\">'</span><span class=\"p\">,</span>   <span class=\"s1\">'quoteChar'</span><span class=\"o\">=</span><span class=\"s1\">'</span><span class=\"se\">\\\"</span><span class=\"s1\">'</span><span class=\"p\">,</span>   <span class=\"s1\">'separatorChar'</span><span class=\"o\">=</span><span class=\"s1\">'|'</span><span class=\"p\">)</span> <span class=\"n\">STORED</span> <span class=\"k\">AS</span> <span class=\"n\">INPUTFORMAT</span>   <span class=\"s1\">'org.apache.hadoop.mapred.TextInputFormat'</span> <span class=\"n\">OUTPUTFORMAT</span>   <span class=\"s1\">'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'</span><span class=\"k\">LOCATION</span>  <span class=\"s1\">'s3://mybucket/querytxt/'</span><span class=\"n\">TBLPROPERTIES</span> <span class=\"p\">(</span>  <span class=\"s1\">'skip.header.line.count'</span><span class=\"o\">=</span><span class=\"s1\">'1'</span><span class=\"p\">)</span> </code></pre></figure><p>We are almost done, now let’s write some SQL queries to reconstruct the stl_querytext table.</p><h2 id=\"reconstruct-the-sql-queries\">Reconstruct the SQL Queries:</h2><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">with</span> <span class=\"n\">cte</span> <span class=\"k\">as</span>    <span class=\"p\">(</span><span class=\"k\">SELECT</span> <span class=\"n\">userid</span><span class=\"p\">,</span>         <span class=\"n\">xid</span><span class=\"p\">,</span>        <span class=\"n\">pid</span><span class=\"p\">,</span>        <span class=\"n\">query</span><span class=\"p\">,</span>         <span class=\"n\">sequence</span><span class=\"p\">,</span>         <span class=\"nb\">text</span><span class=\"p\">,</span>        <span class=\"n\">row_number</span><span class=\"p\">()</span>        <span class=\"n\">OVER</span> <span class=\"p\">(</span><span class=\"n\">PARTITION</span> <span class=\"k\">BY</span> <span class=\"n\">userid</span><span class=\"p\">,</span> <span class=\"n\">xid</span><span class=\"p\">,</span><span class=\"n\">pid</span><span class=\"p\">,</span><span class=\"n\">query</span>    <span class=\"k\">ORDER</span> <span class=\"k\">BY</span>  <span class=\"n\">sequence</span><span class=\"p\">)</span> <span class=\"k\">AS</span> <span class=\"n\">rank</span>    <span class=\"k\">FROM</span> <span class=\"n\">stl_querytext</span>    <span class=\"k\">GROUP</span> <span class=\"k\">BY</span>  <span class=\"n\">userid</span><span class=\"p\">,</span> <span class=\"n\">xid</span><span class=\"p\">,</span><span class=\"n\">pid</span><span class=\"p\">,</span><span class=\"n\">query</span><span class=\"p\">,</span><span class=\"n\">sequence</span><span class=\"p\">,</span><span class=\"nb\">text</span>    <span class=\"k\">ORDER</span> <span class=\"k\">BY</span>  <span class=\"n\">rank</span><span class=\"p\">)</span><span class=\"k\">SELECT</span> <span class=\"n\">userid</span><span class=\"p\">,</span>         <span class=\"n\">xid</span><span class=\"p\">,</span>        <span class=\"n\">pid</span><span class=\"p\">,</span>        <span class=\"n\">query</span><span class=\"p\">,</span>        <span class=\"n\">array_join</span><span class=\"p\">(</span><span class=\"n\">array_agg</span><span class=\"p\">(</span><span class=\"nb\">text</span><span class=\"p\">),</span>         <span class=\"s1\">''</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"nb\">text</span><span class=\"k\">FROM</span> <span class=\"n\">cte</span><span class=\"k\">GROUP</span> <span class=\"k\">BY</span>  <span class=\"n\">userid</span><span class=\"p\">,</span> <span class=\"n\">xid</span><span class=\"p\">,</span><span class=\"n\">pid</span><span class=\"p\">,</span><span class=\"n\">query</span><span class=\"k\">ORDER</span> <span class=\"k\">BY</span>  <span class=\"n\">query</span></code></pre></figure><p><img src=\"/assets/Reconstrcut RedShift STL_QUERYTEXT using AWS Athena.jpg\" alt=\"\" /></p><p>Ohhh!!! Im impressed by its performance, Last time, when  I perform the same activity on RedShift some times it took a min to complete, but Athena did a great job. Try it out and comment below if you face any issues.</p><h2 id=\"update-2020-03-30-1\">Update: 2020-03-30</h2><p>I faced an issue with Athena recently, what happened is the array_agg() is not maintaining the query order. So some queries may be incorrect in the result reset. We can solve them by using <code class=\"language-html highlighter-rouge\">array_join(array_agg(text ORDER BY  sequence), '') AS text</code> and again this syntax will not support in Athena because it 1.7x version. Im not sure when AWS is going to update the Athena’s presto verison. But if you want to use presto in EMR, it’ll work. Use the following query.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">with</span> <span class=\"n\">cte</span> <span class=\"k\">AS</span>     <span class=\"p\">(</span><span class=\"k\">SELECT</span> <span class=\"n\">userid</span><span class=\"p\">,</span>         <span class=\"n\">xid</span><span class=\"p\">,</span>         <span class=\"n\">pid</span><span class=\"p\">,</span>         <span class=\"n\">query</span><span class=\"p\">,</span>         <span class=\"n\">sequence</span><span class=\"p\">,</span>         <span class=\"nb\">text</span><span class=\"p\">,</span>         <span class=\"n\">row_number</span><span class=\"p\">()</span>        <span class=\"n\">OVER</span> <span class=\"p\">(</span><span class=\"n\">PARTITION</span> <span class=\"k\">BY</span> <span class=\"n\">userid</span><span class=\"p\">,</span> <span class=\"n\">xid</span><span class=\"p\">,</span><span class=\"n\">pid</span><span class=\"p\">,</span><span class=\"n\">query</span>    <span class=\"k\">ORDER</span> <span class=\"k\">BY</span>  <span class=\"n\">sequence</span><span class=\"p\">)</span> <span class=\"k\">AS</span> <span class=\"n\">rank</span>    <span class=\"k\">FROM</span> <span class=\"n\">stl_querytext</span>    <span class=\"k\">GROUP</span> <span class=\"k\">BY</span>  <span class=\"n\">userid</span><span class=\"p\">,</span> <span class=\"n\">xid</span><span class=\"p\">,</span><span class=\"n\">pid</span><span class=\"p\">,</span><span class=\"n\">query</span><span class=\"p\">,</span><span class=\"n\">sequence</span><span class=\"p\">,</span><span class=\"nb\">text</span>    <span class=\"k\">ORDER</span> <span class=\"k\">BY</span>  <span class=\"n\">rank</span><span class=\"p\">)</span><span class=\"k\">SELECT</span> <span class=\"n\">userid</span><span class=\"p\">,</span>         <span class=\"n\">xid</span><span class=\"p\">,</span>         <span class=\"n\">pid</span><span class=\"p\">,</span>         <span class=\"n\">query</span><span class=\"p\">,</span>         <span class=\"n\">array_join</span><span class=\"p\">(</span><span class=\"n\">array_agg</span><span class=\"p\">(</span><span class=\"nb\">text</span> <span class=\"k\">ORDER</span> <span class=\"k\">BY</span>  <span class=\"n\">sequence</span><span class=\"p\">),</span> <span class=\"s1\">''</span><span class=\"p\">)</span> <span class=\"k\">AS</span> <span class=\"nb\">text</span><span class=\"k\">FROM</span> <span class=\"n\">cte</span><span class=\"k\">GROUP</span> <span class=\"k\">BY</span>  <span class=\"n\">userid</span><span class=\"p\">,</span> <span class=\"n\">xid</span><span class=\"p\">,</span><span class=\"n\">pid</span><span class=\"p\">,</span><span class=\"n\">query</span><span class=\"k\">ORDER</span> <span class=\"k\">BY</span>  <span class=\"n\">query</span><span class=\"p\">;</span></code></pre></figure>",
            "url": "/2020/03/25/reconstruct-redshift-stl-querytext-using-aws-athena",
            "image": "/assets/Reconstrcut RedShift STL_QUERYTEXT using AWS Athena-cover.jpg",
            
            
            "tags": ["aws","redshift","sql","athena"],
            
            "date_published": "2020-03-25T01:15:00+00:00",
            "date_modified": "2020-03-25T01:15:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2020/03/19/3-things-before-migrating-to-cloudsql-mysql",
            "title": "3 Things Before Migrating To CloudSQL(MySQL)",
            "summary": "Make a note of these 3 things before your migrating your MySQL database to GCP CloudSQL",
            "content_text": "If you are going to migrate your MySQL workloads to GCP’s managed database service CloudSQL, then you have to keep these points in mind. We have done a lot of CloudSQL migrations. But sometimes it’s not smooth as we thought. Generally, people don’t even think that these thinks will make the replication failure. I listing 3 things that ate our brain and time while migrating to CloudSQL.1. Server character set:CloudSQL by default using utf8 as the server character set. But it is customizable, we can change it any time. But still, it’ll mess up your application later. We had a MySQL server on a VM where the server’s character set was latin1. We dump the database and restore it to CloudSQL. While launching the CloudSQL we didn’t set up any Database flags. So the data restore with utf8 character set.Before Migrationmysql&gt; SHOW SESSION VARIABLES LIKE 'character\\_set\\_%';+--------------------------+--------+| Variable_name            | Value  |+--------------------------+--------+| character_set_client     | utf8   || character_set_connection | utf8   || character_set_database   | latin1 || character_set_filesystem | binary || character_set_results    | utf8   || character_set_server     | latin1 || character_set_system     | utf8   |+--------------------------+--------+After Migrationmysql&gt;  SHOW SESSION VARIABLES LIKE 'character\\_set\\_%';+--------------------------+--------+| Variable_name            | Value  |+--------------------------+--------+| character_set_client     | utf8   || character_set_connection | utf8   || character_set_database   | utf8   || character_set_filesystem | binary || character_set_results    | utf8   || character_set_server     | utf8   || character_set_system     | utf8   |+--------------------------+--------+We had a table which has the data in Japanise language.  But after cutover, we were not able to get the exact data. Actual Dataサッポロプレミアム生ビール（中ジョッキ) After the migrationã‚µãƒƒãƒ�ãƒ­ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ç”Ÿãƒ“ãƒ¼ãƒ«ï¼ˆä¸­ã‚¸ãƒ§ãƒƒã‚­)Things we tried to solve but didn’t work:  Changed the character_set_server to latin1.  Dump the table from the same server with --default-character-set=latin1 and restored.  Dump the table from the old MySQL server with --default-character-set=latin1 and restored the dump again with --default-character-set=latin1.  Created a new replica with character_set_server = latin1What we did as a Workaround:From the application, change the default client connection to latin1.  But please make sure the character server character set before the migration.2. MyISAM tables:Everybody aware that CloudSQL only supports the innodb engine. But the CloudSQL migration option will help you to convert the MyISAM engine to Innodb during the restore. We had a very large table. So we let the conversion with CloudSQL. Once the data had been restored then the replication started without any issues. But After a few hours, we noticed that the replication is broken due to Duplicate Key conflict. We got an error message like below.Could not execute Write_rows event on table dbname.tablename; Duplicate entry '32640' for key 'PRIMARY', Error_code: 1062; handler error HA_ERR_FOUND_DUPP_KEY; the event's master log mysql-bin.000576, end_log_pos 16529819We saw that the table had auto_increment primary key, also it was working fine for a few hours.Things we tried to solve but didn’t work:Nothing much tried with the latest dumps. We did this dump and restore many times, but every time it got failed.Workaround:Convert the MyISAM tables to Innodb on the master node, then take the dump file for the migration.3. Don’t include the Views in your dump file:The default dump file restore user in CloudSQL has very limited privilege, it doesn’t have the create view permission. During the restore when it detects the create view command, then it’ll stop the process. And the other bad news is it’ll never log any single entry about this view in the stackdriver log. We had this situation, and we were not able to identify what was causing this issue, then we requested the GCP support team to share the internal logs which only available for them. There we got this error line information.ERROR 1045 (28000) at line 71960: Access denied for user 'cloudsqlimport'@'localhost' (using password: NO)When we extracted line 71960 from the dump file, we got to know that it is a view. Anyhow the GCP’s documentation says don’t include the view during the dump. So if you encountered to create a view or some with definer= those line numbers will not bere in your stackdriver logs.If you had some strange experience with CloudSQL migration, please leave them in the comments.",
            "content_html": "<p>If you are going to migrate your MySQL workloads to GCP’s managed database service CloudSQL, then you have to keep these points in mind. We have done a lot of CloudSQL migrations. But sometimes it’s not smooth as we thought. Generally, people don’t even think that these thinks will make the replication failure. I listing 3 things that ate our brain and time while migrating to CloudSQL.</p><h2 id=\"1-server-character-set\">1. Server character set:</h2><p>CloudSQL by default using <code class=\"language-html highlighter-rouge\">utf8</code> as the server character set. But it is customizable, we can change it any time. But still, it’ll mess up your application later. We had a MySQL server on a VM where the server’s character set was <code class=\"language-html highlighter-rouge\">latin1</code>. We dump the database and restore it to CloudSQL. While launching the CloudSQL we didn’t set up any Database flags. So the data restore with <code class=\"language-html highlighter-rouge\">utf8</code> character set.</p><p><strong>Before Migration</strong></p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"n\">mysql</span><span class=\"o\">&gt;</span> <span class=\"k\">SHOW</span> <span class=\"k\">SESSION</span> <span class=\"n\">VARIABLES</span> <span class=\"k\">LIKE</span> <span class=\"s1\">'character</span><span class=\"se\">\\_</span><span class=\"s1\">set</span><span class=\"se\">\\_</span><span class=\"s1\">%'</span><span class=\"p\">;</span><span class=\"o\">+</span><span class=\"c1\">--------------------------+--------+</span><span class=\"o\">|</span> <span class=\"n\">Variable_name</span>            <span class=\"o\">|</span> <span class=\"n\">Value</span>  <span class=\"o\">|</span><span class=\"o\">+</span><span class=\"c1\">--------------------------+--------+</span><span class=\"o\">|</span> <span class=\"n\">character_set_client</span>     <span class=\"o\">|</span> <span class=\"n\">utf8</span>   <span class=\"o\">|</span><span class=\"o\">|</span> <span class=\"n\">character_set_connection</span> <span class=\"o\">|</span> <span class=\"n\">utf8</span>   <span class=\"o\">|</span><span class=\"o\">|</span> <span class=\"n\">character_set_database</span>   <span class=\"o\">|</span> <span class=\"n\">latin1</span> <span class=\"o\">|</span><span class=\"o\">|</span> <span class=\"n\">character_set_filesystem</span> <span class=\"o\">|</span> <span class=\"nb\">binary</span> <span class=\"o\">|</span><span class=\"o\">|</span> <span class=\"n\">character_set_results</span>    <span class=\"o\">|</span> <span class=\"n\">utf8</span>   <span class=\"o\">|</span><span class=\"o\">|</span> <span class=\"n\">character_set_server</span>     <span class=\"o\">|</span> <span class=\"n\">latin1</span> <span class=\"o\">|</span><span class=\"o\">|</span> <span class=\"n\">character_set_system</span>     <span class=\"o\">|</span> <span class=\"n\">utf8</span>   <span class=\"o\">|</span><span class=\"o\">+</span><span class=\"c1\">--------------------------+--------+</span></code></pre></figure><p><strong>After Migration</strong></p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"n\">mysql</span><span class=\"o\">&gt;</span>  <span class=\"k\">SHOW</span> <span class=\"k\">SESSION</span> <span class=\"n\">VARIABLES</span> <span class=\"k\">LIKE</span> <span class=\"s1\">'character</span><span class=\"se\">\\_</span><span class=\"s1\">set</span><span class=\"se\">\\_</span><span class=\"s1\">%'</span><span class=\"p\">;</span><span class=\"o\">+</span><span class=\"c1\">--------------------------+--------+</span><span class=\"o\">|</span> <span class=\"n\">Variable_name</span>            <span class=\"o\">|</span> <span class=\"n\">Value</span>  <span class=\"o\">|</span><span class=\"o\">+</span><span class=\"c1\">--------------------------+--------+</span><span class=\"o\">|</span> <span class=\"n\">character_set_client</span>     <span class=\"o\">|</span> <span class=\"n\">utf8</span>   <span class=\"o\">|</span><span class=\"o\">|</span> <span class=\"n\">character_set_connection</span> <span class=\"o\">|</span> <span class=\"n\">utf8</span>   <span class=\"o\">|</span><span class=\"o\">|</span> <span class=\"n\">character_set_database</span>   <span class=\"o\">|</span> <span class=\"n\">utf8</span>   <span class=\"o\">|</span><span class=\"o\">|</span> <span class=\"n\">character_set_filesystem</span> <span class=\"o\">|</span> <span class=\"nb\">binary</span> <span class=\"o\">|</span><span class=\"o\">|</span> <span class=\"n\">character_set_results</span>    <span class=\"o\">|</span> <span class=\"n\">utf8</span>   <span class=\"o\">|</span><span class=\"o\">|</span> <span class=\"n\">character_set_server</span>     <span class=\"o\">|</span> <span class=\"n\">utf8</span>   <span class=\"o\">|</span><span class=\"o\">|</span> <span class=\"n\">character_set_system</span>     <span class=\"o\">|</span> <span class=\"n\">utf8</span>   <span class=\"o\">|</span><span class=\"o\">+</span><span class=\"c1\">--------------------------+--------+</span></code></pre></figure><p>We had a table which has the data in Japanise language.  But after cutover, we were not able to get the exact data. <strong>Actual Data</strong></p><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>サッポロプレミアム生ビール（中ジョッキ) </code></pre></div></div><p><strong>After the migration</strong></p><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>ã‚µãƒƒãƒ�ãƒ­ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ç”Ÿãƒ“ãƒ¼ãƒ«ï¼ˆä¸­ã‚¸ãƒ§ãƒƒã‚­)</code></pre></div></div><h3 id=\"things-we-tried-to-solve-but-didnt-work\">Things we tried to solve but didn’t work:</h3><ol>  <li>Changed the <code class=\"language-html highlighter-rouge\">character_set_server</code> to <code class=\"language-html highlighter-rouge\">latin1</code>.</li>  <li>Dump the table from the same server with <code class=\"language-html highlighter-rouge\">--default-character-set=latin1</code> and restored.</li>  <li>Dump the table from the old MySQL server with <code class=\"language-html highlighter-rouge\">--default-character-set=latin1</code> and restored the dump again with <code class=\"language-html highlighter-rouge\">--default-character-set=latin1</code>.</li>  <li>Created a new replica with <code class=\"language-html highlighter-rouge\">character_set_server = latin1</code></li></ol><h3 id=\"what-we-did-as-a-workaround\">What we did as a Workaround:</h3><p>From the application, change the default client connection to <code class=\"language-html highlighter-rouge\">latin1</code>.  But please make sure the character server character set before the migration.</p><h2 id=\"2-myisam-tables\">2. MyISAM tables:</h2><p>Everybody aware that CloudSQL only supports the <code class=\"language-html highlighter-rouge\">innodb</code> engine. But the CloudSQL migration option will help you to convert the MyISAM engine to Innodb during the restore. We had a very large table. So we let the conversion with CloudSQL. Once the data had been restored then the replication started without any issues. But After a few hours, we noticed that the replication is broken due to Duplicate Key conflict. We got an error message like below.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"n\">Could</span> <span class=\"k\">not</span> <span class=\"k\">execute</span> <span class=\"n\">Write_rows</span> <span class=\"n\">event</span> <span class=\"k\">on</span> <span class=\"k\">table</span> <span class=\"n\">dbname</span><span class=\"p\">.</span><span class=\"n\">tablename</span><span class=\"p\">;</span> <span class=\"n\">Duplicate</span> <span class=\"n\">entry</span> <span class=\"s1\">'32640'</span> <span class=\"k\">for</span> <span class=\"k\">key</span> <span class=\"s1\">'PRIMARY'</span><span class=\"p\">,</span> <span class=\"n\">Error_code</span><span class=\"p\">:</span> <span class=\"mi\">1062</span><span class=\"p\">;</span> <span class=\"k\">handler</span> <span class=\"n\">error</span> <span class=\"n\">HA_ERR_FOUND_DUPP_KEY</span><span class=\"p\">;</span> <span class=\"n\">the</span> <span class=\"n\">event</span><span class=\"s1\">'s master log mysql-bin.000576, end_log_pos 16529819</span></code></pre></figure><p>We saw that the table had <code class=\"language-html highlighter-rouge\">auto_increment</code> primary key, also it was working fine for a few hours.</p><h3 id=\"things-we-tried-to-solve-but-didnt-work-1\">Things we tried to solve but didn’t work:</h3><p>Nothing much tried with the latest dumps. We did this dump and restore many times, but every time it got failed.</p><h3 id=\"workaround\">Workaround:</h3><p>Convert the MyISAM tables to Innodb on the master node, then take the dump file for the migration.</p><h2 id=\"3-dont-include-the-views-in-your-dump-file\">3. Don’t include the Views in your dump file:</h2><p>The default dump file restore user in CloudSQL has very limited privilege, it doesn’t have the <code class=\"language-html highlighter-rouge\">create view</code> permission. During the restore when it detects the create view command, then it’ll stop the process. And the other bad news is it’ll never log any single entry about this view in the stackdriver log. We had this situation, and we were not able to identify what was causing this issue, then we requested the GCP support team to share the internal logs which only available for them. There we got this error line information.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"n\">ERROR</span> <span class=\"mi\">1045</span> <span class=\"p\">(</span><span class=\"mi\">28000</span><span class=\"p\">)</span> <span class=\"k\">at</span> <span class=\"n\">line</span> <span class=\"mi\">71960</span><span class=\"p\">:</span> <span class=\"k\">Access</span> <span class=\"n\">denied</span> <span class=\"k\">for</span> <span class=\"k\">user</span> <span class=\"s1\">'cloudsqlimport'</span><span class=\"o\">@</span><span class=\"s1\">'localhost'</span> <span class=\"p\">(</span><span class=\"k\">using</span> <span class=\"n\">password</span><span class=\"p\">:</span> <span class=\"k\">NO</span><span class=\"p\">)</span></code></pre></figure><p>When we extracted line 71960 from the dump file, we got to know that it is a view. Anyhow the GCP’s documentation says don’t include the view during the dump. So if you encountered to create a view or some with <code class=\"language-html highlighter-rouge\">definer=</code> those line numbers will not bere in your stackdriver logs.</p><p>If you had some strange experience with CloudSQL migration, please leave them in the comments.</p>",
            "url": "/2020/03/19/3-things-before-migrating-to-cloudsql-mysql",
            "image": "/assets/Debezium MySQL Snapshot For CloudSQL(MySQL) From Replica.jpg",
            
            
            "tags": ["gcp","mysql","cloudsql","migration"],
            
            "date_published": "2020-03-19T18:40:00+00:00",
            "date_modified": "2020-03-19T18:40:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2020/03/18/redshift-reconstructing-sql-from-sql-querytext",
            "title": "RedShift Reconstructing SQL from STL_QUERYTEXT",
            "summary": "Reconstruct all SQL queries from STL_QUERYTEXT with LISTAGG and partition by",
            "content_text": "If you are managing the RedShift clusters then STL_QUERY and STL_QUERYTEXT tables are not new to you. STL_Query can’t hold the complete SQL query instead we can use STL_QueryText to read the complete query. But there is a challenge, we can’t read that table as it is. Since your queries are saved in multiple rows. So we need to combine all these rows into a single row with LISTAGG function which is well documented here.What is the real challenge?From the AWS documentation example, they are showing to reconstruct the SQL for a single query. But in a production system, you may have 50k+ queries where the LISTAGG function will through some errors related to its limitation. Let’s simulate this. I ran the below query on my production RedShift Cluster.SELECT   Listagg(          CASE                   WHEN Len(Rtrim(text)) = 0 THEN text                   ELSE Rtrim(text)          END, '') within GROUP (ORDER BY SEQUENCE) AS textFROM     stl_querytext limit 10;ERROR:  Result size exceeds LISTAGG limitDETAIL:  -----------------------------------------------  error:  Result size exceeds LISTAGG limit  code:      8001  context:   LISTAGG limit: 65535  query:     180132  location:  0.cpp:228  process:   query0_78_180132 [pid=23735]  -----------------------------------------------Why this error?If you read the LISTAGG function details, If the result set is larger than the maximum VARCHAR size (64K – 1, or 65535), then LISTAGG returns the error. Here I have more than 50k+ rows. So the overall characters will not fit in the result set.What is the solution?:If we run the reconstruct query one by one instead of all then the resultset will fit into the LISTAGG’s limitation. Lets say one of my queries split into 2 rows in the STL_QUERYTEXT table.SELECT text FROM   stl_querytext WHERE  query = 97729 ORDER  BY SEQUENCE; First, process these two rows and then process another query and then the next one.How?We have userid,pid,xid,query columns are common between these two rows. So partition your resultset by query wise. Then do the LISTAGG.LISTAGG(      CASE        WHEN LEN(RTRIM(text)) = 0 THEN text        ELSE RTRIM(text)      END,      ''    ) within group (      order by        sequence    ) over (PARTITION by userid, xid, pid, query) Replace \\n and \\r characters:If you are going to run this query from psql client most of the queries starts with \\r\\nselect * from mytbl or any GUI based tools will return \\n characters.  \\r actually a carriage return  \\n is not a new line its a sting. So we have to replace twice.replace(replace(    LISTAGG(      CASE        WHEN LEN(RTRIM(text)) = 0 THEN text        ELSE RTRIM(text)      END,      ''    ) within group (      order by        sequence    ) over (PARTITION by userid, xid, pid, query),    '\\r',''   ), '\\\\n', '')Final View:For east management, Im going to create a view on top of the STL_QUERYTEXT table to return the reconstructed SQL for all the queries.CREATE VIEW recon_stl_querytext AS   (SELECT DISTINCT userid,                    xid,                    pid,                    query,                    Replace(Replace(Listagg(CASE                                              WHEN Len(Rtrim(text)) = 0 THEN text                                              ELSE Rtrim(text)                                            END, '')                                      within GROUP ( ORDER BY SEQUENCE ) over (                                        PARTITION BY userid, xid, pid, query),                            '\\r'                            , ''), '\\\\n', '')                    AS text    FROM   stl_querytext    ORDER  BY 1,              2,              3,              4); Now you can view the proper data from this view.SELECT * FROM   recon_stl_querytext WHERE  query = 97729; I hope this helps you to get the overall consolidated view on the query history. If you are interested in how to Audit RedShift Historical Queries With pgbadger please click here.",
            "content_html": "<p>If you are managing the RedShift clusters then <strong>STL_QUERY</strong> and <strong>STL_QUERYTEXT</strong> tables are not new to you. STL_Query can’t hold the complete SQL query instead we can use STL_QueryText to read the complete query. But there is a challenge, we can’t read that table as it is. Since your queries are saved in multiple rows. So we need to combine all these rows into a single row with LISTAGG function which is well documented <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/r_STL_QUERYTEXT.html\">here</a>.</p><h2 id=\"what-is-the-real-challenge\">What is the real challenge?</h2><p>From the AWS documentation example, they are showing to reconstruct the SQL for a single query. But in a production system, you may have 50k+ queries where the LISTAGG function will through some errors related to its limitation. Let’s simulate this. I ran the below query on my production RedShift Cluster.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">SELECT</span>   <span class=\"n\">Listagg</span><span class=\"p\">(</span>          <span class=\"k\">CASE</span>                   <span class=\"k\">WHEN</span> <span class=\"n\">Len</span><span class=\"p\">(</span><span class=\"n\">Rtrim</span><span class=\"p\">(</span><span class=\"nb\">text</span><span class=\"p\">))</span> <span class=\"o\">=</span> <span class=\"mi\">0</span> <span class=\"k\">THEN</span> <span class=\"nb\">text</span>                   <span class=\"k\">ELSE</span> <span class=\"n\">Rtrim</span><span class=\"p\">(</span><span class=\"nb\">text</span><span class=\"p\">)</span>          <span class=\"k\">END</span><span class=\"p\">,</span> <span class=\"s1\">''</span><span class=\"p\">)</span> <span class=\"n\">within</span> <span class=\"k\">GROUP</span> <span class=\"p\">(</span><span class=\"k\">ORDER</span> <span class=\"k\">BY</span> <span class=\"n\">SEQUENCE</span><span class=\"p\">)</span> <span class=\"k\">AS</span> <span class=\"nb\">text</span><span class=\"k\">FROM</span>     <span class=\"n\">stl_querytext</span> <span class=\"k\">limit</span> <span class=\"mi\">10</span><span class=\"p\">;</span><span class=\"n\">ERROR</span><span class=\"p\">:</span>  <span class=\"k\">Result</span> <span class=\"k\">size</span> <span class=\"n\">exceeds</span> <span class=\"n\">LISTAGG</span> <span class=\"k\">limit</span><span class=\"n\">DETAIL</span><span class=\"p\">:</span>  <span class=\"c1\">-----------------------------------------------</span>  <span class=\"n\">error</span><span class=\"p\">:</span>  <span class=\"k\">Result</span> <span class=\"k\">size</span> <span class=\"n\">exceeds</span> <span class=\"n\">LISTAGG</span> <span class=\"k\">limit</span>  <span class=\"n\">code</span><span class=\"p\">:</span>      <span class=\"mi\">8001</span>  <span class=\"n\">context</span><span class=\"p\">:</span>   <span class=\"n\">LISTAGG</span> <span class=\"k\">limit</span><span class=\"p\">:</span> <span class=\"mi\">65535</span>  <span class=\"n\">query</span><span class=\"p\">:</span>     <span class=\"mi\">180132</span>  <span class=\"k\">location</span><span class=\"p\">:</span>  <span class=\"mi\">0</span><span class=\"p\">.</span><span class=\"n\">cpp</span><span class=\"p\">:</span><span class=\"mi\">228</span>  <span class=\"n\">process</span><span class=\"p\">:</span>   <span class=\"n\">query0_78_180132</span> <span class=\"p\">[</span><span class=\"n\">pid</span><span class=\"o\">=</span><span class=\"mi\">23735</span><span class=\"p\">]</span>  <span class=\"c1\">-----------------------------------------------</span></code></pre></figure><h2 id=\"why-this-error\">Why this error?</h2><p>If you read the LISTAGG function details, If the result set is larger than the maximum VARCHAR size (64K – 1, or 65535), then LISTAGG returns the error. Here I have more than 50k+ rows. So the overall characters will not fit in the result set.</p><h2 id=\"what-is-the-solution\">What is the solution?:</h2><p>If we run the reconstruct query one by one instead of all then the resultset will fit into the LISTAGG’s limitation. Lets say one of my queries split into 2 rows in the STL_QUERYTEXT table.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">SELECT</span> <span class=\"nb\">text</span> <span class=\"k\">FROM</span>   <span class=\"n\">stl_querytext</span> <span class=\"k\">WHERE</span>  <span class=\"n\">query</span> <span class=\"o\">=</span> <span class=\"mi\">97729</span> <span class=\"k\">ORDER</span>  <span class=\"k\">BY</span> <span class=\"n\">SEQUENCE</span><span class=\"p\">;</span> </code></pre></figure><p><img src=\"/assets/RedShift Reconstructing SQL from STL_QUERYTEXT1.jpg\" alt=\"\" /></p><p>First, process these two rows and then process another query and then the next one.</p><h2 id=\"how\">How?</h2><p>We have <code class=\"language-html highlighter-rouge\">userid</code>,<code class=\"language-html highlighter-rouge\">pid</code>,<code class=\"language-html highlighter-rouge\">xid</code>,<code class=\"language-html highlighter-rouge\">query</code> columns are common between these two rows. So partition your resultset by query wise. Then do the LISTAGG.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"n\">LISTAGG</span><span class=\"p\">(</span>      <span class=\"k\">CASE</span>        <span class=\"k\">WHEN</span> <span class=\"n\">LEN</span><span class=\"p\">(</span><span class=\"n\">RTRIM</span><span class=\"p\">(</span><span class=\"nb\">text</span><span class=\"p\">))</span> <span class=\"o\">=</span> <span class=\"mi\">0</span> <span class=\"k\">THEN</span> <span class=\"nb\">text</span>        <span class=\"k\">ELSE</span> <span class=\"n\">RTRIM</span><span class=\"p\">(</span><span class=\"nb\">text</span><span class=\"p\">)</span>      <span class=\"k\">END</span><span class=\"p\">,</span>      <span class=\"s1\">''</span>    <span class=\"p\">)</span> <span class=\"n\">within</span> <span class=\"k\">group</span> <span class=\"p\">(</span>      <span class=\"k\">order</span> <span class=\"k\">by</span>        <span class=\"n\">sequence</span>    <span class=\"p\">)</span> <span class=\"n\">over</span> <span class=\"p\">(</span><span class=\"n\">PARTITION</span> <span class=\"k\">by</span> <span class=\"n\">userid</span><span class=\"p\">,</span> <span class=\"n\">xid</span><span class=\"p\">,</span> <span class=\"n\">pid</span><span class=\"p\">,</span> <span class=\"n\">query</span><span class=\"p\">)</span> </code></pre></figure><h2 id=\"replace-n-and-r-characters\">Replace \\n and \\r characters:</h2><p>If you are going to run this query from psql client most of the queries starts with <code class=\"language-html highlighter-rouge\">\\r\\nselect * from mytbl</code> or any GUI based tools will return <code class=\"language-html highlighter-rouge\">\\n</code> characters.</p><ul>  <li>\\r actually a carriage return</li>  <li>\\n is not a new line its a sting. So we have to replace twice.</li></ul><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">replace</span><span class=\"p\">(</span><span class=\"k\">replace</span><span class=\"p\">(</span>    <span class=\"n\">LISTAGG</span><span class=\"p\">(</span>      <span class=\"k\">CASE</span>        <span class=\"k\">WHEN</span> <span class=\"n\">LEN</span><span class=\"p\">(</span><span class=\"n\">RTRIM</span><span class=\"p\">(</span><span class=\"nb\">text</span><span class=\"p\">))</span> <span class=\"o\">=</span> <span class=\"mi\">0</span> <span class=\"k\">THEN</span> <span class=\"nb\">text</span>        <span class=\"k\">ELSE</span> <span class=\"n\">RTRIM</span><span class=\"p\">(</span><span class=\"nb\">text</span><span class=\"p\">)</span>      <span class=\"k\">END</span><span class=\"p\">,</span>      <span class=\"s1\">''</span>    <span class=\"p\">)</span> <span class=\"n\">within</span> <span class=\"k\">group</span> <span class=\"p\">(</span>      <span class=\"k\">order</span> <span class=\"k\">by</span>        <span class=\"n\">sequence</span>    <span class=\"p\">)</span> <span class=\"n\">over</span> <span class=\"p\">(</span><span class=\"n\">PARTITION</span> <span class=\"k\">by</span> <span class=\"n\">userid</span><span class=\"p\">,</span> <span class=\"n\">xid</span><span class=\"p\">,</span> <span class=\"n\">pid</span><span class=\"p\">,</span> <span class=\"n\">query</span><span class=\"p\">),</span>    <span class=\"s1\">'</span><span class=\"se\">\\r</span><span class=\"s1\">'</span><span class=\"p\">,</span><span class=\"s1\">''</span>   <span class=\"p\">),</span> <span class=\"s1\">'</span><span class=\"se\">\\\\</span><span class=\"s1\">n'</span><span class=\"p\">,</span> <span class=\"s1\">''</span><span class=\"p\">)</span></code></pre></figure><h2 id=\"final-view\">Final View:</h2><p>For east management, Im going to create a view on top of the STL_QUERYTEXT table to return the reconstructed SQL for all the queries.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">CREATE</span> <span class=\"k\">VIEW</span> <span class=\"n\">recon_stl_querytext</span> <span class=\"k\">AS</span>   <span class=\"p\">(</span><span class=\"k\">SELECT</span> <span class=\"k\">DISTINCT</span> <span class=\"n\">userid</span><span class=\"p\">,</span>                    <span class=\"n\">xid</span><span class=\"p\">,</span>                    <span class=\"n\">pid</span><span class=\"p\">,</span>                    <span class=\"n\">query</span><span class=\"p\">,</span>                    <span class=\"k\">Replace</span><span class=\"p\">(</span><span class=\"k\">Replace</span><span class=\"p\">(</span><span class=\"n\">Listagg</span><span class=\"p\">(</span><span class=\"k\">CASE</span>                                              <span class=\"k\">WHEN</span> <span class=\"n\">Len</span><span class=\"p\">(</span><span class=\"n\">Rtrim</span><span class=\"p\">(</span><span class=\"nb\">text</span><span class=\"p\">))</span> <span class=\"o\">=</span> <span class=\"mi\">0</span> <span class=\"k\">THEN</span> <span class=\"nb\">text</span>                                              <span class=\"k\">ELSE</span> <span class=\"n\">Rtrim</span><span class=\"p\">(</span><span class=\"nb\">text</span><span class=\"p\">)</span>                                            <span class=\"k\">END</span><span class=\"p\">,</span> <span class=\"s1\">''</span><span class=\"p\">)</span>                                      <span class=\"n\">within</span> <span class=\"k\">GROUP</span> <span class=\"p\">(</span> <span class=\"k\">ORDER</span> <span class=\"k\">BY</span> <span class=\"n\">SEQUENCE</span> <span class=\"p\">)</span> <span class=\"n\">over</span> <span class=\"p\">(</span>                                        <span class=\"n\">PARTITION</span> <span class=\"k\">BY</span> <span class=\"n\">userid</span><span class=\"p\">,</span> <span class=\"n\">xid</span><span class=\"p\">,</span> <span class=\"n\">pid</span><span class=\"p\">,</span> <span class=\"n\">query</span><span class=\"p\">),</span>                            <span class=\"s1\">'</span><span class=\"se\">\\r</span><span class=\"s1\">'</span>                            <span class=\"p\">,</span> <span class=\"s1\">''</span><span class=\"p\">),</span> <span class=\"s1\">'</span><span class=\"se\">\\\\</span><span class=\"s1\">n'</span><span class=\"p\">,</span> <span class=\"s1\">''</span><span class=\"p\">)</span>                    <span class=\"k\">AS</span> <span class=\"nb\">text</span>    <span class=\"k\">FROM</span>   <span class=\"n\">stl_querytext</span>    <span class=\"k\">ORDER</span>  <span class=\"k\">BY</span> <span class=\"mi\">1</span><span class=\"p\">,</span>              <span class=\"mi\">2</span><span class=\"p\">,</span>              <span class=\"mi\">3</span><span class=\"p\">,</span>              <span class=\"mi\">4</span><span class=\"p\">);</span> </code></pre></figure><p>Now you can view the proper data from this view.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">SELECT</span> <span class=\"o\">*</span> <span class=\"k\">FROM</span>   <span class=\"n\">recon_stl_querytext</span> <span class=\"k\">WHERE</span>  <span class=\"n\">query</span> <span class=\"o\">=</span> <span class=\"mi\">97729</span><span class=\"p\">;</span> </code></pre></figure><p><img src=\"/assets/RedShift Reconstructing SQL from STL_QUERYTEXT2.jpg\" alt=\"\" /></p><p>I hope this helps you to get the overall consolidated view on the query history. If you are interested in how to <a href=\"https://medium.com/searce/audit-redshift-historical-queries-with-pgbadger-619f7f43fbd0\">Audit RedShift Historical Queries With pgbadger please click here</a>.</p>",
            "url": "/2020/03/18/redshift-reconstructing-sql-from-sql-querytext",
            "image": "/assets/RedShift Reconstructing SQL from STL_QUERYTEXT.jpg",
            
            
            "tags": ["aws","redshift","sql"],
            
            "date_published": "2020-03-18T02:07:00+00:00",
            "date_modified": "2020-03-18T02:07:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2020/03/07/experimenting-aws-redshift-column-level-acl",
            "title": "Experimenting AWS RedShift Column Level ACL",
            "summary": "Test and explore all the fuctions of RedShift column level acces control",
            "content_text": "Good news for the RedShift customers now we can GRANT column-level permissions on the tables. It’s not only limited to tables, but we can also grant on views and materialized views as well. When the Lake formation was announced, this feature was a part of it. But unfortunately, we need to use Redshift Spectrum to achieve this. The wait is over now. Redshift natively supports the column level restrictions. Im experimenting and walk through this feature and test all the statements mentioned in the Redshift documentation.Import a sample table:For our experiment, we need sample data. So I have download a .csv file from mockaroo and then uploaded the CSV file into my S3 bucket.create table test_data (    id INT,    first_name VARCHAR(500),    last_name VARCHAR(500),    email VARCHAR(500),    gender VARCHAR(500),    ip_address VARCHAR(200));COPY test_data  from 's3://my-data-lake/mock_data.csv' iam_role 'arn:aws:iam::1111111111:role/Copying-S3to-RS' CSV;I have executed the copy command for multiple times to make my table as some decent amount of rows.Notes about column level ACL:  You can control the column level access only for SELECT and UPDATE.  Once you assigned some column level restriction, then that user should specifically mention the column names in the query. Select * will not work.  You can control the table, view and materialized views.  If you want to give both select and update to a user, then just use GRANT ALL (column names) will give both access to those columns.  Table Owner and Superusers can grant the column ACL.Experiments:Before we start our experiment, we can create a user for this.create user rs_expriment password 'Mypass123';#1 Grant select only access:Let’s grant select access for a few columns and see how the user can access it in different ways.grant select (id, first_name, last_name) on test_data to rs_expriment;set session authorization rs_expriment;SETselect id, first_name, last_name from test_data limit 10; id | first_name | last_name----+------------+-----------  1 | Bernie     | Hull  3 | Carmina    | Cahill  5 | Leanora    | Boribal  7 | Eal        | Crocetto  9 | Gaylor     | Dugmore 11 | Pren       | Stenhouse 13 | Jonie      | Sloegrave 15 | Heinrik    | Cremen 17 | Lauri      | Fraser 19 | Nicolina   | EdwardsLets query other column and all columns.select id, first_name, last_name, email from test_data limit 10;ERROR:  permission denied for relation test_dataselect * from test_data limit 10;ERROR:  permission denied for relation test_data#2 Grant UPDATE accessgrant update (first_name) on test_data to rs_expriment;GRANTset session authorization rs_expriment;SETupdate test_data set first_name='test_name'  where first_name='Bernie';UPDATE 13Now test with other columns.update test_data set last_name='test_name'  where first_name='Bernie';ERROR:  permission denied for relation test_datadelete from test_data where first_name='test_name';ERROR:  permission denied for relation test_data#3 select + update togehterupdate test_data set first_name=last_name  where first_name='test_name';UPDATE 13update test_data set first_name=email  where first_name='test_name';ERROR:  permission denied for relation test_data#4 Test the statements from the RedShift Doc  If a user has a table-level privilege on a table, then granting the same privilege at the column level has no effect.Anyhow this clearly explains the logic. So we can skip this.  If a user has a table-level privilege on a table, then revoking the same privilege for one or more columns of the table returns an error. Instead, revoke the privilege at the table level.create user table_full_select password 'MyPass123';CREATE USERgrant select on test_data to table_full_select;GRANTrevoke select(id,first_name) on test_data from table_full_select;ERROR:  Cannot revoke SELECT privilege on test_data.id from user table_full_select as the grantee holds this privilege at the relation level. Revoke the relation level privilege instead.  If a user has a column-level privilege, then granting the same privilege at the table level returns an error.create user column_select password 'MyPass123';CREATE USERgrant select(id,first_name) on test_data to column_select;GRANTgrant select on test_data to column_select;ERROR:  No privileges were granted. Some grantees hold column privileges on relation test_data. Check for and revoke column level privileges for these grantees on all relations referenced in this Grant statement before granting table privileges to them.  If a user has a column-level privilege, then revoking the same privilege at the table level revokes both column and table privileges for all columns on the table.Note: If you want to revoke the select/update from a column level privilege user, then if you use just revoke select on or revoke update on will revoke the access. You can use this syntax for revoking access on table level/column level privilege users.create user column_select password 'MyPass123';CREATE USERgrant select(id,first_name) on test_data to column_select;GRANTrevoke select on table test_data from column_select;REVOKE  You can’t grant column-level privileges on late-binding views.create view v_test_data as (select * from public.test_data)WITH NO SCHEMA BINDING;CREATE VIEWTry the grant table level access:grant select on v_test_data to column_select;GRANTTry the same on late-binding view:grant select(id,first_name) on v_test_data to column_select;ERROR:  column \"id\" of relation \"v_test_data\" does not exist.  You must have table-level SELECT privilege on the base tables to create a materialized view. Even if you have column-level privileges on specific columns, you can’t create a materialized view on only those columns.create user mv_user password 'MyPass123';CREATE USERgrant select(id,first_name) on test_data to mv_user;GRANTcreate schema acl;CREATE SCHEMAgrant all on schema acl to mv_user;GRANTset session authorization mv_user;SETCREATE MATERIALIZED VIEW acl.mv_testdata as (select id, first_name from public.test_data);CREATE MATERIALIZED VIEWselect * from acl.mv_testdata limit 2;ERROR:  permission denied for materialized view base relation test_data.oh ho!!! What is this?Because by default you have full access on public schema for all the users. Thats why its created.Update:Thanks AWS Support team for clarifying this. Lets see what happen if have your base table on the different schema?create schema bhuvi;CREATEcreate table bhuvi.test_data (\tid INT,\tfirst_name VARCHAR(500),\tlast_name VARCHAR(500),\temail VARCHAR(500),\tgender VARCHAR(500),\tip_address VARCHAR(200));insert into bhuvi.test_data (id, first_name, last_name, email, gender, ip_address) values (1, 'Arden', 'Connichie', 'aconnichie0@nifty.com', 'Female', '66.179.130.47');insert into bhuvi.test_data (id, first_name, last_name, email, gender, ip_address) values (2, 'Elsie', 'Fryatt', 'efryatt1@jugem.jp', 'Female', '91.228.196.151');insert into bhuvi.test_data (id, first_name, last_name, email, gender, ip_address) values (3, 'Corette', 'Tomasz', 'ctomasz2@miitbeian.gov.cn', 'Female', '134.169.27.141');insert into bhuvi.test_data (id, first_name, last_name, email, gender, ip_address) values (4, 'Margarita', 'Moulden', 'mmoulden3@google.com', 'Female', '248.63.226.96');insert into bhuvi.test_data (id, first_name, last_name, email, gender, ip_address) values (5, 'Eran', 'McCoveney', 'emccoveney4@redcross.org', 'Female', '19.137.118.110');grant select(id,first_name) on bhuvi.test_data to mv_user;GRANTset session authorization mv_user;SETCREATE MATERIALIZED VIEW bhuvi.mv_testdata as (select id, first_name from bhuvi.test_data);ERROR:  permission denied for schema bhuviConclusion:Its not a big deal to work with column level ACL. But its worth to test every small feature. So you have better visibility about the feature and find the bugs like what we found above. I’ll update this blog once the AWS team confirms this as a bug or not.",
            "content_html": "<p>Good news for the RedShift customers now we can <code class=\"language-html highlighter-rouge\">GRANT</code> column-level permissions on the tables. It’s not only limited to tables, but we can also grant on views and materialized views as well. When the Lake formation was announced, this feature was a part of it. But unfortunately, we need to use Redshift Spectrum to achieve this. The wait is over now. Redshift natively supports the column level restrictions. Im experimenting and walk through this feature and test all the statements mentioned in the Redshift documentation.</p><h2 id=\"import-a-sample-table\">Import a sample table:</h2><p>For our experiment, we need sample data. So I have download a <code class=\"language-html highlighter-rouge\">.csv</code> file from <a href=\"https://mockaroo.com/\">mockaroo</a> and then uploaded the CSV file into my S3 bucket.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">create</span> <span class=\"k\">table</span> <span class=\"n\">test_data</span> <span class=\"p\">(</span>    <span class=\"n\">id</span> <span class=\"nb\">INT</span><span class=\"p\">,</span>    <span class=\"n\">first_name</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">500</span><span class=\"p\">),</span>    <span class=\"n\">last_name</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">500</span><span class=\"p\">),</span>    <span class=\"n\">email</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">500</span><span class=\"p\">),</span>    <span class=\"n\">gender</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">500</span><span class=\"p\">),</span>    <span class=\"n\">ip_address</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">200</span><span class=\"p\">)</span><span class=\"p\">);</span><span class=\"k\">COPY</span> <span class=\"n\">test_data</span>  <span class=\"k\">from</span> <span class=\"s1\">'s3://my-data-lake/mock_data.csv'</span> <span class=\"n\">iam_role</span> <span class=\"s1\">'arn:aws:iam::1111111111:role/Copying-S3to-RS'</span> <span class=\"n\">CSV</span><span class=\"p\">;</span></code></pre></figure><p>I have executed the copy command for multiple times to make my table as some decent amount of rows.</p><h2 id=\"notes-about-column-level-acl\">Notes about column level ACL:</h2><ul>  <li>You can control the column level access only for <code class=\"language-html highlighter-rouge\">SELECT</code> and <code class=\"language-html highlighter-rouge\">UPDATE</code>.</li>  <li>Once you assigned some column level restriction, then that user should specifically mention the column names in the query. <code class=\"language-html highlighter-rouge\">Select *</code> will not work.</li>  <li>You can control the table, view and materialized views.</li>  <li>If you want to give both select and update to a user, then just use <code class=\"language-html highlighter-rouge\">GRANT ALL (column names)</code> will give both access to those columns.</li>  <li>Table Owner and Superusers can grant the column ACL.</li></ul><h2 id=\"experiments\">Experiments:</h2><p>Before we start our experiment, we can create a user for this.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">create</span> <span class=\"k\">user</span> <span class=\"n\">rs_expriment</span> <span class=\"n\">password</span> <span class=\"s1\">'Mypass123'</span><span class=\"p\">;</span></code></pre></figure><h3 id=\"1-grant-select-only-access\">#1 Grant select only access:</h3><p>Let’s grant select access for a few columns and see how the user can access it in different ways.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">grant</span> <span class=\"k\">select</span> <span class=\"p\">(</span><span class=\"n\">id</span><span class=\"p\">,</span> <span class=\"n\">first_name</span><span class=\"p\">,</span> <span class=\"n\">last_name</span><span class=\"p\">)</span> <span class=\"k\">on</span> <span class=\"n\">test_data</span> <span class=\"k\">to</span> <span class=\"n\">rs_expriment</span><span class=\"p\">;</span><span class=\"k\">set</span> <span class=\"k\">session</span> <span class=\"k\">authorization</span> <span class=\"n\">rs_expriment</span><span class=\"p\">;</span><span class=\"k\">SET</span><span class=\"k\">select</span> <span class=\"n\">id</span><span class=\"p\">,</span> <span class=\"n\">first_name</span><span class=\"p\">,</span> <span class=\"n\">last_name</span> <span class=\"k\">from</span> <span class=\"n\">test_data</span> <span class=\"k\">limit</span> <span class=\"mi\">10</span><span class=\"p\">;</span> <span class=\"n\">id</span> <span class=\"o\">|</span> <span class=\"n\">first_name</span> <span class=\"o\">|</span> <span class=\"n\">last_name</span><span class=\"c1\">----+------------+-----------</span>  <span class=\"mi\">1</span> <span class=\"o\">|</span> <span class=\"n\">Bernie</span>     <span class=\"o\">|</span> <span class=\"n\">Hull</span>  <span class=\"mi\">3</span> <span class=\"o\">|</span> <span class=\"n\">Carmina</span>    <span class=\"o\">|</span> <span class=\"n\">Cahill</span>  <span class=\"mi\">5</span> <span class=\"o\">|</span> <span class=\"n\">Leanora</span>    <span class=\"o\">|</span> <span class=\"n\">Boribal</span>  <span class=\"mi\">7</span> <span class=\"o\">|</span> <span class=\"n\">Eal</span>        <span class=\"o\">|</span> <span class=\"n\">Crocetto</span>  <span class=\"mi\">9</span> <span class=\"o\">|</span> <span class=\"n\">Gaylor</span>     <span class=\"o\">|</span> <span class=\"n\">Dugmore</span> <span class=\"mi\">11</span> <span class=\"o\">|</span> <span class=\"n\">Pren</span>       <span class=\"o\">|</span> <span class=\"n\">Stenhouse</span> <span class=\"mi\">13</span> <span class=\"o\">|</span> <span class=\"n\">Jonie</span>      <span class=\"o\">|</span> <span class=\"n\">Sloegrave</span> <span class=\"mi\">15</span> <span class=\"o\">|</span> <span class=\"n\">Heinrik</span>    <span class=\"o\">|</span> <span class=\"n\">Cremen</span> <span class=\"mi\">17</span> <span class=\"o\">|</span> <span class=\"n\">Lauri</span>      <span class=\"o\">|</span> <span class=\"n\">Fraser</span> <span class=\"mi\">19</span> <span class=\"o\">|</span> <span class=\"n\">Nicolina</span>   <span class=\"o\">|</span> <span class=\"n\">Edwards</span></code></pre></figure><p>Lets query other column and all columns.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">select</span> <span class=\"n\">id</span><span class=\"p\">,</span> <span class=\"n\">first_name</span><span class=\"p\">,</span> <span class=\"n\">last_name</span><span class=\"p\">,</span> <span class=\"n\">email</span> <span class=\"k\">from</span> <span class=\"n\">test_data</span> <span class=\"k\">limit</span> <span class=\"mi\">10</span><span class=\"p\">;</span><span class=\"n\">ERROR</span><span class=\"p\">:</span>  <span class=\"n\">permission</span> <span class=\"n\">denied</span> <span class=\"k\">for</span> <span class=\"n\">relation</span> <span class=\"n\">test_data</span><span class=\"k\">select</span> <span class=\"o\">*</span> <span class=\"k\">from</span> <span class=\"n\">test_data</span> <span class=\"k\">limit</span> <span class=\"mi\">10</span><span class=\"p\">;</span><span class=\"n\">ERROR</span><span class=\"p\">:</span>  <span class=\"n\">permission</span> <span class=\"n\">denied</span> <span class=\"k\">for</span> <span class=\"n\">relation</span> <span class=\"n\">test_data</span></code></pre></figure><h3 id=\"2-grant-update-access\">#2 Grant UPDATE access</h3><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">grant</span> <span class=\"k\">update</span> <span class=\"p\">(</span><span class=\"n\">first_name</span><span class=\"p\">)</span> <span class=\"k\">on</span> <span class=\"n\">test_data</span> <span class=\"k\">to</span> <span class=\"n\">rs_expriment</span><span class=\"p\">;</span><span class=\"k\">GRANT</span><span class=\"k\">set</span> <span class=\"k\">session</span> <span class=\"k\">authorization</span> <span class=\"n\">rs_expriment</span><span class=\"p\">;</span><span class=\"k\">SET</span><span class=\"k\">update</span> <span class=\"n\">test_data</span> <span class=\"k\">set</span> <span class=\"n\">first_name</span><span class=\"o\">=</span><span class=\"s1\">'test_name'</span>  <span class=\"k\">where</span> <span class=\"n\">first_name</span><span class=\"o\">=</span><span class=\"s1\">'Bernie'</span><span class=\"p\">;</span><span class=\"k\">UPDATE</span> <span class=\"mi\">13</span></code></pre></figure><p>Now test with other columns.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">update</span> <span class=\"n\">test_data</span> <span class=\"k\">set</span> <span class=\"n\">last_name</span><span class=\"o\">=</span><span class=\"s1\">'test_name'</span>  <span class=\"k\">where</span> <span class=\"n\">first_name</span><span class=\"o\">=</span><span class=\"s1\">'Bernie'</span><span class=\"p\">;</span><span class=\"n\">ERROR</span><span class=\"p\">:</span>  <span class=\"n\">permission</span> <span class=\"n\">denied</span> <span class=\"k\">for</span> <span class=\"n\">relation</span> <span class=\"n\">test_data</span><span class=\"k\">delete</span> <span class=\"k\">from</span> <span class=\"n\">test_data</span> <span class=\"k\">where</span> <span class=\"n\">first_name</span><span class=\"o\">=</span><span class=\"s1\">'test_name'</span><span class=\"p\">;</span><span class=\"n\">ERROR</span><span class=\"p\">:</span>  <span class=\"n\">permission</span> <span class=\"n\">denied</span> <span class=\"k\">for</span> <span class=\"n\">relation</span> <span class=\"n\">test_data</span></code></pre></figure><h3 id=\"3-select--update-togehter\">#3 select + update togehter</h3><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">update</span> <span class=\"n\">test_data</span> <span class=\"k\">set</span> <span class=\"n\">first_name</span><span class=\"o\">=</span><span class=\"n\">last_name</span>  <span class=\"k\">where</span> <span class=\"n\">first_name</span><span class=\"o\">=</span><span class=\"s1\">'test_name'</span><span class=\"p\">;</span><span class=\"k\">UPDATE</span> <span class=\"mi\">13</span><span class=\"k\">update</span> <span class=\"n\">test_data</span> <span class=\"k\">set</span> <span class=\"n\">first_name</span><span class=\"o\">=</span><span class=\"n\">email</span>  <span class=\"k\">where</span> <span class=\"n\">first_name</span><span class=\"o\">=</span><span class=\"s1\">'test_name'</span><span class=\"p\">;</span><span class=\"n\">ERROR</span><span class=\"p\">:</span>  <span class=\"n\">permission</span> <span class=\"n\">denied</span> <span class=\"k\">for</span> <span class=\"n\">relation</span> <span class=\"n\">test_data</span></code></pre></figure><h3 id=\"4-test-the-statements-from-the-redshift-doc\">#4 Test the statements from the RedShift Doc</h3><blockquote>  <p>If a user has a table-level privilege on a table, then granting the same privilege at the column level has no effect.</p></blockquote><p>Anyhow this clearly explains the logic. So we can skip this.</p><blockquote>  <p>If a user has a table-level privilege on a table, then revoking the same privilege for one or more columns of the table returns an error. Instead, revoke the privilege at the table level.</p></blockquote><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">create</span> <span class=\"k\">user</span> <span class=\"n\">table_full_select</span> <span class=\"n\">password</span> <span class=\"s1\">'MyPass123'</span><span class=\"p\">;</span><span class=\"k\">CREATE</span> <span class=\"k\">USER</span><span class=\"k\">grant</span> <span class=\"k\">select</span> <span class=\"k\">on</span> <span class=\"n\">test_data</span> <span class=\"k\">to</span> <span class=\"n\">table_full_select</span><span class=\"p\">;</span><span class=\"k\">GRANT</span><span class=\"k\">revoke</span> <span class=\"k\">select</span><span class=\"p\">(</span><span class=\"n\">id</span><span class=\"p\">,</span><span class=\"n\">first_name</span><span class=\"p\">)</span> <span class=\"k\">on</span> <span class=\"n\">test_data</span> <span class=\"k\">from</span> <span class=\"n\">table_full_select</span><span class=\"p\">;</span><span class=\"n\">ERROR</span><span class=\"p\">:</span>  <span class=\"n\">Cannot</span> <span class=\"k\">revoke</span> <span class=\"k\">SELECT</span> <span class=\"n\">privilege</span> <span class=\"k\">on</span> <span class=\"n\">test_data</span><span class=\"p\">.</span><span class=\"n\">id</span> <span class=\"k\">from</span> <span class=\"k\">user</span> <span class=\"n\">table_full_select</span> <span class=\"k\">as</span> <span class=\"n\">the</span> <span class=\"n\">grantee</span> <span class=\"n\">holds</span> <span class=\"n\">this</span> <span class=\"n\">privilege</span> <span class=\"k\">at</span> <span class=\"n\">the</span> <span class=\"n\">relation</span> <span class=\"k\">level</span><span class=\"p\">.</span> <span class=\"k\">Revoke</span> <span class=\"n\">the</span> <span class=\"n\">relation</span> <span class=\"k\">level</span> <span class=\"n\">privilege</span> <span class=\"k\">instead</span><span class=\"p\">.</span></code></pre></figure><blockquote>  <p>If a user has a column-level privilege, then granting the same privilege at the table level returns an error.</p></blockquote><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">create</span> <span class=\"k\">user</span> <span class=\"n\">column_select</span> <span class=\"n\">password</span> <span class=\"s1\">'MyPass123'</span><span class=\"p\">;</span><span class=\"k\">CREATE</span> <span class=\"k\">USER</span><span class=\"k\">grant</span> <span class=\"k\">select</span><span class=\"p\">(</span><span class=\"n\">id</span><span class=\"p\">,</span><span class=\"n\">first_name</span><span class=\"p\">)</span> <span class=\"k\">on</span> <span class=\"n\">test_data</span> <span class=\"k\">to</span> <span class=\"n\">column_select</span><span class=\"p\">;</span><span class=\"k\">GRANT</span><span class=\"k\">grant</span> <span class=\"k\">select</span> <span class=\"k\">on</span> <span class=\"n\">test_data</span> <span class=\"k\">to</span> <span class=\"n\">column_select</span><span class=\"p\">;</span><span class=\"n\">ERROR</span><span class=\"p\">:</span>  <span class=\"k\">No</span> <span class=\"k\">privileges</span> <span class=\"n\">were</span> <span class=\"k\">granted</span><span class=\"p\">.</span> <span class=\"k\">Some</span> <span class=\"n\">grantees</span> <span class=\"k\">hold</span> <span class=\"k\">column</span> <span class=\"k\">privileges</span> <span class=\"k\">on</span> <span class=\"n\">relation</span> <span class=\"n\">test_data</span><span class=\"p\">.</span> <span class=\"k\">Check</span> <span class=\"k\">for</span> <span class=\"k\">and</span> <span class=\"k\">revoke</span> <span class=\"k\">column</span> <span class=\"k\">level</span> <span class=\"k\">privileges</span> <span class=\"k\">for</span> <span class=\"n\">these</span> <span class=\"n\">grantees</span> <span class=\"k\">on</span> <span class=\"k\">all</span> <span class=\"n\">relations</span> <span class=\"n\">referenced</span> <span class=\"k\">in</span> <span class=\"n\">this</span> <span class=\"k\">Grant</span> <span class=\"k\">statement</span> <span class=\"k\">before</span> <span class=\"n\">granting</span> <span class=\"k\">table</span> <span class=\"k\">privileges</span> <span class=\"k\">to</span> <span class=\"n\">them</span><span class=\"p\">.</span></code></pre></figure><blockquote>  <p>If a user has a column-level privilege, then revoking the same privilege at the table level revokes both column and table privileges for all columns on the table.</p></blockquote><p>Note: If you want to revoke the select/update from a column level privilege user, then if you use just <code class=\"language-html highlighter-rouge\">revoke select on</code> or <code class=\"language-html highlighter-rouge\">revoke update on</code> will revoke the access. You can use this syntax for revoking access on table level/column level privilege users.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">create</span> <span class=\"k\">user</span> <span class=\"n\">column_select</span> <span class=\"n\">password</span> <span class=\"s1\">'MyPass123'</span><span class=\"p\">;</span><span class=\"k\">CREATE</span> <span class=\"k\">USER</span><span class=\"k\">grant</span> <span class=\"k\">select</span><span class=\"p\">(</span><span class=\"n\">id</span><span class=\"p\">,</span><span class=\"n\">first_name</span><span class=\"p\">)</span> <span class=\"k\">on</span> <span class=\"n\">test_data</span> <span class=\"k\">to</span> <span class=\"n\">column_select</span><span class=\"p\">;</span><span class=\"k\">GRANT</span><span class=\"k\">revoke</span> <span class=\"k\">select</span> <span class=\"k\">on</span> <span class=\"k\">table</span> <span class=\"n\">test_data</span> <span class=\"k\">from</span> <span class=\"n\">column_select</span><span class=\"p\">;</span><span class=\"k\">REVOKE</span></code></pre></figure><blockquote>  <p>You can’t grant column-level privileges on late-binding views.</p></blockquote><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">create</span> <span class=\"k\">view</span> <span class=\"n\">v_test_data</span> <span class=\"k\">as</span> <span class=\"p\">(</span><span class=\"k\">select</span> <span class=\"o\">*</span> <span class=\"k\">from</span> <span class=\"k\">public</span><span class=\"p\">.</span><span class=\"n\">test_data</span><span class=\"p\">)</span><span class=\"k\">WITH</span> <span class=\"k\">NO</span> <span class=\"k\">SCHEMA</span> <span class=\"n\">BINDING</span><span class=\"p\">;</span><span class=\"k\">CREATE</span> <span class=\"k\">VIEW</span></code></pre></figure><p>Try the grant table level access:</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">grant</span> <span class=\"k\">select</span> <span class=\"k\">on</span> <span class=\"n\">v_test_data</span> <span class=\"k\">to</span> <span class=\"n\">column_select</span><span class=\"p\">;</span><span class=\"k\">GRANT</span></code></pre></figure><p>Try the same on late-binding view:</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">grant</span> <span class=\"k\">select</span><span class=\"p\">(</span><span class=\"n\">id</span><span class=\"p\">,</span><span class=\"n\">first_name</span><span class=\"p\">)</span> <span class=\"k\">on</span> <span class=\"n\">v_test_data</span> <span class=\"k\">to</span> <span class=\"n\">column_select</span><span class=\"p\">;</span><span class=\"n\">ERROR</span><span class=\"p\">:</span>  <span class=\"k\">column</span> <span class=\"nv\">\"id\"</span> <span class=\"k\">of</span> <span class=\"n\">relation</span> <span class=\"nv\">\"v_test_data\"</span> <span class=\"n\">does</span> <span class=\"k\">not</span> <span class=\"n\">exist</span><span class=\"p\">.</span></code></pre></figure><blockquote>  <p>You must have table-level SELECT privilege on the base tables to create a materialized view. Even if you have column-level privileges on specific columns, you can’t create a materialized view on only those columns.</p></blockquote><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">create</span> <span class=\"k\">user</span> <span class=\"n\">mv_user</span> <span class=\"n\">password</span> <span class=\"s1\">'MyPass123'</span><span class=\"p\">;</span><span class=\"k\">CREATE</span> <span class=\"k\">USER</span><span class=\"k\">grant</span> <span class=\"k\">select</span><span class=\"p\">(</span><span class=\"n\">id</span><span class=\"p\">,</span><span class=\"n\">first_name</span><span class=\"p\">)</span> <span class=\"k\">on</span> <span class=\"n\">test_data</span> <span class=\"k\">to</span> <span class=\"n\">mv_user</span><span class=\"p\">;</span><span class=\"k\">GRANT</span><span class=\"k\">create</span> <span class=\"k\">schema</span> <span class=\"n\">acl</span><span class=\"p\">;</span><span class=\"k\">CREATE</span> <span class=\"k\">SCHEMA</span><span class=\"k\">grant</span> <span class=\"k\">all</span> <span class=\"k\">on</span> <span class=\"k\">schema</span> <span class=\"n\">acl</span> <span class=\"k\">to</span> <span class=\"n\">mv_user</span><span class=\"p\">;</span><span class=\"k\">GRANT</span><span class=\"k\">set</span> <span class=\"k\">session</span> <span class=\"k\">authorization</span> <span class=\"n\">mv_user</span><span class=\"p\">;</span><span class=\"k\">SET</span><span class=\"k\">CREATE</span> <span class=\"n\">MATERIALIZED</span> <span class=\"k\">VIEW</span> <span class=\"n\">acl</span><span class=\"p\">.</span><span class=\"n\">mv_testdata</span> <span class=\"k\">as</span> <span class=\"p\">(</span><span class=\"k\">select</span> <span class=\"n\">id</span><span class=\"p\">,</span> <span class=\"n\">first_name</span> <span class=\"k\">from</span> <span class=\"k\">public</span><span class=\"p\">.</span><span class=\"n\">test_data</span><span class=\"p\">);</span><span class=\"k\">CREATE</span> <span class=\"n\">MATERIALIZED</span> <span class=\"k\">VIEW</span><span class=\"k\">select</span> <span class=\"o\">*</span> <span class=\"k\">from</span> <span class=\"n\">acl</span><span class=\"p\">.</span><span class=\"n\">mv_testdata</span> <span class=\"k\">limit</span> <span class=\"mi\">2</span><span class=\"p\">;</span><span class=\"n\">ERROR</span><span class=\"p\">:</span>  <span class=\"n\">permission</span> <span class=\"n\">denied</span> <span class=\"k\">for</span> <span class=\"n\">materialized</span> <span class=\"k\">view</span> <span class=\"n\">base</span> <span class=\"n\">relation</span> <span class=\"n\">test_data</span><span class=\"p\">.</span></code></pre></figure><h3 id=\"oh-ho-what-is-this\">oh ho!!! What is this?</h3><p>Because by default you have full access on public schema for all the users. Thats why its created.</p><h3 id=\"update\">Update:</h3><p>Thanks AWS Support team for clarifying this. Lets see what happen if have your base table on the different schema?</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">create</span> <span class=\"k\">schema</span> <span class=\"n\">bhuvi</span><span class=\"p\">;</span><span class=\"k\">CREATE</span><span class=\"k\">create</span> <span class=\"k\">table</span> <span class=\"n\">bhuvi</span><span class=\"p\">.</span><span class=\"n\">test_data</span> <span class=\"p\">(</span>\t<span class=\"n\">id</span> <span class=\"nb\">INT</span><span class=\"p\">,</span>\t<span class=\"n\">first_name</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">500</span><span class=\"p\">),</span>\t<span class=\"n\">last_name</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">500</span><span class=\"p\">),</span>\t<span class=\"n\">email</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">500</span><span class=\"p\">),</span>\t<span class=\"n\">gender</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">500</span><span class=\"p\">),</span>\t<span class=\"n\">ip_address</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">200</span><span class=\"p\">)</span><span class=\"p\">);</span><span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">bhuvi</span><span class=\"p\">.</span><span class=\"n\">test_data</span> <span class=\"p\">(</span><span class=\"n\">id</span><span class=\"p\">,</span> <span class=\"n\">first_name</span><span class=\"p\">,</span> <span class=\"n\">last_name</span><span class=\"p\">,</span> <span class=\"n\">email</span><span class=\"p\">,</span> <span class=\"n\">gender</span><span class=\"p\">,</span> <span class=\"n\">ip_address</span><span class=\"p\">)</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">'Arden'</span><span class=\"p\">,</span> <span class=\"s1\">'Connichie'</span><span class=\"p\">,</span> <span class=\"s1\">'aconnichie0@nifty.com'</span><span class=\"p\">,</span> <span class=\"s1\">'Female'</span><span class=\"p\">,</span> <span class=\"s1\">'66.179.130.47'</span><span class=\"p\">);</span><span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">bhuvi</span><span class=\"p\">.</span><span class=\"n\">test_data</span> <span class=\"p\">(</span><span class=\"n\">id</span><span class=\"p\">,</span> <span class=\"n\">first_name</span><span class=\"p\">,</span> <span class=\"n\">last_name</span><span class=\"p\">,</span> <span class=\"n\">email</span><span class=\"p\">,</span> <span class=\"n\">gender</span><span class=\"p\">,</span> <span class=\"n\">ip_address</span><span class=\"p\">)</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"s1\">'Elsie'</span><span class=\"p\">,</span> <span class=\"s1\">'Fryatt'</span><span class=\"p\">,</span> <span class=\"s1\">'efryatt1@jugem.jp'</span><span class=\"p\">,</span> <span class=\"s1\">'Female'</span><span class=\"p\">,</span> <span class=\"s1\">'91.228.196.151'</span><span class=\"p\">);</span><span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">bhuvi</span><span class=\"p\">.</span><span class=\"n\">test_data</span> <span class=\"p\">(</span><span class=\"n\">id</span><span class=\"p\">,</span> <span class=\"n\">first_name</span><span class=\"p\">,</span> <span class=\"n\">last_name</span><span class=\"p\">,</span> <span class=\"n\">email</span><span class=\"p\">,</span> <span class=\"n\">gender</span><span class=\"p\">,</span> <span class=\"n\">ip_address</span><span class=\"p\">)</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"s1\">'Corette'</span><span class=\"p\">,</span> <span class=\"s1\">'Tomasz'</span><span class=\"p\">,</span> <span class=\"s1\">'ctomasz2@miitbeian.gov.cn'</span><span class=\"p\">,</span> <span class=\"s1\">'Female'</span><span class=\"p\">,</span> <span class=\"s1\">'134.169.27.141'</span><span class=\"p\">);</span><span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">bhuvi</span><span class=\"p\">.</span><span class=\"n\">test_data</span> <span class=\"p\">(</span><span class=\"n\">id</span><span class=\"p\">,</span> <span class=\"n\">first_name</span><span class=\"p\">,</span> <span class=\"n\">last_name</span><span class=\"p\">,</span> <span class=\"n\">email</span><span class=\"p\">,</span> <span class=\"n\">gender</span><span class=\"p\">,</span> <span class=\"n\">ip_address</span><span class=\"p\">)</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s1\">'Margarita'</span><span class=\"p\">,</span> <span class=\"s1\">'Moulden'</span><span class=\"p\">,</span> <span class=\"s1\">'mmoulden3@google.com'</span><span class=\"p\">,</span> <span class=\"s1\">'Female'</span><span class=\"p\">,</span> <span class=\"s1\">'248.63.226.96'</span><span class=\"p\">);</span><span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">bhuvi</span><span class=\"p\">.</span><span class=\"n\">test_data</span> <span class=\"p\">(</span><span class=\"n\">id</span><span class=\"p\">,</span> <span class=\"n\">first_name</span><span class=\"p\">,</span> <span class=\"n\">last_name</span><span class=\"p\">,</span> <span class=\"n\">email</span><span class=\"p\">,</span> <span class=\"n\">gender</span><span class=\"p\">,</span> <span class=\"n\">ip_address</span><span class=\"p\">)</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"s1\">'Eran'</span><span class=\"p\">,</span> <span class=\"s1\">'McCoveney'</span><span class=\"p\">,</span> <span class=\"s1\">'emccoveney4@redcross.org'</span><span class=\"p\">,</span> <span class=\"s1\">'Female'</span><span class=\"p\">,</span> <span class=\"s1\">'19.137.118.110'</span><span class=\"p\">);</span><span class=\"k\">grant</span> <span class=\"k\">select</span><span class=\"p\">(</span><span class=\"n\">id</span><span class=\"p\">,</span><span class=\"n\">first_name</span><span class=\"p\">)</span> <span class=\"k\">on</span> <span class=\"n\">bhuvi</span><span class=\"p\">.</span><span class=\"n\">test_data</span> <span class=\"k\">to</span> <span class=\"n\">mv_user</span><span class=\"p\">;</span><span class=\"k\">GRANT</span><span class=\"k\">set</span> <span class=\"k\">session</span> <span class=\"k\">authorization</span> <span class=\"n\">mv_user</span><span class=\"p\">;</span><span class=\"k\">SET</span><span class=\"k\">CREATE</span> <span class=\"n\">MATERIALIZED</span> <span class=\"k\">VIEW</span> <span class=\"n\">bhuvi</span><span class=\"p\">.</span><span class=\"n\">mv_testdata</span> <span class=\"k\">as</span> <span class=\"p\">(</span><span class=\"k\">select</span> <span class=\"n\">id</span><span class=\"p\">,</span> <span class=\"n\">first_name</span> <span class=\"k\">from</span> <span class=\"n\">bhuvi</span><span class=\"p\">.</span><span class=\"n\">test_data</span><span class=\"p\">);</span><span class=\"n\">ERROR</span><span class=\"p\">:</span>  <span class=\"n\">permission</span> <span class=\"n\">denied</span> <span class=\"k\">for</span> <span class=\"k\">schema</span> <span class=\"n\">bhuvi</span></code></pre></figure><h2 id=\"conclusion\">Conclusion:</h2><p>Its not a big deal to work with column level ACL. But its worth to test every small feature. So you have better visibility about the feature and find the bugs like what we found above. I’ll update this blog once the AWS team confirms this as a bug or not.</p>",
            "url": "/2020/03/07/experimenting-aws-redshift-column-level-acl",
            "image": "/assets/Experimenting AWS RedShift Column Level ACL.jpg",
            
            
            "tags": ["aws","redshift","security"],
            
            "date_published": "2020-03-07T11:50:00+00:00",
            "date_modified": "2020-03-07T11:50:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2020/02/04/internals-of-google-cloud-spanner",
            "title": "Internals of Google Cloud Spanner",
            "summary": "Understand the internals of Google Cloud Spanner and how the Cloud Spanner works with regional and multi regional.",
            "content_text": "I have learned a lot more internal things about Google Cloud Spanner from past two days. I read some of the portions of the Spanner white paper and the deep internal things from the Google Cloud Next event videos from Youtube. I’ll share the video links here, but I want to summarize all the learnings in one place. Thats why I wrote this blog post. A special thanks to Deepti Srivastava(Product Manager for Spanner) who presented the Spanner Deep Dive sessions in the Google Cloud Next Event.The Pain with MySQL:In 2005,2006 Google was using the MySQL at massive scale. Google Adwords is one the biggest platform where 90+ MySQL Shards are used to store the data. Due to some maintenance they reshard the MySQL Clusters. This process took 2 years to complete. Google understood that they are growing very fast and these kinds of databases will be a pain in future. That is how the Spanner was born.BigTable and Spanner:Once they decided to build something new with distributed, the Big Table team was the one who started working for the Spanner process. Because BigTable uses distributed process, storage and highly available(or may be some other reasons as well).Colossus:Colossus is the distributed file system which is derived from the GFS. A high performance file system is needed for a super database. This project started by BigTable team and the BigTable is powered by Colossus. So Spanner also got the colossus as a filesystem.Why Spanner?:The Google Adwords is MySQL bases stack and a lot of fans for SQL(at 2005). They wanted to build something with SQL, its dealing with Money, so ACID compliance are the other main key point. The pain with MySQL is resharding. So they want the sharding features like the traditional NoSQL sharding that will take care of resharding and rebalancing. Plus more availability, Horizontal Scale and globally distributed.Spanner Architecture:Spanner is global database system, per region we’ll get minimum of 3 shards. Each shard will be in each zone. In Spanner terms a shard is called as Split. If your provision 1 Node Spanner cluster, you’ll get 2 more Nodes on the different zone which are invisible to you. And the Compute and Storage layers are de-coupled. Paxos algorithm is used to maintain one leader at a time and rest of the nodes will be the followers.Based on the partitions, we’ll have more Splits(shards) in the storage layer. Each shard will be replicated to the other Zones. For eg: if you have a shard called S1 on Zone A, it’ll be replicated to Zone B and C. The replication works based on Leader follower method. So the Paxos will help to maintain the quorum and will help to select a new Leader during the failure. If you are writing something on this Split, the Spanner APIs are aware of the Leaders. So the write directly goes to the Zone where it has the Leader Split. Each Split has its own leader zone.Global strong Consistency:when I was watching the deep dive video of Spanner, they were discussing the strong consistency. Spanner supports the strong consistency across all the nodes(Globally). If you write something on US region, you can read that same data from the Asia region or any other region. How they implemented this logic? Its called TrueTime.TrueTime:Spanner is very keen in syncronizing and maintains the same time across all the nodes over the global datacenters. Their hardwares are built with Atomic Clocks to maintain the time. If you take a look at the Server Hardware Rack, the Server is having 4 time servers. 2 Servers are connected with GPS and the remaining 2 are connect with Automic Oscillators. There are 2 different brands of Oscillators for better failover processing. The GPS time servers will sync with Oscillators to synchronize the time across the global datacenters with every 30sec interval.Now you may have a question, how this TrueTime will help with the Consistency? No worries, please scroll down.Consistency with TrueTimeTo understand the relationship between consistency and TrueTime, we have to understand how a write operation has been performed in Spanner. During every write operation the Spanner picks up the current TrueTime value and this TrueTime timestamp will create an order for the write operations. So every commit has been shipped with a timestamp.For Eg: If you are writing a data on Node 1, it’ll commit the data with the TrueTime timestamp and replicate the data and timestamp to the other nodes. This timestamp is same on all the nodes. Lets say we committed this data on Node 1, if you are reading the same data from the Node B, then the Spanner API will ask the leader of the Split for last committed data’s timestamp, if the timestamp is matching from the Node A’s timestamp then the data will be returned from Node B, else it’ll wait until the Node A sync the data to Node B and then it’ll return the data.Life cycle of a single row Write operation:Here is the life cycle of a single write operation. We are writing a row that will go to Split 2. Now the Spanner API will understand who is the leader node for Split 2, then the request will go to Zone B node(Blue indication refers to the leader). Then it’ll acquire the lock write it on the split. Once this write has been done, it’ll send the requests to Zone A and C Nodes to write the same. It’ll wait for the acknowledgement from the majority of the nodes. Once the leader split got the majority of the acknowledgement, then it’ll send the success response to the client.Multi Row write operation:If you are writing the data in a single transaction, but the data resides on different splits, then the spanner will handle it in a different way. For eg: we have to update 2 rows.  Row 1 is in Split1 - Zone C is the Leader Split  Row 2 is in Split2 - Zone B is the Leader SplitWhen we initiate the transaction, the Spanner API will understand that the rows are in different split. And they will randomly pick a Co-ordinator zone. In our example, the API has chosen the Zone C is the coordinator zone. The following steps will be performed for the multiple row operations.  Select coordinator zone. (Zone C)  Acquire the locks on the data on both leaders splits at the same time.  Add the new data on both Leader splits.Leader Splits will replicate the new data to the follower splits. And then Get the acknowledgement from the follower splits (Both splits will wait to get the acknowledgement).  Then zone B split will send a message to the Coordinator zone’s split that its done with the update and its ready to commit.  Then the Split1 in zone C will tell to the Split2, go ahead and commit the data. Same time, Split 1 also will commit.  The commit request will go to all the splits(both Leader and follower) and commit the data permanently.  And then the success response will go the client.Life of a Read operation:While reading the data from Spanner, the data will be fetched from the nearest follower split. Lets explain this with an example. Refer the below image.We want to read the data from MyTable, for the value 123. This value is stored in Split 2. Now once the request reached the Spanner Frontend server, then it’ll understand who is the nearest follower split and forward the request to that split. In our case, Zone A is the nearest split. Once the request reached the split, then that split will ask to the Leader split to get the last committed TrueTime. And then it’ll compare the Timestamp with its own timestamp. If both are matched then it’ll serve the data to the application. If the timestamps are not matched then the leader split will ask the follower to wait until it sync the data to that Zone. And then the split will serve the data.Stale/Time bounded read:Spanner support MVCC. So it’ll keep the old data for some period of time. If our applications are fine to get the old data (older than X seconds) then we don’t need to wait for data sync from the leader split. For example, We have to tell the Split that we are fine with 15sec old data, then it’ll check the committed timestamp and that is less than 15 seconds, then  the old data will be served to the application.Spanner with Multi Region:Till now, we read every operation scenarios within the region(zone level only), but Spanner is built for scale to multi regions. The architecture and write/read operations will have a slight difference in the multi region setup. In the single region concept, we need at least minimum of 3 zones to create the cluster. And the zones are supports both read and write. But in Multi region concept, One Continent will be act as a Leader and the rest of the Continent will be the followers. In Spanner terms, the Continent where we have more region will be the quorum. All the writes will go to any region in this continent. In the quorum continent, 2 regions will be hosting the data nodes, and 1 region will host the witness for failover. Other continents will have read only replica nodes.Consistency in Multi-region:In multi region concept, the wites are always performed on the Quorum continent. Lets say, US region is the R/W continent, then if you are sending a write request from the US region, then the Spanner API will send it the nearest region, once the data has been committed then the success response will go to the client. If you are sending a write request from Asia region, then the Asia region’s API servers will put the request into Google’s internal network and send the request to the US region’s API server. Then that US region API server will commit the data and the success response will be send it to Asia region client.For Reads, the process is same as single region concept, if the TrueTime matches, then the data will be served from the local region, else it’ll wait until the data sync to the local region and then served to the clients.Conclusion:I hope I covered most of the internal concepts of Spanner. But still there are a lot more things to learn in Cloud Spanner. Im sharing the Google Cloud Next event videos links.  Cloud Spanner 101  Cloud Spanner 201  Spanner Internals Part 1: What Makes Spanner Tick?  Spanner Internals Part 2: Global Meta-Data and Scalable Data Backend  Cloud Spanner: How It WorksAll the images which are using in this blog post are taken  from the Google Cloud’s youtube videos",
            "content_html": "<p>I have learned a lot more internal things about Google Cloud Spanner from past two days. I read some of the portions of the Spanner white paper and the deep internal things from the Google Cloud Next event videos from Youtube. I’ll share the video links here, but I want to summarize all the learnings in one place. Thats why I wrote this blog post. A special thanks to Deepti Srivastava(Product Manager for Spanner) who presented the Spanner Deep Dive sessions in the Google Cloud Next Event.</p><h2 id=\"the-pain-with-mysql\">The Pain with MySQL:</h2><p>In 2005,2006 Google was using the MySQL at massive scale. Google Adwords is one the biggest platform where 90+ MySQL Shards are used to store the data. Due to some maintenance they reshard the MySQL Clusters. This process took 2 years to complete. Google understood that they are growing very fast and these kinds of databases will be a pain in future. That is how the Spanner was born.</p><h2 id=\"bigtable-and-spanner\">BigTable and Spanner:</h2><p>Once they decided to build something new with distributed, the Big Table team was the one who started working for the Spanner process. Because BigTable uses distributed process, storage and highly available(or may be some other reasons as well).</p><h2 id=\"colossus\">Colossus:</h2><p>Colossus is the distributed file system which is derived from the GFS. A high performance file system is needed for a super database. This project started by BigTable team and the BigTable is powered by Colossus. So Spanner also got the colossus as a filesystem.</p><h2 id=\"why-spanner\">Why Spanner?:</h2><p>The Google Adwords is MySQL bases stack and a lot of fans for SQL(at 2005). They wanted to build something with SQL, its dealing with Money, so ACID compliance are the other main key point. The pain with MySQL is resharding. So they want the sharding features like the traditional NoSQL sharding that will take care of resharding and rebalancing. Plus more availability, Horizontal Scale and globally distributed.</p><h2 id=\"spanner-architecture\">Spanner Architecture:</h2><p>Spanner is global database system, per region we’ll get minimum of 3 shards. Each shard will be in each zone. In Spanner terms a shard is called as Split. If your provision 1 Node Spanner cluster, you’ll get 2 more Nodes on the different zone which are invisible to you. And the Compute and Storage layers are de-coupled. Paxos algorithm is used to maintain one leader at a time and rest of the nodes will be the followers.</p><p><img src=\"/assets/Internals of Google Cloud Spanner1.jpg\" alt=\"\" /></p><p>Based on the partitions, we’ll have more Splits(shards) in the storage layer. Each shard will be replicated to the other Zones. For eg: if you have a shard called S1 on Zone A, it’ll be replicated to Zone B and C. The replication works based on Leader follower method. So the Paxos will help to maintain the quorum and will help to select a new Leader during the failure. If you are writing something on this Split, the Spanner APIs are aware of the Leaders. So the write directly goes to the Zone where it has the Leader Split. Each Split has its own leader zone.</p><p><img src=\"/assets/Internals of Google Cloud Spanner2.jpg\" alt=\"\" /></p><h2 id=\"global-strong-consistency\">Global strong Consistency:</h2><p>when I was watching the deep dive video of Spanner, they were discussing the strong consistency. Spanner supports the strong consistency across all the nodes(Globally). If you write something on US region, you can read that same data from the Asia region or any other region. How they implemented this logic? Its called TrueTime.</p><h2 id=\"truetime\">TrueTime:</h2><p><img src=\"/assets/Internals of Google Cloud Spanner9.jpg\" alt=\"\" /></p><p>Spanner is very keen in syncronizing and maintains the same time across all the nodes over the global datacenters. Their hardwares are built with Atomic Clocks to maintain the time. If you take a look at the Server Hardware Rack, the Server is having 4 time servers. 2 Servers are connected with GPS and the remaining 2 are connect with Automic Oscillators. There are 2 different brands of Oscillators for better failover processing. The GPS time servers will sync with Oscillators to synchronize the time across the global datacenters with every 30sec interval.</p><p><img src=\"/assets/Internals of Google Cloud Spanner3.jpg\" alt=\"\" /></p><p>Now you may have a question, how this TrueTime will help with the Consistency? No worries, please scroll down.</p><h2 id=\"consistency-with-truetime\">Consistency with TrueTime</h2><p>To understand the relationship between consistency and TrueTime, we have to understand how a write operation has been performed in Spanner. During every write operation the Spanner picks up the current TrueTime value and this TrueTime timestamp will create an order for the write operations. So every commit has been shipped with a timestamp.</p><p><strong>For Eg:</strong> If you are writing a data on Node 1, it’ll commit the data with the TrueTime timestamp and replicate the data and timestamp to the other nodes. This timestamp is same on all the nodes. Lets say we committed this data on Node 1, if you are reading the same data from the Node B, then the Spanner API will ask the leader of the Split for last committed data’s timestamp, if the timestamp is matching from the Node A’s timestamp then the data will be returned from Node B, else it’ll wait until the Node A sync the data to Node B and then it’ll return the data.</p><h2 id=\"life-cycle-of-a-single-row-write-operation\">Life cycle of a single row Write operation:</h2><p>Here is the life cycle of a single write operation. We are writing a row that will go to Split 2. Now the Spanner API will understand who is the leader node for Split 2, then the request will go to Zone B node(Blue indication refers to the leader). Then it’ll acquire the lock write it on the split. Once this write has been done, it’ll send the requests to Zone A and C Nodes to write the same. It’ll wait for the acknowledgement from the majority of the nodes. Once the leader split got the majority of the acknowledgement, then it’ll send the success response to the client.</p><p><img src=\"/assets/Internals of Google Cloud Spanner4.jpg\" alt=\"\" /></p><h3 id=\"multi-row-write-operation\">Multi Row write operation:</h3><p>If you are writing the data in a single transaction, but the data resides on different splits, then the spanner will handle it in a different way. For eg: we have to update 2 rows.</p><ul>  <li>Row 1 is in Split1 - Zone C is the Leader Split</li>  <li>Row 2 is in Split2 - Zone B is the Leader Split</li></ul><p>When we initiate the transaction, the Spanner API will understand that the rows are in different split. And they will randomly pick a Co-ordinator zone. In our example, the API has chosen the Zone C is the coordinator zone. The following steps will be performed for the multiple row operations.</p><ol>  <li>Select coordinator zone. (Zone C)</li>  <li>Acquire the locks on the data on both leaders splits at the same time.</li>  <li>Add the new data on both Leader splits.Leader Splits will replicate the new data to the follower splits. And then Get the acknowledgement from the follower splits (Both splits will wait to get the acknowledgement).</li>  <li>Then zone B split will send a message to the Coordinator zone’s split that its done with the update and its ready to commit.</li>  <li>Then the Split1 in zone C will tell to the Split2, go ahead and commit the data. Same time, Split 1 also will commit.</li>  <li>The commit request will go to all the splits(both Leader and follower) and commit the data permanently.</li>  <li>And then the success response will go the client.</li></ol><p><img src=\"/assets/Internals of Google Cloud Spanner5.jpg\" alt=\"\" /></p><h2 id=\"life-of-a-read-operation\">Life of a Read operation:</h2><p>While reading the data from Spanner, the data will be fetched from the nearest follower split. Lets explain this with an example. Refer the below image.</p><p><img src=\"/assets/Internals of Google Cloud Spanner6.jpg\" alt=\"\" /></p><p>We want to read the data from MyTable, for the value 123. This value is stored in Split 2. Now once the request reached the Spanner Frontend server, then it’ll understand who is the nearest follower split and forward the request to that split. In our case, Zone A is the nearest split. Once the request reached the split, then that split will ask to the Leader split to get the last committed TrueTime. And then it’ll compare the Timestamp with its own timestamp. If both are matched then it’ll serve the data to the application. If the timestamps are not matched then the leader split will ask the follower to wait until it sync the data to that Zone. And then the split will serve the data.</p><h2 id=\"staletime-bounded-read\">Stale/Time bounded read:</h2><p>Spanner support MVCC. So it’ll keep the old data for some period of time. If our applications are fine to get the old data (older than X seconds) then we don’t need to wait for data sync from the leader split. For example, We have to tell the Split that we are fine with 15sec old data, then it’ll check the committed timestamp and that is less than 15 seconds, then  the old data will be served to the application.</p><p><img src=\"/assets/Internals of Google Cloud Spanner7.jpg\" alt=\"\" /></p><h2 id=\"spanner-with-multi-region\">Spanner with Multi Region:</h2><p>Till now, we read every operation scenarios within the region(zone level only), but Spanner is built for scale to multi regions. The architecture and write/read operations will have a slight difference in the multi region setup. In the single region concept, we need at least minimum of 3 zones to create the cluster. And the zones are supports both read and write. But in Multi region concept, One Continent will be act as a Leader and the rest of the Continent will be the followers. In Spanner terms, the Continent where we have more region will be the quorum. All the writes will go to any region in this continent. In the quorum continent, 2 regions will be hosting the data nodes, and 1 region will host the witness for failover. Other continents will have read only replica nodes.</p><p><img src=\"/assets/Internals of Google Cloud Spanner8.jpg\" alt=\"\" /></p><h3 id=\"consistency-in-multi-region\">Consistency in Multi-region:</h3><p>In multi region concept, the wites are always performed on the Quorum continent. Lets say, US region is the R/W continent, then if you are sending a write request from the US region, then the Spanner API will send it the nearest region, once the data has been committed then the success response will go to the client. If you are sending a write request from Asia region, then the Asia region’s API servers will put the request into Google’s internal network and send the request to the US region’s API server. Then that US region API server will commit the data and the success response will be send it to Asia region client.</p><p>For Reads, the process is same as single region concept, if the TrueTime matches, then the data will be served from the local region, else it’ll wait until the data sync to the local region and then served to the clients.</p><h2 id=\"conclusion\">Conclusion:</h2><p>I hope I covered most of the internal concepts of Spanner. But still there are a lot more things to learn in Cloud Spanner. Im sharing the Google Cloud Next event videos links.</p><ol>  <li><a href=\"https://www.youtube.com/watch?v=IfsTINNCooY\">Cloud Spanner 101</a></li>  <li><a href=\"https://www.youtube.com/watch?v=Tzhe7sUNDbg\">Cloud Spanner 201</a></li>  <li><a href=\"https://www.youtube.com/watch?v=nvlt0dA7rsQ\">Spanner Internals Part 1: What Makes Spanner Tick?</a></li>  <li><a href=\"https://www.youtube.com/watch?v=zy-rcR4MoN4\">Spanner Internals Part 2: Global Meta-Data and Scalable Data Backend</a></li>  <li><a href=\"https://www.youtube.com/watch?v=QPpSzxs_8bc\">Cloud Spanner: How It Works</a></li></ol><p><em>All the images which are using in this blog post are taken  from the Google Cloud’s youtube videos</em></p>",
            "url": "/2020/02/04/internals-of-google-cloud-spanner",
            "image": "/assets/Internals of Google Cloud Spanner.gif",
            
            
            "tags": ["distributed database","GCP","Cloud Spanner","NewSQL","Internals"],
            
            "date_published": "2020-02-04T16:35:00+00:00",
            "date_modified": "2020-02-04T16:35:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2020/01/21/debezium-mysql-snapshot-for-cloudsql-mysql-from-replica",
            "title": "Debezium MySQL Snapshot For CloudSQL(MySQL) From Replica",
            "summary": "Debezium take snapshot from GCP CloudSQL(MySQL) from Read Replica. We have to create a VM replica or CloudSQL Replica for snapshot. ",
            "content_text": "The snapshot in Debezium will do a historical data load from the source database to the Kafka topics. But generally its not a good practice to this if you have a huge data in your tables. Recently I have published many blog posts to perform this snapshot from Read Replica(with/without GTID, AWS Aurora). One guy commented that, in GCP the MySQL managed service is called CloudSQL. There we don’t have much control to stop replication, perform the modifications that we want. So how can we avoid snapshots in CloudSQL and take debezium snapshots from CloudSQL Read Replica? I have spent some time today and figured out a way to do this.The Approach:We can’t enable binlogs on read replica. So we have to setup an external read replica for this. If the external replica is a VM, then we can enable the log-slave-updates with GTID. Then we can follow this blog post to solve this problem. But I want to solve this by using CloudSQL read replica.  For this create a new read replica for your cloudsql.  If you already have a read replica, then create one more (because we’ll break the replication, so don’t use the active one).  Disable the replication on the new Replica and make a note of the master’s binlog information.  Then Promote the replica. So it’ll automatically enable the binlog.  Now create a connector to read data from the replica node.  Once the snapshot is done, manually update the connect-offset with Master’s binlog info.  Update the connector with Master’s IP address.  Then it’ll read from Master.Proof Of Concept:Read my previous blog posts to install and configure Confluent Kafka connect and etc.  CloudSQL Master IP  - 172.24.0.13  CloudSQL Replica IP - 172.24.0.19Sample data:Create a new database to test this sync and insert some values.create database bhuvi;use bhuvi;create table rohi (id int,fn varchar(10),ln varchar(10),phone int);insert into rohi values (1, 'rohit', 'last',87611);insert into rohi values (2, 'rohit', 'last',87611);insert into rohi values (3, 'rohit', 'last',87611);insert into rohi values (4, 'rohit', 'last',87611);insert into rohi values (5, 'rohit', 'last',87611);Once your replica is in sync with Master, disable the replication from the CloudSQL Console.Then login to the Replica and get the master’s binlog file name and position from the show slave statusMaster_Host: 172.24.0.13Master_User: cloudsqlreplicaMaster_Port: 3306Connect_Retry: 60Master_Log_File: mysql-bin.000001Read_Master_Log_Pos: 10259755Relay_Log_File: relay-log.000002Relay_Log_Pos: 26567Relay_Master_Log_File: mysql-bin.000001Slave_IO_Running: NoSlave_SQL_Running: NoLast_Error:Skip_Counter: 0Exec_Master_Log_Pos: 10259755  Master_UUID: c3c1d467-3bee-11ea-907b-4201ac18000c  Binlog File Name - Master_Log_File  Binlog Position - 10259755  Maser’s UUID - Master_UUIDNow promote the replica, it’ll enable the binlog on this replica server.To simulate the complexity, add one more row on the master node(this row will not be replicated, since the replication is disables). So once the snapshot done, we’ll switch the MySQL IP. Then it should read this new row.insert into rohi values (6, 'rohit', 'last',87611);Create the MySQL Connector:Use the below MySQL connector JSON config.  (replace the MySQL details, kafka details, and Transformation things){\"name\": \"mysql-connector-db01\",\"config\": {\"name\": \"mysql-connector-db01\",\"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\",\"database.server.id\": \"1\",\"tasks.max\": \"3\",\"database.history.kafka.bootstrap.servers\": \"10.128.0.12:9092\",\"database.history.kafka.topic\": \"replica-schema-changes.mysql\",\"database.server.name\": \"mysql-db01\",\"database.hostname\": \"172.24.0.19\",\"database.port\": \"3306\",\"database.user\": \"root\",\"database.password\": \"****\",\"database.whitelist\": \"bhuvi\",\"internal.key.converter.schemas.enable\": \"false\",\"key.converter.schemas.enable\": \"false\",\"internal.key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\"internal.value.converter.schemas.enable\": \"false\",\"value.converter.schemas.enable\": \"false\",\"internal.value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\"key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\"transforms\": \"unwrap\",\"transforms.unwrap.type\": \"io.debezium.transforms.ExtractNewRecordState\",\"transforms.unwrap.add.source.fields\": \"ts_ms\",\"tombstones.on.delete\": false}}Install the Connector:curl -X POST -H \"Accept: application/json\" -H \"Content-Type: application/json\" http://localhost:8083/connectors -d @mysql.jsonIngest the master binlog info:Get the last read binlog info from the Kafka topic and manually add the master’s binlog into it.kafkacat -b localhost:9092 -C -t connect-offsets  -f 'Partition(%p) %k %s\\n'Partition(1) [\"mysql-connector-db01\",{\"server\":\"mysql-db01\"}] {\"file\":\"mysql-bin.000004\",\"pos\":89114,\"gtids\":\"4ac96fbf-3c4b-11ea-8ea0-4201ac180012:1-19,c3c1d467-3bee-11ea-907b-4201ac18000c:1-36003\"}Now replica the binlog file name, position,  UUID. (sometimes you’ll get multiple UUID, so remove everything, just keep the master’s UUID)echo '[\"mysql-connector-db01\",{\"server\":\"mysql-db01\"}]|{\"file\":\"mysql-bin.000001\",\"pos\":10259755,\"gtids\":\"c3c1d467-3bee-11ea-907b-4201ac18000c:1-36003\"}' | kafkacat -P -b localhost -t connect-offsets -K \\| -p 1Check this data in connect-offsetskafka-console-consumer --bootstrap-server localhost:9092 --topic connect-offsets --from-beginning{\"file\":\"mysql-bin.000004\",\"pos\":67739,\"gtids\":\"4ac96fbf-3c4b-11ea-8ea0-4201ac180012:1-237,c3c1d467-3bee-11ea-907b-4201ac18000c:1-36003\"}{\"file\":\"mysql-bin.000001\",\"pos\":10259755,\"gtids\":\"c3c1d467-3bee-11ea-907b-4201ac18000c:1-36003\"}Update the connector:Use the below Config file to update it. Make sure the database.history.kafka.topic will be a new topic andsnapshot.mode must be SCHEMA_ONLY_RECOVERY. And the update the MySQL details, kafka.{\"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\",\"snapshot.locking.mode\": \"none\",\"tasks.max\": \"3\",\"database.history.kafka.topic\": \"master-schema-changes.mysql\",\"transforms\": \"unwrap\",\"internal.key.converter.schemas.enable\": \"false\",\"transforms.unwrap.add.source.fields\": \"ts_ms\",\"tombstones.on.delete\": \"false\",\"transforms.unwrap.type\": \"io.debezium.transforms.ExtractNewRecordState\",\"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\"database.whitelist\": \"bhuvi\",\"key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\"database.user\": \"root\",\"database.server.id\": \"1\",\"database.history.kafka.bootstrap.servers\": \"10.128.0.12:9092\",\"database.server.name\": \"mysql-db01\",\"database.port\": \"3306\",\"key.converter.schemas.enable\": \"false\",\"internal.key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\"database.hostname\": \"172.24.0.13\",\"database.password\": \"****\",\"internal.value.converter.schemas.enable\": \"false\",\"name\": \"mysql-connector-db01\",\"value.converter.schemas.enable\": \"false\",\"internal.value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\"snapshot.mode\": \"SCHEMA_ONLY_RECOVERY\"}Once its updated, check the status of the connector.curl GET localhost:8083/connectors/mysql-connector-db01/status{\"name\": \"mysql-connector-db01\",\"connector\": {\"state\": \"RUNNING\",\"worker_id\": \"10.128.0.14:8083\"},\"tasks\": \\[{\"id\": 0,\"state\": \"RUNNING\",\"worker_id\": \"10.128.0.14:8083\"}\\],\"type\": \"source\"}Check the binlog inforation from the connect-offset topic.kafka-console-consumer --bootstrap-server localhost:9092 --topic connect-offsets --from-beginning{\"file\":\"mysql-bin.000004\",\"pos\":67739,\"gtids\":\"4ac96fbf-3c4b-11ea-8ea0-4201ac180012:1-237,c3c1d467-3bee-11ea-907b-4201ac18000c:1-36003\"}{\"file\":\"mysql-bin.000001\",\"pos\":10259755,\"gtids\":\"c3c1d467-3bee-11ea-907b-4201ac18000c:1-36003\"}{\"ts_sec\":1579610781,\"file\":\"mysql-bin.000001\",\"pos\":10263525,\"gtids\":\"c3c1d467-3bee-11ea-907b-4201ac18000c:1-36016\",\"row\":1,\"server_id\":128423640,\"event\":2}You can see the thrid line which is nothing but the 6th row that we inserted on master after disabling the replication. Now insert one more row and see whether is read by the connector or not.mysql&gt; insert into rohi values (7, 'rohit', 'last',87611);kafka-console-consumer --bootstrap-server localhost:9092 --topic connect-offsets --from-beginning{\"file\":\"mysql-bin.000004\",\"pos\":67739,\"gtids\":\"4ac96fbf-3c4b-11ea-8ea0-4201ac180012:1-237,c3c1d467-3bee-11ea-907b-4201ac18000c:1-36003\"}{\"file\":\"mysql-bin.000001\",\"pos\":10259755,\"gtids\":\"c3c1d467-3bee-11ea-907b-4201ac18000c:1-36003\"}{\"ts_sec\":1579610781,\"file\":\"mysql-bin.000001\",\"pos\":10263525,\"gtids\":\"c3c1d467-3bee-11ea-907b-4201ac18000c:1-36016\",\"row\":1,\"server_id\":128423640,\"event\":2}{\"ts_sec\":1579612311,\"file\":\"mysql-bin.000001\",\"pos\":10350729,\"gtids\":\"c3c1d467-3bee-11ea-907b-4201ac18000c:1-36322\",\"row\":1,\"server_id\":128423640,\"event\":2}Check the data in the table’s data from Kafakakafka-console-consumer --bootstrap-server localhost:9092 --topic mysql-db01.bhuvi.rohi --from-beginning{\"id\":1,\"fn\":\"rohit\",\"ln\":\"last\",\"phone\":87611,\"__ts_ms\":0}{\"id\":2,\"fn\":\"rohit\",\"ln\":\"last\",\"phone\":87611,\"__ts_ms\":0}{\"id\":3,\"fn\":\"rohit\",\"ln\":\"last\",\"phone\":87611,\"__ts_ms\":0}{\"id\":4,\"fn\":\"rohit\",\"ln\":\"last\",\"phone\":87611,\"__ts_ms\":0}{\"id\":5,\"fn\":\"rohit\",\"ln\":\"last\",\"phone\":87611,\"__ts_ms\":0}{\"id\":6,\"fn\":\"rohit\",\"ln\":\"last\",\"phone\":87611,\"__ts_ms\":1579610781000}{\"id\":7,\"fn\":\"rohit\",\"ln\":\"last\",\"phone\":87611,\"__ts_ms\":1579612311000}Debezium Series blogs:  Build Production Grade Debezium Cluster With Confluent Kafka  Monitor Debezium MySQL Connector With Prometheus And Grafana  Debezium MySQL Snapshot From Read Replica With GTID  Debezium MySQL Snapshot From Read Replica And Resume From Master  Debezium MySQL Snapshot For AWS RDS Aurora From Backup Snaphot  RealTime CDC From MySQL Using AWS MSK With Debezium",
            "content_html": "<p>The snapshot in Debezium will do a historical data load from the source database to the Kafka topics. But generally its not a good practice to this if you have a huge data in your tables. Recently I have published many blog posts to perform this snapshot from Read Replica(with/without GTID, AWS Aurora). One guy commented that, in GCP the MySQL managed service is called CloudSQL. There we don’t have much control to stop replication, perform the modifications that we want. So how can we avoid snapshots in CloudSQL and take debezium snapshots from CloudSQL Read Replica? I have spent some time today and figured out a way to do this.</p><h2 id=\"the-approach\">The Approach:</h2><p>We can’t enable binlogs on read replica. So we have to setup an external read replica for this. If the external replica is a VM, then we can enable the <code class=\"language-html highlighter-rouge\">log-slave-updates</code> with GTID. Then we can <a href=\"https://thedataguy.in/debezium-mysql-snapshot-from-read-replica-with-gtid/\"><strong>follow this blog post</strong></a> to solve this problem. But I want to solve this by using CloudSQL read replica.</p><ul>  <li>For this create a new read replica for your cloudsql.</li>  <li>If you already have a read replica, then create one more (because we’ll break the replication, so don’t use the active one).</li>  <li>Disable the replication on the new Replica and make a note of the master’s binlog information.</li>  <li>Then Promote the replica. So it’ll automatically enable the binlog.</li>  <li>Now create a connector to read data from the replica node.</li>  <li>Once the snapshot is done, manually update the <code class=\"language-html highlighter-rouge\">connect-offset</code> with Master’s binlog info.</li>  <li>Update the connector with Master’s IP address.</li>  <li>Then it’ll read from Master.</li></ul><h2 id=\"proof-of-concept\">Proof Of Concept:</h2><p>Read my previous blog posts to install and configure Confluent Kafka connect and etc.</p><ul>  <li>CloudSQL Master IP  - 172.24.0.13</li>  <li>CloudSQL Replica IP - 172.24.0.19</li></ul><h2 id=\"sample-data\">Sample data:</h2><p>Create a new database to test this sync and insert some values.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">create</span> <span class=\"k\">database</span> <span class=\"n\">bhuvi</span><span class=\"p\">;</span><span class=\"n\">use</span> <span class=\"n\">bhuvi</span><span class=\"p\">;</span><span class=\"k\">create</span> <span class=\"k\">table</span> <span class=\"n\">rohi</span> <span class=\"p\">(</span><span class=\"n\">id</span> <span class=\"nb\">int</span><span class=\"p\">,</span><span class=\"n\">fn</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">),</span><span class=\"n\">ln</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">),</span><span class=\"n\">phone</span> <span class=\"nb\">int</span><span class=\"p\">);</span><span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">rohi</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">'rohit'</span><span class=\"p\">,</span> <span class=\"s1\">'last'</span><span class=\"p\">,</span><span class=\"mi\">87611</span><span class=\"p\">);</span><span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">rohi</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"s1\">'rohit'</span><span class=\"p\">,</span> <span class=\"s1\">'last'</span><span class=\"p\">,</span><span class=\"mi\">87611</span><span class=\"p\">);</span><span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">rohi</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"s1\">'rohit'</span><span class=\"p\">,</span> <span class=\"s1\">'last'</span><span class=\"p\">,</span><span class=\"mi\">87611</span><span class=\"p\">);</span><span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">rohi</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s1\">'rohit'</span><span class=\"p\">,</span> <span class=\"s1\">'last'</span><span class=\"p\">,</span><span class=\"mi\">87611</span><span class=\"p\">);</span><span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">rohi</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"s1\">'rohit'</span><span class=\"p\">,</span> <span class=\"s1\">'last'</span><span class=\"p\">,</span><span class=\"mi\">87611</span><span class=\"p\">);</span></code></pre></figure><p>Once your replica is in sync with Master, disable the replication from the CloudSQL Console.</p><p><img src=\"/assets/Debezium MySQL Snapshot For CloudSQL(MySQL) From Replica1.jpg\" alt=\"\" /></p><p>Then login to the Replica and get the master’s binlog file name and position from the <code class=\"language-html highlighter-rouge\">show slave status</code></p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"n\">Master_Host</span><span class=\"p\">:</span> <span class=\"mi\">172</span><span class=\"p\">.</span><span class=\"mi\">24</span><span class=\"p\">.</span><span class=\"mi\">0</span><span class=\"p\">.</span><span class=\"mi\">13</span><span class=\"n\">Master_User</span><span class=\"p\">:</span> <span class=\"n\">cloudsqlreplica</span><span class=\"n\">Master_Port</span><span class=\"p\">:</span> <span class=\"mi\">3306</span><span class=\"n\">Connect_Retry</span><span class=\"p\">:</span> <span class=\"mi\">60</span><span class=\"n\">Master_Log_File</span><span class=\"p\">:</span> <span class=\"n\">mysql</span><span class=\"o\">-</span><span class=\"n\">bin</span><span class=\"p\">.</span><span class=\"mi\">000001</span><span class=\"n\">Read_Master_Log_Pos</span><span class=\"p\">:</span> <span class=\"mi\">10259755</span><span class=\"n\">Relay_Log_File</span><span class=\"p\">:</span> <span class=\"n\">relay</span><span class=\"o\">-</span><span class=\"n\">log</span><span class=\"p\">.</span><span class=\"mi\">000002</span><span class=\"n\">Relay_Log_Pos</span><span class=\"p\">:</span> <span class=\"mi\">26567</span><span class=\"n\">Relay_Master_Log_File</span><span class=\"p\">:</span> <span class=\"n\">mysql</span><span class=\"o\">-</span><span class=\"n\">bin</span><span class=\"p\">.</span><span class=\"mi\">000001</span><span class=\"n\">Slave_IO_Running</span><span class=\"p\">:</span> <span class=\"k\">No</span><span class=\"n\">Slave_SQL_Running</span><span class=\"p\">:</span> <span class=\"k\">No</span><span class=\"n\">Last_Error</span><span class=\"p\">:</span><span class=\"n\">Skip_Counter</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"n\">Exec_Master_Log_Pos</span><span class=\"p\">:</span> <span class=\"mi\">10259755</span>  <span class=\"n\">Master_UUID</span><span class=\"p\">:</span> <span class=\"n\">c3c1d467</span><span class=\"o\">-</span><span class=\"mi\">3</span><span class=\"n\">bee</span><span class=\"o\">-</span><span class=\"mi\">11</span><span class=\"n\">ea</span><span class=\"o\">-</span><span class=\"mi\">907</span><span class=\"n\">b</span><span class=\"o\">-</span><span class=\"mi\">4201</span><span class=\"n\">ac18000c</span></code></pre></figure><ul>  <li>Binlog File Name - Master_Log_File</li>  <li>Binlog Position - 10259755</li>  <li>Maser’s UUID - Master_UUID</li></ul><p>Now promote the replica, it’ll enable the binlog on this replica server.</p><p><img src=\"/assets/Debezium MySQL Snapshot For CloudSQL(MySQL) From Replica2.jpg\" alt=\"\" /></p><p>To simulate the complexity, add one more row on the master node(this row will not be replicated, since the replication is disables). So once the snapshot done, we’ll switch the MySQL IP. Then it should read this new row.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">rohi</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"s1\">'rohit'</span><span class=\"p\">,</span> <span class=\"s1\">'last'</span><span class=\"p\">,</span><span class=\"mi\">87611</span><span class=\"p\">);</span></code></pre></figure><h2 id=\"create-the-mysql-connector\">Create the MySQL Connector:</h2><p>Use the below MySQL connector JSON config.  (replace the MySQL details, kafka details, and Transformation things)</p><figure class=\"highlight\"><pre><code class=\"language-json\" data-lang=\"json\"><span class=\"p\">{</span><span class=\"w\"></span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"mysql-connector-db01\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"config\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"mysql-connector-db01\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"connector.class\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"io.debezium.connector.mysql.MySqlConnector\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.server.id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"1\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"tasks.max\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"3\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.history.kafka.bootstrap.servers\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"10.128.0.12:9092\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.history.kafka.topic\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"replica-schema-changes.mysql\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.server.name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"mysql-db01\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.hostname\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"172.24.0.19\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.port\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"3306\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.user\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"root\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.password\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"****\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.whitelist\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"bhuvi\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"internal.key.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"key.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"internal.key.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"internal.value.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"value.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"internal.value.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"value.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"key.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"transforms\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"unwrap\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"transforms.unwrap.type\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"io.debezium.transforms.ExtractNewRecordState\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"transforms.unwrap.add.source.fields\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"ts_ms\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"tombstones.on.delete\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">false</span><span class=\"w\"></span><span class=\"p\">}</span><span class=\"w\"></span><span class=\"p\">}</span></code></pre></figure><h3 id=\"install-the-connector\">Install the Connector:</h3><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>curl -X POST -H \"Accept: application/json\" -H \"Content-Type: application/json\" http://localhost:8083/connectors -d @mysql.json</code></pre></div></div><h3 id=\"ingest-the-master-binlog-info\">Ingest the master binlog info:</h3><p>Get the last read binlog info from the Kafka topic and manually add the master’s binlog into it.</p><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>kafkacat -b localhost:9092 -C -t connect-offsets  -f 'Partition(%p) %k %s\\n'Partition(1) [\"mysql-connector-db01\",{\"server\":\"mysql-db01\"}] {\"file\":\"mysql-bin.000004\",\"pos\":89114,\"gtids\":\"4ac96fbf-3c4b-11ea-8ea0-4201ac180012:1-19,c3c1d467-3bee-11ea-907b-4201ac18000c:1-36003\"}</code></pre></div></div><p>Now replica the binlog file name, position,  UUID. (sometimes you’ll get multiple UUID, so remove everything, just keep the master’s UUID)</p><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>echo '[\"mysql-connector-db01\",{\"server\":\"mysql-db01\"}]|{\"file\":\"mysql-bin.000001\",\"pos\":10259755,\"gtids\":\"c3c1d467-3bee-11ea-907b-4201ac18000c:1-36003\"}' | kafkacat -P -b localhost -t connect-offsets -K \\| -p 1</code></pre></div></div><p>Check this data in <code class=\"language-html highlighter-rouge\">connect-offsets</code></p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">kafka-console-consumer <span class=\"nt\">--bootstrap-server</span> localhost:9092 <span class=\"nt\">--topic</span> connect-offsets <span class=\"nt\">--from-beginning</span><span class=\"o\">{</span><span class=\"s2\">\"file\"</span>:<span class=\"s2\">\"mysql-bin.000004\"</span>,<span class=\"s2\">\"pos\"</span>:67739,<span class=\"s2\">\"gtids\"</span>:<span class=\"s2\">\"4ac96fbf-3c4b-11ea-8ea0-4201ac180012:1-237,c3c1d467-3bee-11ea-907b-4201ac18000c:1-36003\"</span><span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"file\"</span>:<span class=\"s2\">\"mysql-bin.000001\"</span>,<span class=\"s2\">\"pos\"</span>:10259755,<span class=\"s2\">\"gtids\"</span>:<span class=\"s2\">\"c3c1d467-3bee-11ea-907b-4201ac18000c:1-36003\"</span><span class=\"o\">}</span></code></pre></figure><h3 id=\"update-the-connector\">Update the connector:</h3><p>Use the below Config file to update it. Make sure the <code class=\"language-html highlighter-rouge\">database.history.kafka.topic</code> will be a new topic and<code class=\"language-html highlighter-rouge\">snapshot.mode</code> must be <code class=\"language-html highlighter-rouge\">SCHEMA_ONLY_RECOVERY</code>. And the update the MySQL details, kafka.</p><figure class=\"highlight\"><pre><code class=\"language-json\" data-lang=\"json\"><span class=\"p\">{</span><span class=\"w\"></span><span class=\"nl\">\"connector.class\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"io.debezium.connector.mysql.MySqlConnector\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"snapshot.locking.mode\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"none\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"tasks.max\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"3\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.history.kafka.topic\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"master-schema-changes.mysql\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"transforms\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"unwrap\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"internal.key.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"transforms.unwrap.add.source.fields\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"ts_ms\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"tombstones.on.delete\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"transforms.unwrap.type\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"io.debezium.transforms.ExtractNewRecordState\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"value.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.whitelist\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"bhuvi\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"key.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.user\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"root\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.server.id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"1\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.history.kafka.bootstrap.servers\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"10.128.0.12:9092\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.server.name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"mysql-db01\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.port\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"3306\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"key.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"internal.key.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.hostname\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"172.24.0.13\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.password\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"****\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"internal.value.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"mysql-connector-db01\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"value.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"internal.value.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"snapshot.mode\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"SCHEMA_ONLY_RECOVERY\"</span><span class=\"w\"></span><span class=\"p\">}</span></code></pre></figure><p>Once its updated, check the status of the connector.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">curl GET localhost:8083/connectors/mysql-connector-db01/status<span class=\"o\">{</span><span class=\"s2\">\"name\"</span>: <span class=\"s2\">\"mysql-connector-db01\"</span>,<span class=\"s2\">\"connector\"</span>: <span class=\"o\">{</span><span class=\"s2\">\"state\"</span>: <span class=\"s2\">\"RUNNING\"</span>,<span class=\"s2\">\"worker_id\"</span>: <span class=\"s2\">\"10.128.0.14:8083\"</span><span class=\"o\">}</span>,<span class=\"s2\">\"tasks\"</span>: <span class=\"se\">\\[</span><span class=\"o\">{</span><span class=\"s2\">\"id\"</span>: 0,<span class=\"s2\">\"state\"</span>: <span class=\"s2\">\"RUNNING\"</span>,<span class=\"s2\">\"worker_id\"</span>: <span class=\"s2\">\"10.128.0.14:8083\"</span><span class=\"o\">}</span><span class=\"se\">\\]</span>,<span class=\"s2\">\"type\"</span>: <span class=\"s2\">\"source\"</span><span class=\"o\">}</span></code></pre></figure><p>Check the binlog inforation from the connect-offset topic.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">kafka-console-consumer <span class=\"nt\">--bootstrap-server</span> localhost:9092 <span class=\"nt\">--topic</span> connect-offsets <span class=\"nt\">--from-beginning</span><span class=\"o\">{</span><span class=\"s2\">\"file\"</span>:<span class=\"s2\">\"mysql-bin.000004\"</span>,<span class=\"s2\">\"pos\"</span>:67739,<span class=\"s2\">\"gtids\"</span>:<span class=\"s2\">\"4ac96fbf-3c4b-11ea-8ea0-4201ac180012:1-237,c3c1d467-3bee-11ea-907b-4201ac18000c:1-36003\"</span><span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"file\"</span>:<span class=\"s2\">\"mysql-bin.000001\"</span>,<span class=\"s2\">\"pos\"</span>:10259755,<span class=\"s2\">\"gtids\"</span>:<span class=\"s2\">\"c3c1d467-3bee-11ea-907b-4201ac18000c:1-36003\"</span><span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"ts_sec\"</span>:1579610781,<span class=\"s2\">\"file\"</span>:<span class=\"s2\">\"mysql-bin.000001\"</span>,<span class=\"s2\">\"pos\"</span>:10263525,<span class=\"s2\">\"gtids\"</span>:<span class=\"s2\">\"c3c1d467-3bee-11ea-907b-4201ac18000c:1-36016\"</span>,<span class=\"s2\">\"row\"</span>:1,<span class=\"s2\">\"server_id\"</span>:128423640,<span class=\"s2\">\"event\"</span>:2<span class=\"o\">}</span></code></pre></figure><p>You can see the thrid line which is nothing but the 6th row that we inserted on master after disabling the replication. Now insert one more row and see whether is read by the connector or not.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"n\">mysql</span><span class=\"o\">&gt;</span> <span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">rohi</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"s1\">'rohit'</span><span class=\"p\">,</span> <span class=\"s1\">'last'</span><span class=\"p\">,</span><span class=\"mi\">87611</span><span class=\"p\">);</span></code></pre></figure><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">kafka-console-consumer <span class=\"nt\">--bootstrap-server</span> localhost:9092 <span class=\"nt\">--topic</span> connect-offsets <span class=\"nt\">--from-beginning</span><span class=\"o\">{</span><span class=\"s2\">\"file\"</span>:<span class=\"s2\">\"mysql-bin.000004\"</span>,<span class=\"s2\">\"pos\"</span>:67739,<span class=\"s2\">\"gtids\"</span>:<span class=\"s2\">\"4ac96fbf-3c4b-11ea-8ea0-4201ac180012:1-237,c3c1d467-3bee-11ea-907b-4201ac18000c:1-36003\"</span><span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"file\"</span>:<span class=\"s2\">\"mysql-bin.000001\"</span>,<span class=\"s2\">\"pos\"</span>:10259755,<span class=\"s2\">\"gtids\"</span>:<span class=\"s2\">\"c3c1d467-3bee-11ea-907b-4201ac18000c:1-36003\"</span><span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"ts_sec\"</span>:1579610781,<span class=\"s2\">\"file\"</span>:<span class=\"s2\">\"mysql-bin.000001\"</span>,<span class=\"s2\">\"pos\"</span>:10263525,<span class=\"s2\">\"gtids\"</span>:<span class=\"s2\">\"c3c1d467-3bee-11ea-907b-4201ac18000c:1-36016\"</span>,<span class=\"s2\">\"row\"</span>:1,<span class=\"s2\">\"server_id\"</span>:128423640,<span class=\"s2\">\"event\"</span>:2<span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"ts_sec\"</span>:1579612311,<span class=\"s2\">\"file\"</span>:<span class=\"s2\">\"mysql-bin.000001\"</span>,<span class=\"s2\">\"pos\"</span>:10350729,<span class=\"s2\">\"gtids\"</span>:<span class=\"s2\">\"c3c1d467-3bee-11ea-907b-4201ac18000c:1-36322\"</span>,<span class=\"s2\">\"row\"</span>:1,<span class=\"s2\">\"server_id\"</span>:128423640,<span class=\"s2\">\"event\"</span>:2<span class=\"o\">}</span></code></pre></figure><h3 id=\"check-the-data-in-the-tables-data-from-kafaka\">Check the data in the table’s data from Kafaka</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">kafka-console-consumer <span class=\"nt\">--bootstrap-server</span> localhost:9092 <span class=\"nt\">--topic</span> mysql-db01.bhuvi.rohi <span class=\"nt\">--from-beginning</span><span class=\"o\">{</span><span class=\"s2\">\"id\"</span>:1,<span class=\"s2\">\"fn\"</span>:<span class=\"s2\">\"rohit\"</span>,<span class=\"s2\">\"ln\"</span>:<span class=\"s2\">\"last\"</span>,<span class=\"s2\">\"phone\"</span>:87611,<span class=\"s2\">\"__ts_ms\"</span>:0<span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"id\"</span>:2,<span class=\"s2\">\"fn\"</span>:<span class=\"s2\">\"rohit\"</span>,<span class=\"s2\">\"ln\"</span>:<span class=\"s2\">\"last\"</span>,<span class=\"s2\">\"phone\"</span>:87611,<span class=\"s2\">\"__ts_ms\"</span>:0<span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"id\"</span>:3,<span class=\"s2\">\"fn\"</span>:<span class=\"s2\">\"rohit\"</span>,<span class=\"s2\">\"ln\"</span>:<span class=\"s2\">\"last\"</span>,<span class=\"s2\">\"phone\"</span>:87611,<span class=\"s2\">\"__ts_ms\"</span>:0<span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"id\"</span>:4,<span class=\"s2\">\"fn\"</span>:<span class=\"s2\">\"rohit\"</span>,<span class=\"s2\">\"ln\"</span>:<span class=\"s2\">\"last\"</span>,<span class=\"s2\">\"phone\"</span>:87611,<span class=\"s2\">\"__ts_ms\"</span>:0<span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"id\"</span>:5,<span class=\"s2\">\"fn\"</span>:<span class=\"s2\">\"rohit\"</span>,<span class=\"s2\">\"ln\"</span>:<span class=\"s2\">\"last\"</span>,<span class=\"s2\">\"phone\"</span>:87611,<span class=\"s2\">\"__ts_ms\"</span>:0<span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"id\"</span>:6,<span class=\"s2\">\"fn\"</span>:<span class=\"s2\">\"rohit\"</span>,<span class=\"s2\">\"ln\"</span>:<span class=\"s2\">\"last\"</span>,<span class=\"s2\">\"phone\"</span>:87611,<span class=\"s2\">\"__ts_ms\"</span>:1579610781000<span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"id\"</span>:7,<span class=\"s2\">\"fn\"</span>:<span class=\"s2\">\"rohit\"</span>,<span class=\"s2\">\"ln\"</span>:<span class=\"s2\">\"last\"</span>,<span class=\"s2\">\"phone\"</span>:87611,<span class=\"s2\">\"__ts_ms\"</span>:1579612311000<span class=\"o\">}</span></code></pre></figure><h3 id=\"debezium-series-blogs\">Debezium Series blogs:</h3><ol>  <li><a href=\"https://thedataguy.in/build-production-grade-debezium-with-confluent-kafka-cluster/\">Build Production Grade Debezium Cluster With Confluent Kafka</a></li>  <li><a href=\"https://thedataguy.in/monitor-debezium-mysql-connector-with-prometheus-and-grafana/\">Monitor Debezium MySQL Connector With Prometheus And Grafana</a></li>  <li><a href=\"https://thedataguy.in/debezium-mysql-snapshot-from-read-replica-with-gtid/\">Debezium MySQL Snapshot From Read Replica With GTID</a></li>  <li><a href=\"https://thedataguy.in/debezium-mysql-snapshot-from-read-replica-and-resume-from-master/\">Debezium MySQL Snapshot From Read Replica And Resume From Master</a></li>  <li><a href=\"https://thedataguy.in/debezium-mysql-snapshot-for-aws-rds-aurora-from-backup-snaphot/\">Debezium MySQL Snapshot For AWS RDS Aurora From Backup Snaphot</a></li>  <li><a href=\"https://medium.com/searce/realtime-cdc-from-mysql-using-aws-msk-with-debezium-28da5a4ca873\">RealTime CDC From MySQL Using AWS MSK With Debezium</a></li></ol>",
            "url": "/2020/01/21/debezium-mysql-snapshot-for-cloudsql-mysql-from-replica",
            "image": "/assets/Debezium MySQL Snapshot For CloudSQL(MySQL) From Replica.jpg",
            
            
            "tags": ["kafka","debezium","mysql","replication"],
            
            "date_published": "2020-01-21T14:16:00+00:00",
            "date_modified": "2020-01-21T14:16:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2020/01/19/setup-multi-data-center-neo4j-cluster-in-aws-and-gcp",
            "title": "Setup Multi Data Center Neo4j Cluster In AWS and GCP",
            "summary": "In this blog we covered how to setup a multi datacenter or multi region neo4j cluster on AWS and GCP. ",
            "content_text": "Neo4j’s multi datacenter deployments are well suited for a geo-distributed workload and also provide a better disaster recovery solution. But to be frank, its not an actual distributed databases like Google Spanner or CocroachDB. Here it’s just grouping/labeling your Neo4j Nodes with different data center names. Even though it has a lot more benefits, like load balancing to a particular group, replicating the data to read replica from the existing read replica instead of replicating from master and etc. Like my previous blog, this also just guides to setting up the Multi datacenter cluster in AWS and GCP.AWS/GCP:This blog just gives you simple steps to create a fresh Neo4j Multi data center cluster. From AWS/GCP you just need to whitelist the IP address in the security group(AWS) and Firewall rules(GCP). Otherwise, all the steps are common for both deployments.Setup Details:            Name      IP      Region      Role                  Node-1      10.128.0.98      us-central      CORE              Node-2      10.142.0.4      us-east-1      CORE              Node-3      10.128.0.102      us-central      REPLICA              Node-4      10.128.0.99      us-central      REPLICA              Node-5      10.142.0.23      us-east-1      REPLICA              Node-6      10.142.0.16      us-east-1      REPLICA      Install Neo4j on all the nodes:apt-get -y updateapt -y install openjdk-8-jrewget -O - https://debian.neo4j.org/neotechnology.gpg.key | sudo apt-key add -echo 'deb https://debian.neo4j.org/repo stable/' | sudo tee -a /etc/apt/sources.list.d/neo4j.listsudo apt-get -y updatesudo apt-get -y install neo4j-enterprise=1:3.5.14neo4j-admin set-initial-password 'root'Necessary Ports:  5000 - discovery_listen_address  6000 - transaction_advertised_address  7000 - raft_advertised_address  7473 - HTTPS interface to access the Neo4j cluster in browser  7474 - HTTP  interface to access the Neo4j cluster in browser  7687 - Used by Cypher Shell and by Neo4j Browser  6362 - Backup port to seed the data from the Leader node.Please allow the above ports between all the nodes.Configure Multi Datacenter Cluster:In our setup, we use 2 nodes as a minimum number of nodes to form a cluster, also we always need 2 runtime nodes to make the cluster up and running. Update the following values in the /etc/neo4j/neo4j.conf file. Before doing this just stop the neo4j service and delete the store data (/var/lib/neo4j/data/databases/graph.db/)Node 1dbms.security.auth_enabled=truedbms.connectors.default_listen_address=0.0.0.0dbms.connectors.default_advertised_address=10.128.0.98dbms.mode=COREcausal_clustering.minimum_core_cluster_size_at_formation=3causal_clustering.minimum_core_cluster_size_at_runtime=3causal_clustering.initial_discovery_members=10.128.0.98:5000,10.142.0.4:5000causal_clustering.initial_discovery_members=10.128.0.98:5000,10.142.0.4:5000causal_clustering.discovery_listen_address=0.0.0.0:5000causal_clustering.raft_advertised_address=10.128.0.98:7000causal_clustering.transaction_advertised_address=10.128.0.98:6000causal_clustering.server_groups=us-central-1Node 2dbms.security.auth_enabled=truedbms.connectors.default_listen_address=0.0.0.0dbms.connectors.default_advertised_address=10.142.0.4dbms.mode=COREcausal_clustering.minimum_core_cluster_size_at_formation=2causal_clustering.minimum_core_cluster_size_at_runtime=2causal_clustering.initial_discovery_members=10.128.0.98:5000,10.142.0.4:5000causal_clustering.initial_discovery_members=10.128.0.98:5000,10.142.0.4:5000causal_clustering.discovery_listen_address=0.0.0.0:5000causal_clustering.raft_advertised_address=10.142.0.4:7000causal_clustering.transaction_advertised_address=10.142.0.4:6000causal_clustering.server_groups=us-east-1Start the Neo4jNow we can start the neo4j service it’ll form a 2 node cluster.service neo4j startneo4j&gt; CALL dbms.cluster.overview();| id                                     | addresses                                                                          | role       | groups           | database  || \"e1ee4fd2-6698-470a-950c-a143bab5d904\" | \\[\"bolt://10.128.0.98:7687\", \"http://10.128.0.98:7474\", \"https://10.128.0.98:7473\"\\] | \"LEADER\"   | \\[\"us-central-1\"\\] | \"default\" || \"a19a9bc8-ad89-4451-bbd1-8d5f35190eda\" | \\[\"bolt://10.142.0.4:7687\", \"http://10.142.0.4:7474\", \"https://10.142.0.4:7473\"\\]    | \"FOLLOWER\" | \\[\"us-east-1\"\\]    | \"default\" |Adding the Replica (us-central)Edit the neo4j.conf file on the node 3 and 4.Node 3dbms.security.auth_enabled=truedbms.connectors.default_listen_address=0.0.0.0dbms.connectors.default_advertised_address=10.128.0.99dbms.mode=READ_REPLICAcausal_clustering.minimum_core_cluster_size_at_formation=3causal_clustering.minimum_core_cluster_size_at_runtime=3causal_clustering.initial_discovery_members=10.128.0.98:5000causal_clustering.discovery_listen_address=0.0.0.0:5000causal_clustering.raft_advertised_address=10.128.0.99:7000causal_clustering.transaction_advertised_address=10.128.0.99:6000causal_clustering.server_groups=us-central-1Node 4dbms.security.auth_enabled=truedbms.connectors.default_listen_address=0.0.0.0dbms.connectors.default_advertised_address=10.128.0.102dbms.mode=READ_REPLICAcausal_clustering.minimum_core_cluster_size_at_formation=3causal_clustering.minimum_core_cluster_size_at_runtime=3causal_clustering.initial_discovery_members=10.128.0.98:5000causal_clustering.discovery_listen_address=0.0.0.0:5000causal_clustering.raft_advertised_address=10.128.0.102:7000causal_clustering.transaction_advertised_address=10.128.0.102:6000causal_clustering.server_groups=us-central-1Adding the Replica (us-east-1)Edit the neo4j.conf file on the node 5 and 6.Node 5dbms.security.auth_enabled=truedbms.connectors.default_listen_address=0.0.0.0dbms.connectors.default_advertised_address=10.142.0.23dbms.mode=READ_REPLICAcausal_clustering.minimum_core_cluster_size_at_formation=2causal_clustering.minimum_core_cluster_size_at_runtime=2causal_clustering.initial_discovery_members=10.142.0.4:5000causal_clustering.discovery_listen_address=0.0.0.0:5000causal_clustering.raft_advertised_address=10.142.0.23:7000causal_clustering.transaction_advertised_address=10.142.0.23:6000causal_clustering.server_groups=us-east-1Node 6dbms.security.auth_enabled=truedbms.connectors.default_listen_address=0.0.0.0dbms.connectors.default_advertised_address=10.142.0.16dbms.mode=READ_REPLICAcausal_clustering.minimum_core_cluster_size_at_formation=2causal_clustering.minimum_core_cluster_size_at_runtime=2causal_clustering.initial_discovery_members=10.142.0.4:5000causal_clustering.discovery_listen_address=0.0.0.0:5000causal_clustering.raft_advertised_address=10.142.0.16:7000causal_clustering.transaction_advertised_address=10.142.0.16:6000causal_clustering.server_groups=us-east-1Start the Replica nodes:Now start the neo4j service on all the read replica nodes, and then check the cluster status.service neo4j startneo4j&gt; CALL dbms.cluster.overview();| id                                     | addresses                                                                             | role           | groups           | database  |\\+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| \"74124822-6963-49a5-af49-2c69132bfa0b\" | \\[\"bolt://10.128.0.102:7687\", \"http://10.128.0.102:7474\", \"https://10.128.0.102:7473\"\\] | \"READ_REPLICA\" | \\[\"us-central-1\"\\] | \"default\" || \"e1ee4fd2-6698-470a-950c-a143bab5d904\" | \\[\"bolt://10.128.0.98:7687\", \"http://10.128.0.98:7474\", \"https://10.128.0.98:7473\"\\]    | \"LEADER\"       | \\[\"us-central-1\"\\] | \"default\" || \"bb1c383e-39ff-46a8-9a03-3df2d5967029\" | \\[\"bolt://10.128.0.99:7687\", \"http://10.128.0.99:7474\", \"https://10.128.0.99:7473\"\\]    | \"READ_REPLICA\" | \\[\"us-central-1\"\\] | \"default\" || \"f7dae729-cf87-4c62-a68f-1fa067b9a3fa\" | \\[\"bolt://10.142.0.16:7687\", \"http://10.142.0.16:7474\", \"https://10.142.0.16:7473\"\\]    | \"READ_REPLICA\" | \\[\"us-east-1\"\\]    | \"default\" || \"3199a28f-f7af-49a4-adcb-918aab086eb8\" | \\[\"bolt://10.142.0.23:7687\", \"http://10.142.0.23:7474\", \"https://10.142.0.23:7473\"\\]    | \"READ_REPLICA\" | \\[\"us-east-1\"\\]    | \"default\" || \"a19a9bc8-ad89-4451-bbd1-8d5f35190eda\" | \\[\"bolt://10.142.0.4:7687\", \"http://10.142.0.4:7474\", \"https://10.142.0.4:7473\"\\]       | \"FOLLOWER\"     | \\[\"us-east-1\"\\]    | \"default\" |Our multi datacenter cluster is ready. It just gives you a simple configuration guide for multi data center design. But I didn’t cover the best practices here. May be I’ll write a new blog for that.References:  Multi Datacenter Design samples  More features for using Multi datacenter design  Load balancing with Multi datacenter",
            "content_html": "<p>Neo4j’s multi datacenter deployments are well suited for a geo-distributed workload and also provide a better disaster recovery solution. But to be frank, its not an actual distributed databases like Google Spanner or CocroachDB. Here it’s just grouping/labeling your Neo4j Nodes with different data center names. Even though it has a lot more benefits, like load balancing to a particular group, replicating the data to read replica from the existing read replica instead of replicating from master and etc. Like my <a href=\"https://thedataguy.in/setup-neo4j-causal-cluster-on-gcp-and-aws/\"><strong>previous blog</strong></a>, this also just guides to setting up the Multi datacenter cluster in AWS and GCP.</p><h2 id=\"awsgcp\">AWS/GCP:</h2><p>This blog just gives you simple steps to create a fresh Neo4j Multi data center cluster. From AWS/GCP you just need to whitelist the IP address in the security group(AWS) and Firewall rules(GCP). Otherwise, all the steps are common for both deployments.</p><h2 id=\"setup-details\">Setup Details:</h2><table>  <thead>    <tr>      <th>Name</th>      <th>IP</th>      <th>Region</th>      <th>Role</th>    </tr>  </thead>  <tbody>    <tr>      <td>Node-1</td>      <td>10.128.0.98</td>      <td>us-central</td>      <td>CORE</td>    </tr>    <tr>      <td>Node-2</td>      <td>10.142.0.4</td>      <td>us-east-1</td>      <td>CORE</td>    </tr>    <tr>      <td>Node-3</td>      <td>10.128.0.102</td>      <td>us-central</td>      <td>REPLICA</td>    </tr>    <tr>      <td>Node-4</td>      <td>10.128.0.99</td>      <td>us-central</td>      <td>REPLICA</td>    </tr>    <tr>      <td>Node-5</td>      <td>10.142.0.23</td>      <td>us-east-1</td>      <td>REPLICA</td>    </tr>    <tr>      <td>Node-6</td>      <td>10.142.0.16</td>      <td>us-east-1</td>      <td>REPLICA</td>    </tr>  </tbody></table><h2 id=\"install-neo4j-on-all-the-nodes\">Install Neo4j on all the nodes:</h2><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">apt-get <span class=\"nt\">-y</span> updateapt <span class=\"nt\">-y</span> <span class=\"nb\">install </span>openjdk-8-jrewget <span class=\"nt\">-O</span> - https://debian.neo4j.org/neotechnology.gpg.key | <span class=\"nb\">sudo </span>apt-key add -<span class=\"nb\">echo</span> <span class=\"s1\">'deb https://debian.neo4j.org/repo stable/'</span> | <span class=\"nb\">sudo tee</span> <span class=\"nt\">-a</span> /etc/apt/sources.list.d/neo4j.list<span class=\"nb\">sudo </span>apt-get <span class=\"nt\">-y</span> update<span class=\"nb\">sudo </span>apt-get <span class=\"nt\">-y</span> <span class=\"nb\">install </span>neo4j-enterprise<span class=\"o\">=</span>1:3.5.14neo4j-admin set-initial-password <span class=\"s1\">'root'</span></code></pre></figure><h2 id=\"necessary-ports\">Necessary Ports:</h2><ol>  <li>5000 - discovery_listen_address</li>  <li>6000 - transaction_advertised_address</li>  <li>7000 - raft_advertised_address</li>  <li>7473 - HTTPS interface to access the Neo4j cluster in browser</li>  <li>7474 - HTTP  interface to access the Neo4j cluster in browser</li>  <li>7687 - Used by Cypher Shell and by Neo4j Browser</li>  <li>6362 - Backup port to seed the data from the Leader node.</li></ol><p>Please allow the above ports between all the nodes.</p><h2 id=\"configure-multi-datacenter-cluster\">Configure Multi Datacenter Cluster:</h2><p>In our setup, we use 2 nodes as a minimum number of nodes to form a cluster, also we always need 2 runtime nodes to make the cluster up and running. Update the following values in the <code class=\"language-html highlighter-rouge\">/etc/neo4j/neo4j.conf</code> file. Before doing this just stop the neo4j service and delete the store data (<code class=\"language-html highlighter-rouge\">/var/lib/neo4j/data/databases/graph.db/</code>)</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">Node 1dbms.security.auth_enabled<span class=\"o\">=</span><span class=\"nb\">true</span>dbms.connectors.default_listen_address<span class=\"o\">=</span>0.0.0.0dbms.connectors.default_advertised_address<span class=\"o\">=</span>10.128.0.98dbms.mode<span class=\"o\">=</span>COREcausal_clustering.minimum_core_cluster_size_at_formation<span class=\"o\">=</span>3causal_clustering.minimum_core_cluster_size_at_runtime<span class=\"o\">=</span>3causal_clustering.initial_discovery_members<span class=\"o\">=</span>10.128.0.98:5000,10.142.0.4:5000causal_clustering.initial_discovery_members<span class=\"o\">=</span>10.128.0.98:5000,10.142.0.4:5000causal_clustering.discovery_listen_address<span class=\"o\">=</span>0.0.0.0:5000causal_clustering.raft_advertised_address<span class=\"o\">=</span>10.128.0.98:7000causal_clustering.transaction_advertised_address<span class=\"o\">=</span>10.128.0.98:6000causal_clustering.server_groups<span class=\"o\">=</span>us-central-1Node 2dbms.security.auth_enabled<span class=\"o\">=</span><span class=\"nb\">true</span>dbms.connectors.default_listen_address<span class=\"o\">=</span>0.0.0.0dbms.connectors.default_advertised_address<span class=\"o\">=</span>10.142.0.4dbms.mode<span class=\"o\">=</span>COREcausal_clustering.minimum_core_cluster_size_at_formation<span class=\"o\">=</span>2causal_clustering.minimum_core_cluster_size_at_runtime<span class=\"o\">=</span>2causal_clustering.initial_discovery_members<span class=\"o\">=</span>10.128.0.98:5000,10.142.0.4:5000causal_clustering.initial_discovery_members<span class=\"o\">=</span>10.128.0.98:5000,10.142.0.4:5000causal_clustering.discovery_listen_address<span class=\"o\">=</span>0.0.0.0:5000causal_clustering.raft_advertised_address<span class=\"o\">=</span>10.142.0.4:7000causal_clustering.transaction_advertised_address<span class=\"o\">=</span>10.142.0.4:6000causal_clustering.server_groups<span class=\"o\">=</span>us-east-1</code></pre></figure><h2 id=\"start-the-neo4j\">Start the Neo4j</h2><p>Now we can start the neo4j service it’ll form a 2 node cluster.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">service neo4j start</code></pre></figure><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"n\">neo4j</span><span class=\"o\">&gt;</span> <span class=\"k\">CALL</span> <span class=\"n\">dbms</span><span class=\"p\">.</span><span class=\"k\">cluster</span><span class=\"p\">.</span><span class=\"n\">overview</span><span class=\"p\">();</span><span class=\"o\">|</span> <span class=\"n\">id</span>                                     <span class=\"o\">|</span> <span class=\"n\">addresses</span>                                                                          <span class=\"o\">|</span> <span class=\"k\">role</span>       <span class=\"o\">|</span> <span class=\"n\">groups</span>           <span class=\"o\">|</span> <span class=\"k\">database</span>  <span class=\"o\">|</span><span class=\"o\">|</span> <span class=\"nv\">\"e1ee4fd2-6698-470a-950c-a143bab5d904\"</span> <span class=\"o\">|</span> <span class=\"err\">\\</span><span class=\"p\">[</span><span class=\"nv\">\"bolt://10.128.0.98:7687\"</span><span class=\"p\">,</span> <span class=\"nv\">\"http://10.128.0.98:7474\"</span><span class=\"p\">,</span> <span class=\"nv\">\"https://10.128.0.98:7473\"</span><span class=\"err\">\\</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"nv\">\"LEADER\"</span>   <span class=\"o\">|</span> <span class=\"err\">\\</span><span class=\"p\">[</span><span class=\"nv\">\"us-central-1\"</span><span class=\"err\">\\</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"nv\">\"default\"</span> <span class=\"o\">|</span><span class=\"o\">|</span> <span class=\"nv\">\"a19a9bc8-ad89-4451-bbd1-8d5f35190eda\"</span> <span class=\"o\">|</span> <span class=\"err\">\\</span><span class=\"p\">[</span><span class=\"nv\">\"bolt://10.142.0.4:7687\"</span><span class=\"p\">,</span> <span class=\"nv\">\"http://10.142.0.4:7474\"</span><span class=\"p\">,</span> <span class=\"nv\">\"https://10.142.0.4:7473\"</span><span class=\"err\">\\</span><span class=\"p\">]</span>    <span class=\"o\">|</span> <span class=\"nv\">\"FOLLOWER\"</span> <span class=\"o\">|</span> <span class=\"err\">\\</span><span class=\"p\">[</span><span class=\"nv\">\"us-east-1\"</span><span class=\"err\">\\</span><span class=\"p\">]</span>    <span class=\"o\">|</span> <span class=\"nv\">\"default\"</span> <span class=\"o\">|</span></code></pre></figure><h2 id=\"adding-the-replica-us-central\">Adding the Replica (us-central)</h2><p>Edit the <code class=\"language-html highlighter-rouge\">neo4j.conf</code> file on the node 3 and 4.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">Node 3dbms.security.auth_enabled<span class=\"o\">=</span><span class=\"nb\">true</span>dbms.connectors.default_listen_address<span class=\"o\">=</span>0.0.0.0dbms.connectors.default_advertised_address<span class=\"o\">=</span>10.128.0.99dbms.mode<span class=\"o\">=</span>READ_REPLICAcausal_clustering.minimum_core_cluster_size_at_formation<span class=\"o\">=</span>3causal_clustering.minimum_core_cluster_size_at_runtime<span class=\"o\">=</span>3causal_clustering.initial_discovery_members<span class=\"o\">=</span>10.128.0.98:5000causal_clustering.discovery_listen_address<span class=\"o\">=</span>0.0.0.0:5000causal_clustering.raft_advertised_address<span class=\"o\">=</span>10.128.0.99:7000causal_clustering.transaction_advertised_address<span class=\"o\">=</span>10.128.0.99:6000causal_clustering.server_groups<span class=\"o\">=</span>us-central-1Node 4dbms.security.auth_enabled<span class=\"o\">=</span><span class=\"nb\">true</span>dbms.connectors.default_listen_address<span class=\"o\">=</span>0.0.0.0dbms.connectors.default_advertised_address<span class=\"o\">=</span>10.128.0.102dbms.mode<span class=\"o\">=</span>READ_REPLICAcausal_clustering.minimum_core_cluster_size_at_formation<span class=\"o\">=</span>3causal_clustering.minimum_core_cluster_size_at_runtime<span class=\"o\">=</span>3causal_clustering.initial_discovery_members<span class=\"o\">=</span>10.128.0.98:5000causal_clustering.discovery_listen_address<span class=\"o\">=</span>0.0.0.0:5000causal_clustering.raft_advertised_address<span class=\"o\">=</span>10.128.0.102:7000causal_clustering.transaction_advertised_address<span class=\"o\">=</span>10.128.0.102:6000causal_clustering.server_groups<span class=\"o\">=</span>us-central-1</code></pre></figure><h2 id=\"adding-the-replica-us-east-1\">Adding the Replica (us-east-1)</h2><p>Edit the <code class=\"language-html highlighter-rouge\">neo4j.conf</code> file on the node 5 and 6.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">Node 5dbms.security.auth_enabled<span class=\"o\">=</span><span class=\"nb\">true</span>dbms.connectors.default_listen_address<span class=\"o\">=</span>0.0.0.0dbms.connectors.default_advertised_address<span class=\"o\">=</span>10.142.0.23dbms.mode<span class=\"o\">=</span>READ_REPLICAcausal_clustering.minimum_core_cluster_size_at_formation<span class=\"o\">=</span>2causal_clustering.minimum_core_cluster_size_at_runtime<span class=\"o\">=</span>2causal_clustering.initial_discovery_members<span class=\"o\">=</span>10.142.0.4:5000causal_clustering.discovery_listen_address<span class=\"o\">=</span>0.0.0.0:5000causal_clustering.raft_advertised_address<span class=\"o\">=</span>10.142.0.23:7000causal_clustering.transaction_advertised_address<span class=\"o\">=</span>10.142.0.23:6000causal_clustering.server_groups<span class=\"o\">=</span>us-east-1Node 6dbms.security.auth_enabled<span class=\"o\">=</span><span class=\"nb\">true</span>dbms.connectors.default_listen_address<span class=\"o\">=</span>0.0.0.0dbms.connectors.default_advertised_address<span class=\"o\">=</span>10.142.0.16dbms.mode<span class=\"o\">=</span>READ_REPLICAcausal_clustering.minimum_core_cluster_size_at_formation<span class=\"o\">=</span>2causal_clustering.minimum_core_cluster_size_at_runtime<span class=\"o\">=</span>2causal_clustering.initial_discovery_members<span class=\"o\">=</span>10.142.0.4:5000causal_clustering.discovery_listen_address<span class=\"o\">=</span>0.0.0.0:5000causal_clustering.raft_advertised_address<span class=\"o\">=</span>10.142.0.16:7000causal_clustering.transaction_advertised_address<span class=\"o\">=</span>10.142.0.16:6000causal_clustering.server_groups<span class=\"o\">=</span>us-east-1</code></pre></figure><h2 id=\"start-the-replica-nodes\">Start the Replica nodes:</h2><p>Now start the neo4j service on all the read replica nodes, and then check the cluster status.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">service neo4j start</code></pre></figure><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"n\">neo4j</span><span class=\"o\">&gt;</span> <span class=\"k\">CALL</span> <span class=\"n\">dbms</span><span class=\"p\">.</span><span class=\"k\">cluster</span><span class=\"p\">.</span><span class=\"n\">overview</span><span class=\"p\">();</span><span class=\"o\">|</span> <span class=\"n\">id</span>                                     <span class=\"o\">|</span> <span class=\"n\">addresses</span>                                                                             <span class=\"o\">|</span> <span class=\"k\">role</span>           <span class=\"o\">|</span> <span class=\"n\">groups</span>           <span class=\"o\">|</span> <span class=\"k\">database</span>  <span class=\"o\">|</span><span class=\"err\">\\</span><span class=\"o\">+</span><span class=\"c1\">--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><span class=\"o\">|</span> <span class=\"nv\">\"74124822-6963-49a5-af49-2c69132bfa0b\"</span> <span class=\"o\">|</span> <span class=\"err\">\\</span><span class=\"p\">[</span><span class=\"nv\">\"bolt://10.128.0.102:7687\"</span><span class=\"p\">,</span> <span class=\"nv\">\"http://10.128.0.102:7474\"</span><span class=\"p\">,</span> <span class=\"nv\">\"https://10.128.0.102:7473\"</span><span class=\"err\">\\</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"nv\">\"READ_REPLICA\"</span> <span class=\"o\">|</span> <span class=\"err\">\\</span><span class=\"p\">[</span><span class=\"nv\">\"us-central-1\"</span><span class=\"err\">\\</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"nv\">\"default\"</span> <span class=\"o\">|</span><span class=\"o\">|</span> <span class=\"nv\">\"e1ee4fd2-6698-470a-950c-a143bab5d904\"</span> <span class=\"o\">|</span> <span class=\"err\">\\</span><span class=\"p\">[</span><span class=\"nv\">\"bolt://10.128.0.98:7687\"</span><span class=\"p\">,</span> <span class=\"nv\">\"http://10.128.0.98:7474\"</span><span class=\"p\">,</span> <span class=\"nv\">\"https://10.128.0.98:7473\"</span><span class=\"err\">\\</span><span class=\"p\">]</span>    <span class=\"o\">|</span> <span class=\"nv\">\"LEADER\"</span>       <span class=\"o\">|</span> <span class=\"err\">\\</span><span class=\"p\">[</span><span class=\"nv\">\"us-central-1\"</span><span class=\"err\">\\</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"nv\">\"default\"</span> <span class=\"o\">|</span><span class=\"o\">|</span> <span class=\"nv\">\"bb1c383e-39ff-46a8-9a03-3df2d5967029\"</span> <span class=\"o\">|</span> <span class=\"err\">\\</span><span class=\"p\">[</span><span class=\"nv\">\"bolt://10.128.0.99:7687\"</span><span class=\"p\">,</span> <span class=\"nv\">\"http://10.128.0.99:7474\"</span><span class=\"p\">,</span> <span class=\"nv\">\"https://10.128.0.99:7473\"</span><span class=\"err\">\\</span><span class=\"p\">]</span>    <span class=\"o\">|</span> <span class=\"nv\">\"READ_REPLICA\"</span> <span class=\"o\">|</span> <span class=\"err\">\\</span><span class=\"p\">[</span><span class=\"nv\">\"us-central-1\"</span><span class=\"err\">\\</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"nv\">\"default\"</span> <span class=\"o\">|</span><span class=\"o\">|</span> <span class=\"nv\">\"f7dae729-cf87-4c62-a68f-1fa067b9a3fa\"</span> <span class=\"o\">|</span> <span class=\"err\">\\</span><span class=\"p\">[</span><span class=\"nv\">\"bolt://10.142.0.16:7687\"</span><span class=\"p\">,</span> <span class=\"nv\">\"http://10.142.0.16:7474\"</span><span class=\"p\">,</span> <span class=\"nv\">\"https://10.142.0.16:7473\"</span><span class=\"err\">\\</span><span class=\"p\">]</span>    <span class=\"o\">|</span> <span class=\"nv\">\"READ_REPLICA\"</span> <span class=\"o\">|</span> <span class=\"err\">\\</span><span class=\"p\">[</span><span class=\"nv\">\"us-east-1\"</span><span class=\"err\">\\</span><span class=\"p\">]</span>    <span class=\"o\">|</span> <span class=\"nv\">\"default\"</span> <span class=\"o\">|</span><span class=\"o\">|</span> <span class=\"nv\">\"3199a28f-f7af-49a4-adcb-918aab086eb8\"</span> <span class=\"o\">|</span> <span class=\"err\">\\</span><span class=\"p\">[</span><span class=\"nv\">\"bolt://10.142.0.23:7687\"</span><span class=\"p\">,</span> <span class=\"nv\">\"http://10.142.0.23:7474\"</span><span class=\"p\">,</span> <span class=\"nv\">\"https://10.142.0.23:7473\"</span><span class=\"err\">\\</span><span class=\"p\">]</span>    <span class=\"o\">|</span> <span class=\"nv\">\"READ_REPLICA\"</span> <span class=\"o\">|</span> <span class=\"err\">\\</span><span class=\"p\">[</span><span class=\"nv\">\"us-east-1\"</span><span class=\"err\">\\</span><span class=\"p\">]</span>    <span class=\"o\">|</span> <span class=\"nv\">\"default\"</span> <span class=\"o\">|</span><span class=\"o\">|</span> <span class=\"nv\">\"a19a9bc8-ad89-4451-bbd1-8d5f35190eda\"</span> <span class=\"o\">|</span> <span class=\"err\">\\</span><span class=\"p\">[</span><span class=\"nv\">\"bolt://10.142.0.4:7687\"</span><span class=\"p\">,</span> <span class=\"nv\">\"http://10.142.0.4:7474\"</span><span class=\"p\">,</span> <span class=\"nv\">\"https://10.142.0.4:7473\"</span><span class=\"err\">\\</span><span class=\"p\">]</span>       <span class=\"o\">|</span> <span class=\"nv\">\"FOLLOWER\"</span>     <span class=\"o\">|</span> <span class=\"err\">\\</span><span class=\"p\">[</span><span class=\"nv\">\"us-east-1\"</span><span class=\"err\">\\</span><span class=\"p\">]</span>    <span class=\"o\">|</span> <span class=\"nv\">\"default\"</span> <span class=\"o\">|</span></code></pre></figure><p>Our multi datacenter cluster is ready. It just gives you a simple configuration guide for multi data center design. But I didn’t cover the best practices here. May be I’ll write a new blog for that.</p><h2 id=\"references\">References:</h2><ol>  <li><a href=\"https://neo4j.com/docs/operations-manual/3.5/clustering-advanced/multi-data-center/design/#multi-dc-core-server-deployment-scenarios\">Multi Datacenter Design samples</a></li>  <li><a href=\"https://neo4j.com/docs/operations-manual/3.5/clustering-advanced/multi-data-center/configuration/\">More features for using Multi datacenter design</a></li>  <li><a href=\"https://neo4j.com/docs/operations-manual/3.5/clustering-advanced/multi-data-center/load-balancing/\">Load balancing with Multi datacenter</a></li></ol>",
            "url": "/2020/01/19/setup-multi-data-center-neo4j-cluster-in-aws-and-gcp",
            "image": "/assets/Setup Multi Data Center Neo4j Cluster In AWS and GCP.jpg",
            
            
            "tags": ["Neo4j","Graph-Database","Cluster","Replication"],
            
            "date_published": "2020-01-19T17:45:00+00:00",
            "date_modified": "2020-01-19T17:45:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2020/01/18/setup-neo4j-causal-cluster-on-gcp-and-aws",
            "title": "Setup Neo4j Causal Cluster On GCP And AWS",
            "summary": "Setup and configure the neo4j causal cluster on GCP and AWS. Also we covered how to solve the store copy ID mismatch.",
            "content_text": "Neo4j is one of the top-rated Graph database platforms which supports community based Graph database and Enterprise as well. If you want to make the Neo4j database would be highly available, then we have to go with Enterprise edition that has the feature called Causal Cluster. This in blog, we are going to see how to setup and configure Neo4j causal cluster on GCP and AWS cloud platforms. The Neo4j’s documentation has well explained about this cluster setup, but I ran into some issues while deploying this on my own. Those issues made me write this blog.Causal Cluster:This is nothing but a traditional replication mechanism. The whole cluster mechanism is behind the Raft algorithm. But it has some different terms.  Core - A master and slave kind of setup. If the leader fails other core node will become a new master.  Replica - Just a slave, but it’ll not participate in any election and it’ll never become the leader.AWS/GCP:This blog just gives you simple steps to create a fresh  Neo4j causal cluster. From AWS/GCP you just need to whitelist the IP address in the security group(AWS) and Firewall rules(GCP). Otherwise, all the steps are common for both deployments.Setup Details:Here we are going to setup a 3 node cluster. We need a minimum of 2 core CPU and 2GB memory for this.  Node 1 - 10.128.0.72  Node 2 - 10.128.0.80  Node 3 - 10.128.0.81Install Neo4j on all the nodes:apt-get -y updateapt -y install openjdk-8-jrewget -O - https://debian.neo4j.org/neotechnology.gpg.key | sudo apt-key add -echo 'deb https://debian.neo4j.org/repo stable/' | sudo tee -a /etc/apt/sources.list.d/neo4j.listsudo apt-get -y updatesudo apt-get -y install neo4j-enterprise=1:3.5.14neo4j-admin set-initial-password 'root'Necessary Ports:  5000 - discovery_listen_address  6000 - transaction_advertised_address  7000 - raft_advertised_address  7473 - HTTPS interface to access the Neo4j cluster in browser  7474 - HTTP  interface to access the Neo4j cluster in browser  7687 - Used by Cypher Shell and by Neo4j Browser  6362 - Backup port to seed the data from the Leader node.Please allow the above ports between all the nodes.Configure the causal cluster:In our setup, we use 3 nodes as a minimum number of nodes to form a cluster, also we always need 3 runtime nodes to make the cluster up and running. Update the following values in the /etc/neo4j/neo4j.conf file.Node 1dbms.security.auth_enabled=truedbms.connectors.default_listen_address=0.0.0.0dbms.connectors.default_advertised_address=10.128.0.72dbms.mode=COREcausal_clustering.minimum_core_cluster_size_at_formation=3causal_clustering.minimum_core_cluster_size_at_runtime=3causal_clustering.initial_discovery_members=10.128.0.72:5000,10.128.0.80:5000,10.128.0.81:5000causal_clustering.discovery_listen_address=0.0.0.0:5000causal_clustering.raft_advertised_address=10.128.0.72:7000causal_clustering.transaction_advertised_address=10.128.0.72:6000Node 2dbms.security.auth_enabled=truedbms.connectors.default_listen_address=0.0.0.0dbms.connectors.default_advertised_address=10.128.0.80dbms.mode=COREcausal_clustering.minimum_core_cluster_size_at_formation=3causal_clustering.minimum_core_cluster_size_at_runtime=3causal_clustering.initial_discovery_members=10.128.0.72:5000,10.128.0.80:5000,10.128.0.81:5000causal_clustering.discovery_listen_address=0.0.0.0:5000causal_clustering.raft_advertised_address=10.128.0.80:7000causal_clustering.transaction_advertised_address=10.128.0.80:6000Node 3dbms.security.auth_enabled=truedbms.connectors.default_listen_address=0.0.0.0dbms.connectors.default_advertised_address=10.128.0.81dbms.mode=COREcausal_clustering.minimum_core_cluster_size_at_formation=3causal_clustering.minimum_core_cluster_size_at_runtime=3causal_clustering.initial_discovery_members=10.128.0.72:5000,10.128.0.80:5000,10.128.0.81:5000causal_clustering.discovery_listen_address=0.0.0.0:5000causal_clustering.raft_advertised_address=10.128.0.81:7000causal_clustering.transaction_advertised_address=10.128.0.81:6000Cleanup the databases:Its a fresh cluster, so all the clusters should have the same data, here once we installed the neo4j software, then the neo4j service will automatically starts and it’ll create the default database calles graph.db on all the nodes. But according to the cluster, all the nodes should have same files and some metadata. Obiviously in our each node generated their own metadata in the graph database. Thats why we need to delete this system database from all the nodes.service neo4j stop  rm -rf /var/lib/neo4j/data/databases/graph.db/  &lt;or you can move it to another location&gt;mv /var/lib/neo4j/data/databases/graph.db/ /opt/  If you did’t do this step, or missed anyone of the node then you’ll get the error Store copy failed due to store ID mismatch Or in another case, if you already have some other databases with some data, then perform seed(its like a dump and restore from the existing node)Start the Neo4j cluster:Now start the neo4j service on all the nodes. Order doesn’t matter here. Until all three nodes are up, your cluster won’t be formed. So all three nodes should be up.service neo4j startOnce you stated it’ll wait for all three nodes are up and then perform an election to pick a Leader node. Once its up, we can query the cluster status via cypher-shellcypher-shell  -u neo4j -p rootneo4j&gt; CALL dbms.cluster.overview();Also you can access the database from the browser with HTTPS interface. If you have private VPN between the neo4j node and the computer where you are trying to access the HTTPS interface then you directly use the Private IP with port 7374 Or if you want to use its public IP, then you have to change the dbms.connectors.default_advertised_address IP to the Node’s Public IP.Conclusion:Again I’m confirming here is that this is just to give you a kickstart guide for neo4j cluster setup. You read more in-depth in the documentation page. I ran to some issues, but somehow I solved it. Here are some references to those errors.  Neo4j - casual cluster store copy failed  Not able to create a cluster",
            "content_html": "<p>Neo4j is one of the top-rated Graph database platforms which supports community based Graph database and Enterprise as well. If you want to make the Neo4j database would be highly available, then we have to go with Enterprise edition that has the feature called Causal Cluster. This in blog, we are going to see how to setup and configure Neo4j causal cluster on GCP and AWS cloud platforms. The Neo4j’s documentation has well explained about this cluster setup, but I ran into some issues while deploying this on my own. Those issues made me write this blog.</p><h2 id=\"causal-cluster\">Causal Cluster:</h2><p>This is nothing but a traditional replication mechanism. The whole cluster mechanism is behind the Raft algorithm. But it has some different terms.</p><ul>  <li>Core - A master and slave kind of setup. If the leader fails other core node will become a new master.</li>  <li>Replica - Just a slave, but it’ll not participate in any election and it’ll never become the leader.</li></ul><h2 id=\"awsgcp\">AWS/GCP:</h2><p>This blog just gives you simple steps to create a fresh  Neo4j causal cluster. From AWS/GCP you just need to whitelist the IP address in the security group(AWS) and Firewall rules(GCP). Otherwise, all the steps are common for both deployments.</p><h2 id=\"setup-details\">Setup Details:</h2><p>Here we are going to setup a 3 node cluster. We need a minimum of 2 core CPU and 2GB memory for this.</p><ul>  <li><strong>Node 1</strong> - 10.128.0.72</li>  <li><strong>Node 2</strong> - 10.128.0.80</li>  <li><strong>Node 3</strong> - 10.128.0.81</li></ul><h2 id=\"install-neo4j-on-all-the-nodes\">Install Neo4j on all the nodes:</h2><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">apt-get <span class=\"nt\">-y</span> updateapt <span class=\"nt\">-y</span> <span class=\"nb\">install </span>openjdk-8-jrewget <span class=\"nt\">-O</span> - https://debian.neo4j.org/neotechnology.gpg.key | <span class=\"nb\">sudo </span>apt-key add -<span class=\"nb\">echo</span> <span class=\"s1\">'deb https://debian.neo4j.org/repo stable/'</span> | <span class=\"nb\">sudo tee</span> <span class=\"nt\">-a</span> /etc/apt/sources.list.d/neo4j.list<span class=\"nb\">sudo </span>apt-get <span class=\"nt\">-y</span> update<span class=\"nb\">sudo </span>apt-get <span class=\"nt\">-y</span> <span class=\"nb\">install </span>neo4j-enterprise<span class=\"o\">=</span>1:3.5.14neo4j-admin set-initial-password <span class=\"s1\">'root'</span></code></pre></figure><h2 id=\"necessary-ports\">Necessary Ports:</h2><ol>  <li>5000 - discovery_listen_address</li>  <li>6000 - transaction_advertised_address</li>  <li>7000 - raft_advertised_address</li>  <li>7473 - HTTPS interface to access the Neo4j cluster in browser</li>  <li>7474 - HTTP  interface to access the Neo4j cluster in browser</li>  <li>7687 - Used by Cypher Shell and by Neo4j Browser</li>  <li>6362 - Backup port to seed the data from the Leader node.</li></ol><p>Please allow the above ports between all the nodes.</p><h2 id=\"configure-the-causal-cluster\">Configure the causal cluster:</h2><p>In our setup, we use 3 nodes as a minimum number of nodes to form a cluster, also we always need 3 runtime nodes to make the cluster up and running. Update the following values in the <code class=\"language-html highlighter-rouge\">/etc/neo4j/neo4j.conf</code> file.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">Node 1dbms.security.auth_enabled<span class=\"o\">=</span><span class=\"nb\">true</span>dbms.connectors.default_listen_address<span class=\"o\">=</span>0.0.0.0dbms.connectors.default_advertised_address<span class=\"o\">=</span>10.128.0.72dbms.mode<span class=\"o\">=</span>COREcausal_clustering.minimum_core_cluster_size_at_formation<span class=\"o\">=</span>3causal_clustering.minimum_core_cluster_size_at_runtime<span class=\"o\">=</span>3causal_clustering.initial_discovery_members<span class=\"o\">=</span>10.128.0.72:5000,10.128.0.80:5000,10.128.0.81:5000causal_clustering.discovery_listen_address<span class=\"o\">=</span>0.0.0.0:5000causal_clustering.raft_advertised_address<span class=\"o\">=</span>10.128.0.72:7000causal_clustering.transaction_advertised_address<span class=\"o\">=</span>10.128.0.72:6000Node 2dbms.security.auth_enabled<span class=\"o\">=</span><span class=\"nb\">true</span>dbms.connectors.default_listen_address<span class=\"o\">=</span>0.0.0.0dbms.connectors.default_advertised_address<span class=\"o\">=</span>10.128.0.80dbms.mode<span class=\"o\">=</span>COREcausal_clustering.minimum_core_cluster_size_at_formation<span class=\"o\">=</span>3causal_clustering.minimum_core_cluster_size_at_runtime<span class=\"o\">=</span>3causal_clustering.initial_discovery_members<span class=\"o\">=</span>10.128.0.72:5000,10.128.0.80:5000,10.128.0.81:5000causal_clustering.discovery_listen_address<span class=\"o\">=</span>0.0.0.0:5000causal_clustering.raft_advertised_address<span class=\"o\">=</span>10.128.0.80:7000causal_clustering.transaction_advertised_address<span class=\"o\">=</span>10.128.0.80:6000Node 3dbms.security.auth_enabled<span class=\"o\">=</span><span class=\"nb\">true</span>dbms.connectors.default_listen_address<span class=\"o\">=</span>0.0.0.0dbms.connectors.default_advertised_address<span class=\"o\">=</span>10.128.0.81dbms.mode<span class=\"o\">=</span>COREcausal_clustering.minimum_core_cluster_size_at_formation<span class=\"o\">=</span>3causal_clustering.minimum_core_cluster_size_at_runtime<span class=\"o\">=</span>3causal_clustering.initial_discovery_members<span class=\"o\">=</span>10.128.0.72:5000,10.128.0.80:5000,10.128.0.81:5000causal_clustering.discovery_listen_address<span class=\"o\">=</span>0.0.0.0:5000causal_clustering.raft_advertised_address<span class=\"o\">=</span>10.128.0.81:7000causal_clustering.transaction_advertised_address<span class=\"o\">=</span>10.128.0.81:6000</code></pre></figure><h2 id=\"cleanup-the-databases\">Cleanup the databases:</h2><p>Its a fresh cluster, so all the clusters should have the same data, here once we installed the neo4j software, then the neo4j service will automatically starts and it’ll create the default database calles <code class=\"language-html highlighter-rouge\">graph.db</code> on all the nodes. But according to the cluster, all the nodes should have same files and some metadata. Obiviously in our each node generated their own metadata in the graph database. Thats why we need to delete this system database from all the nodes.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">service neo4j stop  <span class=\"nb\">rm</span> <span class=\"nt\">-rf</span> /var/lib/neo4j/data/databases/graph.db/  &lt;or you can move it to another location&gt;<span class=\"nb\">mv</span> /var/lib/neo4j/data/databases/graph.db/ /opt/  </code></pre></figure><p>If you did’t do this step, or missed anyone of the node then you’ll get the error <code class=\"language-html highlighter-rouge\">Store copy failed due to store ID mismatch</code> Or in another case, if you already have some other databases with some data, then perform seed(its like a dump and restore from the existing node)</p><h2 id=\"start-the-neo4j-cluster\">Start the Neo4j cluster:</h2><p>Now start the neo4j service on all the nodes. Order doesn’t matter here. Until all three nodes are up, your cluster won’t be formed. So all three nodes should be up.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">service neo4j start</code></pre></figure><p>Once you stated it’ll wait for all three nodes are up and then perform an election to pick a Leader node. Once its up, we can query the cluster status via <code class=\"language-html highlighter-rouge\">cypher-shell</code></p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">cypher-shell  <span class=\"nt\">-u</span> neo4j <span class=\"nt\">-p</span> rootneo4j&gt; CALL dbms.cluster.overview<span class=\"o\">()</span><span class=\"p\">;</span></code></pre></figure><p><img src=\"/assets/Setup Neo4j Causal Cluster On GCP And AWS1.jpg\" alt=\"\" /></p><p>Also you can access the database from the browser with HTTPS interface. If you have private VPN between the neo4j node and the computer where you are trying to access the HTTPS interface then you directly use the <code class=\"language-html highlighter-rouge\">Private IP</code> with port <code class=\"language-html highlighter-rouge\">7374</code> Or if you want to use its public IP, then you have to change the <code class=\"language-html highlighter-rouge\">dbms.connectors.default_advertised_address</code> IP to the Node’s Public IP.</p><p><img src=\"/assets/Setup Neo4j Causal Cluster On GCP And AWS2.jpg\" alt=\"\" /></p><p><img src=\"/assets/Setup Neo4j Causal Cluster On GCP And AWS3.jpg\" alt=\"\" /></p><h2 id=\"conclusion\">Conclusion:</h2><p>Again I’m confirming here is that this is just to give you a kickstart guide for neo4j cluster setup. You read more in-depth in the <a href=\"https://neo4j.com/docs/operations-manual/current/clustering/\">documentation page</a>. I ran to some issues, but somehow I solved it. Here are some references to those errors.</p><ol>  <li><a href=\"https://stackoverflow.com/questions/59768748/neo4j-casual-cluster-store-copy-failed\">Neo4j - casual cluster store copy failed</a></li>  <li><a href=\"https://www.reddit.com/r/Neo4j/comments/epoubc/not_able_to_create_a_cluster/\">Not able to create a cluster</a></li></ol>",
            "url": "/2020/01/18/setup-neo4j-causal-cluster-on-gcp-and-aws",
            "image": "/assets/Setup Neo4j Causal Cluster On GCP And AWS.png",
            
            
            "tags": ["Neo4j","Cluster","Graph-Database"],
            
            "date_published": "2020-01-18T19:45:00+00:00",
            "date_modified": "2020-01-18T19:45:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2020/01/02/debezium-mysql-snapshot-for-aws-rds-aurora-from-backup-snaphot",
            "title": "Debezium MySQL Snapshot For AWS RDS Aurora From Backup Snaphot",
            "summary": "Debezium MySQL connector load historical data of AWS RDS Aurora from its snapshot. Using crash recovery, we can get the binlog information.",
            "content_text": "I have published enough Debezium MySQL connector tutorials for taking snapshots from Read Replica. To continue my research I wanted to do something for AWS RDS Aurora as well. But aurora is not using binlog bases replication. So we can’t use the list of tutorials that I published already. In Aurora, we can get the binlog file name and its position from its snapshot of the source Cluster. So I used a snapshot for loading the historical data, and once it’s loaded we can resume the CDC from the main cluster.Requirements:  Running aurora cluster.  Aurora cluster must have binlogs enabled.  Make binlog retention period to a minimum 3 days(its a best practice).  Debezium connector should be able to access both the clusters.  Make sure you have different security groups for the main RDS Aurora cluster and the Snapshot cluster.Sample data in source aurora:create database bhuvi;use bhuvi;create table rohi (id int,fn varchar(10),ln varchar(10),phone int);insert into rohi values (1, 'rohit', 'last',87611);insert into rohi values (2, 'rohit', 'last',87611);insert into rohi values (3, 'rohit', 'last',87611);insert into rohi values (4, 'rohit', 'last',87611);insert into rohi values (5, 'rohit', 'last',87611);Take Aurora snapshot:Go to the RDS console and select your source Aurora master node. Take a snapshot of it. Once the snapshot has been done, you see that in the snapshots tab.New cluster from snapshot:Then create a new cluster from the snapshot. Once its launched, we can get the binlog info from the logs.In RDS Console, select the instance name. Click on the Logs &amp; Events tab. Below the Recent events, you can see the binlog information of the source Aurora node while talking the snapshot. This cluster also needs to enable with binlog.Register the MySQL Connector:Follow this link to configure Kafka cluster and connector. Create a file called mysql.json and add the Snapshot cluster’s information.{\"name\": \"mysql-connector-db01\",\"config\": {\"name\": \"mysql-connector-db01\",\"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\",\"database.server.id\": \"1\",\"tasks.max\": \"1\",\"database.history.kafka.bootstrap.servers\": \"YOUR-BOOTSTRAP-SERVER:9092\",\"database.history.kafka.topic\": \"schema-changes.mysql\",\"database.server.name\": \"mysql-db01\",\"database.hostname\": \"SNAPSHOT-INSTANCE-ENDPOINT\",\"database.port\": \"3306\",\"database.user\": \"bhuvi\",\"database.password\": \"****\",\"database.whitelist\": \"bhuvi\",\"snapshot.mode\": \"initial\",\"snapshot.locking.mode\": \"none\",\"key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\"key.converter.schemas.enable\": \"false\",\"value.converter.schemas.enable\": \"false\",\"internal.key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\"internal.value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\"internal.key.converter.schemas.enable\": \"false\",\"internal.value.converter.schemas.enable\": \"false\",\"transforms\": \"unwrap\",\"transforms.unwrap.add.source.fields\": \"ts_ms\",\"tombstones.on.delete\": \"false\",\"transforms.unwrap.type\": \"io.debezium.transforms.ExtractNewRecordState\"}}Run the below command to register it on the connector node.curl -X POST -H \"Accept: application/json\" -H \"Content-Type: application/json\" http://localhost:8083/connectors -d @mysql.jsonOnce the snapshot has been done, you can see the snapshot cluster’s current binlog file name and its position in the connect-offsets topic.kafka-console-consumer --bootstrap-server localhost:9092 --topic connect-offsets --from-beginning{\"file\":\"mysql-bin-changelog.000006\",\"pos\":154}Add more data on the source Cluster:To simulate the real production setup, add few more rows to the rohi table.insert into rohi values (6, 'rohit', 'last',87611);insert into rohi values (7, 'rohit', 'last',87611);Also, create a new table.use bhuvi;create table testtbl (id int);insert into testtbl values (1);Because, once we switch to the source cluster, it should read these new data.Update the Source Aurora binlog info:Stop the connector service and manually inject the binlog information that we got from the Snapshot cluster’s Log &amp; Events section.connector-node# systemctl stop confluent-connect-distributedGet the last read binlog information and its parition from the connect-offsets topic.kafkacat -b localhost:9092 -C -t connect-offsets  -f 'Partition(%p) %k %s\\n'Partition(0) [\"mysql-connector-db01\",{\"server\":\"mysql-db01\"}] {\"file\":\"mysql-bin-changelog.000006\",\"pos\":154}  kafkacat - command-line utility from confluent.  -b localhost:9092  - broker details  -C - Consumer  -t connect-offsets - topic  Partition(0) - The partition name where we have the binlog info.  mysql-connector-db01 - connector name  \"server\":\"mysql-db01 - server name we used in mysql.json fileRun the following command to inject the binlog info to the connect-offsets topic.echo '[\"mysql-connector-db01\",{\"server\":\"mysql-db01\"}]|{\"file\":\"mysql-bin-changelog.000002\",\"pos\":2170}' | \\kafkacat -P -b localhost:9092 -t connect-offsets -K\\ | -p 0  mysql-connector-db01 - connector name  \"server\":\"mysql-db01 - server name we used in mysql.json file  {\"file\":\"mysql-bin-changelog.000002\",\"pos\":2170} - Binlog info from the snapshot cluster’s log.  kafkacat - command-line utility from confluent.  -P - Producer  -b localhost:9092  - broker details  -t connect-offsets - topic  -p 0 Partition where we have the binlog info.Now if you read the data from the consumer, it’ll show the new binlog.kafka-console-consumer --bootstrap-server localhost:9092 --topic connect-offsets --from-beginning{\"file\":\"mysql-bin-changelog.000006\",\"pos\":154}{\"file\":\"mysql-bin-changelog.000002\",\"pos\":2170}Switch to Source Cluster:Before doing the switchover, we need to make that the connector should not access to the snapshot cluster once the connector service started. We can achieve this in 2 ways.  Anyhow, we read all the from the snapshot cluster, so delete it.  In the Snapshot cluster’s security group, remove the connector’s node IP.I recommend using the 2nd option. Now start the connector service. After a few seconds, you can see the logs like below.\\[2020-01-02 06:57:21,448\\] INFO Starting MySqlConnectorTask with configuration: (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,450\\] INFO    connector.class = io.debezium.connector.mysql.MySqlConnector (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,450\\] INFO    snapshot.locking.mode = none (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,451\\] INFO    tasks.max = 1 (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,451\\] INFO    database.history.kafka.topic = replica-schema-changes.mysql (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,452\\] INFO    transforms = unwrap (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,452\\] INFO    internal.key.converter.schemas.enable = false (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,452\\] INFO    transforms.unwrap.add.source.fields = ts_ms (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,453\\] INFO    tombstones.on.delete = false (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,453\\] INFO    transforms.unwrap.type = io.debezium.transforms.ExtractNewRecordState (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,453\\] INFO    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,453\\] INFO    database.whitelist = bhuvi (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,453\\] INFO    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,453\\] INFO    database.user = admin (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,453\\] INFO    database.server.id = 1 (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,453\\] INFO    database.history.kafka.bootstrap.servers = 172.31.40.132:9092 (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,453\\] INFO    database.server.name = mysql-db01 (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,453\\] INFO    database.port = 3306 (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,454\\] INFO    key.converter.schemas.enable = false (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,454\\] INFO    internal.key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,454\\] INFO    task.class = io.debezium.connector.mysql.MySqlConnectorTask (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,454\\] INFO    database.hostname = snapshot-cluster.cluster-chbcar19iy5o.us-east-1.rds.amazonaws.com (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,454\\] INFO    database.password = ******** (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,454\\] INFO    internal.value.converter.schemas.enable = false (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,454\\] INFO    name = mysql-connector-db01 (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,454\\] INFO    value.converter.schemas.enable = false (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,454\\] INFO    internal.value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,454\\] INFO    snapshot.mode = initial (io.debezium.connector.common.BaseSourceTask)\\[2020-01-02 06:57:21,512\\] INFO \\[Producer clientId=connector-producer-mysql-connector-db01-0\\] Cluster ID: H-jsdNk9SUuud35n3AIk8g (org.apache.kafka.clients.Metadata)Update the Endpoint:Create an updated config file which has the endpoint of Source Aurora endpoint and the snapshot mode = schema only recovery .And the main important thing is use a different topic for schema changes history. Else you’ll end up with some error like below.ERROR Failed due to error: Error processing binlog event (io.debezium.connector.mysql.BinlogReader)org.apache.kafka.connect.errors.ConnectException: Encountered change event for table bhuvi.rohi whose schema isn't known to this connectorFile: mysql-update.json{\"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\",\"snapshot.locking.mode\": \"none\",\"tasks.max\": \"3\",\"database.history.kafka.topic\": \"schema-changes.mysql\",\"transforms\": \"unwrap\",\"internal.key.converter.schemas.enable\": \"false\",\"transforms.unwrap.add.source.fields\": \"ts_ms\",\"tombstones.on.delete\": \"false\",\"transforms.unwrap.type\": \"io.debezium.transforms.ExtractNewRecordState\",\"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\"database.whitelist\": \"bhuvi\",\"key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\"database.user\": \"admin\",\"database.server.id\": \"1\",\"database.history.kafka.bootstrap.servers\": \"BROKER-NODE-IP:9092\",\"database.server.name\": \"mysql-db01\",\"database.port\": \"3306\",\"key.converter.schemas.enable\": \"false\",\"internal.key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\"database.hostname\": \"SOURCE-AURORA-ENDPOINT\",\"database.password\": \"*****\",\"internal.value.converter.schemas.enable\": \"false\",\"name\": \"mysql-connector-db01\",\"value.converter.schemas.enable\": \"false\",\"internal.value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\"snapshot.mode\": \"SCHEMA_ONLY_RECOVERY\"}Run the below command to update the  MySQL connector.curl -X PUT -H \"Accept: application/json\" -H \"Content-Type: application/json\" http://localhost:8083/connectors/mysql-connector-db01/config -d @mysql-update.jsonThen immediately it’ll start reading from the Source Aurora cluster from the binlog position mysql-bin-changelog.000002 2170You can see these changes from the connect-offsets topic.kafka-console-consumer --bootstrap-server localhost:9092 --topic connect-offsets --from-beginning{\"file\":\"mysql-bin-changelog.000006\",\"pos\":154}{\"file\":\"mysql-bin-changelog.000002\",\"pos\":2170}{\"ts_sec\":1577948351,\"file\":\"mysql-bin-changelog.000003\",\"pos\":1207,\"row\":1,\"server_id\":2115919109,\"event\":2}And we add 2 more rows to the rohi table. You can see those new values from  the bhuvi.rohi topic.kafka-console-consumer --bootstrap-server localhost:9092 --topic mysql-db01.bhuvi.rohi --from-beginning{\"id\":1,\"fn\":\"rohit\",\"ln\":\"last\",\"phone\":87611,\"__ts_ms\":0}{\"id\":2,\"fn\":\"rohit\",\"ln\":\"last\",\"phone\":87611,\"__ts_ms\":0}{\"id\":3,\"fn\":\"rohit\",\"ln\":\"last\",\"phone\":87611,\"__ts_ms\":0}{\"id\":4,\"fn\":\"rohit\",\"ln\":\"last\",\"phone\":87611,\"__ts_ms\":0}{\"id\":5,\"fn\":\"rohit\",\"ln\":\"last\",\"phone\":87611,\"__ts_ms\":0}{\"id\":6,\"fn\":\"rohit\",\"ln\":\"last\",\"phone\":87611,\"__ts_ms\":1577948298000}{\"id\":7,\"fn\":\"rohit\",\"ln\":\"last\",\"phone\":87611,\"__ts_ms\":1577948304000}Also, you can the new table testtbl added to the topic.kafka-topics --zookeeper localhost:2181 --listconnect-configsconnect-offsetsconnect-statusdefault_ksql_processing_logmysql-db01mysql-db01.bhuvi.rohimysql-db01.bhuvi.testtblreplica-schema-changes.mysqlschema-changes.mysqlDebezium Series blogs:  Build Production Grade Debezium Cluster With Confluent Kafka  Monitor Debezium MySQL Connector With Prometheus And Grafana  Debezium MySQL Snapshot From Read Replica With GTID  Debezium MySQL Snapshot From Read Replica And Resume From Master  Debezium MySQL Snapshot For AWS RDS Aurora From Backup Snaphot  RealTime CDC From MySQL Using AWS MSK With Debezium",
            "content_html": "<p>I have published enough Debezium MySQL connector tutorials for taking snapshots from Read Replica. To continue my research I wanted to do something for AWS RDS Aurora as well. But aurora is not using binlog bases replication. So we can’t use the list of tutorials that I published already. In Aurora, we can get the binlog file name and its position from its snapshot of the source Cluster. So I used a snapshot for loading the historical data, and once it’s loaded we can resume the CDC from the main cluster.</p><h2 id=\"requirements\">Requirements:</h2><ol>  <li>Running aurora cluster.</li>  <li>Aurora cluster must have <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/enable-binary-logging-aurora/\">binlogs enabled</a>.</li>  <li>Make binlog retention period to a minimum 3 days(its a best practice).</li>  <li>Debezium connector should be able to access both the clusters.</li>  <li>Make sure you have different security groups for the main RDS Aurora cluster and the Snapshot cluster.</li></ol><h2 id=\"sample-data-in-source-aurora\">Sample data in source aurora:</h2><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">create</span> <span class=\"k\">database</span> <span class=\"n\">bhuvi</span><span class=\"p\">;</span><span class=\"n\">use</span> <span class=\"n\">bhuvi</span><span class=\"p\">;</span><span class=\"k\">create</span> <span class=\"k\">table</span> <span class=\"n\">rohi</span> <span class=\"p\">(</span><span class=\"n\">id</span> <span class=\"nb\">int</span><span class=\"p\">,</span><span class=\"n\">fn</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">),</span><span class=\"n\">ln</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">),</span><span class=\"n\">phone</span> <span class=\"nb\">int</span><span class=\"p\">);</span><span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">rohi</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">'rohit'</span><span class=\"p\">,</span> <span class=\"s1\">'last'</span><span class=\"p\">,</span><span class=\"mi\">87611</span><span class=\"p\">);</span><span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">rohi</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"s1\">'rohit'</span><span class=\"p\">,</span> <span class=\"s1\">'last'</span><span class=\"p\">,</span><span class=\"mi\">87611</span><span class=\"p\">);</span><span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">rohi</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"s1\">'rohit'</span><span class=\"p\">,</span> <span class=\"s1\">'last'</span><span class=\"p\">,</span><span class=\"mi\">87611</span><span class=\"p\">);</span><span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">rohi</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s1\">'rohit'</span><span class=\"p\">,</span> <span class=\"s1\">'last'</span><span class=\"p\">,</span><span class=\"mi\">87611</span><span class=\"p\">);</span><span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">rohi</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"s1\">'rohit'</span><span class=\"p\">,</span> <span class=\"s1\">'last'</span><span class=\"p\">,</span><span class=\"mi\">87611</span><span class=\"p\">);</span></code></pre></figure><h2 id=\"take-aurora-snapshot\">Take Aurora snapshot:</h2><p>Go to the RDS console and select your source Aurora master node. Take a snapshot of it. Once the snapshot has been done, you see that in the snapshots tab.</p><p><img src=\"/assets/Debezium MySQL Snapshot For AWS RDS Aurora From Backup Snaphot1.jpg\" alt=\"\" /></p><h2 id=\"new-cluster-from-snapshot\">New cluster from snapshot:</h2><p>Then create a new cluster from the snapshot. Once its launched, we can get the binlog info from the logs.</p><p>In RDS Console, select the instance name. Click on the Logs &amp; Events tab. Below the Recent events, you can see the binlog information of the source Aurora node while talking the snapshot. This cluster also needs to enable with binlog.</p><p><img src=\"/assets/Debezium MySQL Snapshot For AWS RDS Aurora From Backup Snaphot2.jpg\" alt=\"\" /></p><h2 id=\"register-the-mysql-connector\">Register the MySQL Connector:</h2><p>Follow this link to <a href=\"https://thedataguy.in/build-production-grade-debezium-with-confluent-kafka-cluster/\">configure Kafka cluster and connector.</a> Create a file called <code class=\"language-html highlighter-rouge\">mysql.json</code> and add the Snapshot cluster’s information.</p><figure class=\"highlight\"><pre><code class=\"language-json\" data-lang=\"json\"><span class=\"p\">{</span><span class=\"w\"></span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"mysql-connector-db01\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"config\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"mysql-connector-db01\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"connector.class\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"io.debezium.connector.mysql.MySqlConnector\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.server.id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"1\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"tasks.max\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"1\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.history.kafka.bootstrap.servers\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"YOUR-BOOTSTRAP-SERVER:9092\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.history.kafka.topic\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"schema-changes.mysql\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.server.name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"mysql-db01\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.hostname\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"SNAPSHOT-INSTANCE-ENDPOINT\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.port\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"3306\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.user\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"bhuvi\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.password\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"****\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.whitelist\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"bhuvi\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"snapshot.mode\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"initial\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"snapshot.locking.mode\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"none\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"key.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"value.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"key.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"value.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"internal.key.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"internal.value.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"internal.key.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"internal.value.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"transforms\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"unwrap\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"transforms.unwrap.add.source.fields\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"ts_ms\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"tombstones.on.delete\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"transforms.unwrap.type\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"io.debezium.transforms.ExtractNewRecordState\"</span><span class=\"w\"></span><span class=\"p\">}</span><span class=\"w\"></span><span class=\"p\">}</span></code></pre></figure><p>Run the below command to register it on the connector node.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">curl <span class=\"nt\">-X</span> POST <span class=\"nt\">-H</span> <span class=\"s2\">\"Accept: application/json\"</span> <span class=\"nt\">-H</span> <span class=\"s2\">\"Content-Type: application/json\"</span> http://localhost:8083/connectors <span class=\"nt\">-d</span> @mysql.json</code></pre></figure><p>Once the snapshot has been done, you can see the snapshot cluster’s current binlog file name and its position in the <code class=\"language-html highlighter-rouge\">connect-offsets</code> topic.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">kafka-console-consumer <span class=\"nt\">--bootstrap-server</span> localhost:9092 <span class=\"nt\">--topic</span> connect-offsets <span class=\"nt\">--from-beginning</span><span class=\"o\">{</span><span class=\"s2\">\"file\"</span>:<span class=\"s2\">\"mysql-bin-changelog.000006\"</span>,<span class=\"s2\">\"pos\"</span>:154<span class=\"o\">}</span></code></pre></figure><h2 id=\"add-more-data-on-the-source-cluster\">Add more data on the source Cluster:</h2><p>To simulate the real production setup, add few more rows to the <code class=\"language-html highlighter-rouge\">rohi</code> table.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">rohi</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"s1\">'rohit'</span><span class=\"p\">,</span> <span class=\"s1\">'last'</span><span class=\"p\">,</span><span class=\"mi\">87611</span><span class=\"p\">);</span><span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">rohi</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"s1\">'rohit'</span><span class=\"p\">,</span> <span class=\"s1\">'last'</span><span class=\"p\">,</span><span class=\"mi\">87611</span><span class=\"p\">);</span></code></pre></figure><p>Also, create a new table.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"n\">use</span> <span class=\"n\">bhuvi</span><span class=\"p\">;</span><span class=\"k\">create</span> <span class=\"k\">table</span> <span class=\"n\">testtbl</span> <span class=\"p\">(</span><span class=\"n\">id</span> <span class=\"nb\">int</span><span class=\"p\">);</span><span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">testtbl</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">);</span></code></pre></figure><p>Because, once we switch to the source cluster, it should read these new data.</p><h2 id=\"update-the-source-aurora-binlog-info\">Update the Source Aurora binlog info:</h2><p>Stop the connector service and manually inject the binlog information that we got from the Snapshot cluster’s Log &amp; Events section.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">connector-node# systemctl stop confluent-connect-distributed</code></pre></figure><p>Get the last read binlog information and its parition from the <code class=\"language-html highlighter-rouge\">connect-offsets</code> topic.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">kafkacat <span class=\"nt\">-b</span> localhost:9092 <span class=\"nt\">-C</span> <span class=\"nt\">-t</span> connect-offsets  <span class=\"nt\">-f</span> <span class=\"s1\">'Partition(%p) %k %s\\n'</span>Partition<span class=\"o\">(</span>0<span class=\"o\">)</span> <span class=\"o\">[</span><span class=\"s2\">\"mysql-connector-db01\"</span>,<span class=\"o\">{</span><span class=\"s2\">\"server\"</span>:<span class=\"s2\">\"mysql-db01\"</span><span class=\"o\">}]</span> <span class=\"o\">{</span><span class=\"s2\">\"file\"</span>:<span class=\"s2\">\"mysql-bin-changelog.000006\"</span>,<span class=\"s2\">\"pos\"</span>:154<span class=\"o\">}</span></code></pre></figure><ul>  <li><code class=\"language-html highlighter-rouge\">kafkacat</code> - command-line utility from confluent.</li>  <li><code class=\"language-html highlighter-rouge\">-b localhost:9092</code>  - broker details</li>  <li><code class=\"language-html highlighter-rouge\">-C</code> - Consumer</li>  <li><code class=\"language-html highlighter-rouge\">-t connect-offsets</code> - topic</li>  <li><code class=\"language-html highlighter-rouge\">Partition(0)</code> - The partition name where we have the binlog info.</li>  <li><code class=\"language-html highlighter-rouge\">mysql-connector-db01</code> - connector name</li>  <li><code class=\"language-html highlighter-rouge\">\"server\":\"mysql-db01</code> - server name we used in <code class=\"language-html highlighter-rouge\">mysql.json</code> file</li></ul><p>Run the following command to inject the binlog info to the <code class=\"language-html highlighter-rouge\">connect-offsets</code> topic.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nb\">echo</span> <span class=\"s1\">'[\"mysql-connector-db01\",{\"server\":\"mysql-db01\"}]|{\"file\":\"mysql-bin-changelog.000002\",\"pos\":2170}'</span> | <span class=\"se\">\\</span>kafkacat <span class=\"nt\">-P</span> <span class=\"nt\">-b</span> localhost:9092 <span class=\"nt\">-t</span> connect-offsets <span class=\"nt\">-K</span><span class=\"se\">\\ </span>| <span class=\"nt\">-p</span> 0</code></pre></figure><ul>  <li><code class=\"language-html highlighter-rouge\">mysql-connector-db01</code> - connector name</li>  <li><code class=\"language-html highlighter-rouge\">\"server\":\"mysql-db01</code> - server name we used in <code class=\"language-html highlighter-rouge\">mysql.json</code> file</li>  <li><code class=\"language-html highlighter-rouge\">{\"file\":\"mysql-bin-changelog.000002\",\"pos\":2170}</code> - Binlog info from the snapshot cluster’s log.</li>  <li><code class=\"language-html highlighter-rouge\">kafkacat</code> - command-line utility from confluent.</li>  <li><code class=\"language-html highlighter-rouge\">-P</code> - Producer</li>  <li><code class=\"language-html highlighter-rouge\">-b localhost:9092</code>  - broker details</li>  <li><code class=\"language-html highlighter-rouge\">-t connect-offsets</code> - topic</li>  <li><code class=\"language-html highlighter-rouge\">-p 0</code> Partition where we have the binlog info.</li></ul><p>Now if you read the data from the consumer, it’ll show the new binlog.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">kafka-console-consumer <span class=\"nt\">--bootstrap-server</span> localhost:9092 <span class=\"nt\">--topic</span> connect-offsets <span class=\"nt\">--from-beginning</span><span class=\"o\">{</span><span class=\"s2\">\"file\"</span>:<span class=\"s2\">\"mysql-bin-changelog.000006\"</span>,<span class=\"s2\">\"pos\"</span>:154<span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"file\"</span>:<span class=\"s2\">\"mysql-bin-changelog.000002\"</span>,<span class=\"s2\">\"pos\"</span>:2170<span class=\"o\">}</span></code></pre></figure><h2 id=\"switch-to-source-cluster\">Switch to Source Cluster:</h2><p>Before doing the switchover, we need to make that the connector should not access to the snapshot cluster once the connector service started. We can achieve this in 2 ways.</p><ol>  <li>Anyhow, we read all the from the snapshot cluster, so delete it.</li>  <li>In the Snapshot cluster’s security group, remove the connector’s node IP.</li></ol><p>I recommend using the 2nd option. Now start the connector service. After a few seconds, you can see the logs like below.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"se\">\\[</span>2020-01-02 06:57:21,448<span class=\"se\">\\]</span> INFO Starting MySqlConnectorTask with configuration: <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,450<span class=\"se\">\\]</span> INFO    connector.class <span class=\"o\">=</span> io.debezium.connector.mysql.MySqlConnector <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,450<span class=\"se\">\\]</span> INFO    snapshot.locking.mode <span class=\"o\">=</span> none <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,451<span class=\"se\">\\]</span> INFO    tasks.max <span class=\"o\">=</span> 1 <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,451<span class=\"se\">\\]</span> INFO    database.history.kafka.topic <span class=\"o\">=</span> replica-schema-changes.mysql <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,452<span class=\"se\">\\]</span> INFO    transforms <span class=\"o\">=</span> unwrap <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,452<span class=\"se\">\\]</span> INFO    internal.key.converter.schemas.enable <span class=\"o\">=</span> <span class=\"nb\">false</span> <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,452<span class=\"se\">\\]</span> INFO    transforms.unwrap.add.source.fields <span class=\"o\">=</span> ts_ms <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,453<span class=\"se\">\\]</span> INFO    tombstones.on.delete <span class=\"o\">=</span> <span class=\"nb\">false</span> <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,453<span class=\"se\">\\]</span> INFO    transforms.unwrap.type <span class=\"o\">=</span> io.debezium.transforms.ExtractNewRecordState <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,453<span class=\"se\">\\]</span> INFO    value.converter <span class=\"o\">=</span> org.apache.kafka.connect.json.JsonConverter <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,453<span class=\"se\">\\]</span> INFO    database.whitelist <span class=\"o\">=</span> bhuvi <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,453<span class=\"se\">\\]</span> INFO    key.converter <span class=\"o\">=</span> org.apache.kafka.connect.json.JsonConverter <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,453<span class=\"se\">\\]</span> INFO    database.user <span class=\"o\">=</span> admin <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,453<span class=\"se\">\\]</span> INFO    database.server.id <span class=\"o\">=</span> 1 <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,453<span class=\"se\">\\]</span> INFO    database.history.kafka.bootstrap.servers <span class=\"o\">=</span> 172.31.40.132:9092 <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,453<span class=\"se\">\\]</span> INFO    database.server.name <span class=\"o\">=</span> mysql-db01 <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,453<span class=\"se\">\\]</span> INFO    database.port <span class=\"o\">=</span> 3306 <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,454<span class=\"se\">\\]</span> INFO    key.converter.schemas.enable <span class=\"o\">=</span> <span class=\"nb\">false</span> <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,454<span class=\"se\">\\]</span> INFO    internal.key.converter <span class=\"o\">=</span> org.apache.kafka.connect.json.JsonConverter <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,454<span class=\"se\">\\]</span> INFO    task.class <span class=\"o\">=</span> io.debezium.connector.mysql.MySqlConnectorTask <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,454<span class=\"se\">\\]</span> INFO    database.hostname <span class=\"o\">=</span> snapshot-cluster.cluster-chbcar19iy5o.us-east-1.rds.amazonaws.com <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,454<span class=\"se\">\\]</span> INFO    database.password <span class=\"o\">=</span> <span class=\"k\">********</span> <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,454<span class=\"se\">\\]</span> INFO    internal.value.converter.schemas.enable <span class=\"o\">=</span> <span class=\"nb\">false</span> <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,454<span class=\"se\">\\]</span> INFO    name <span class=\"o\">=</span> mysql-connector-db01 <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,454<span class=\"se\">\\]</span> INFO    value.converter.schemas.enable <span class=\"o\">=</span> <span class=\"nb\">false</span> <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,454<span class=\"se\">\\]</span> INFO    internal.value.converter <span class=\"o\">=</span> org.apache.kafka.connect.json.JsonConverter <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,454<span class=\"se\">\\]</span> INFO    snapshot.mode <span class=\"o\">=</span> initial <span class=\"o\">(</span>io.debezium.connector.common.BaseSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2020-01-02 06:57:21,512<span class=\"se\">\\]</span> INFO <span class=\"se\">\\[</span>Producer <span class=\"nv\">clientId</span><span class=\"o\">=</span>connector-producer-mysql-connector-db01-0<span class=\"se\">\\]</span> Cluster ID: H-jsdNk9SUuud35n3AIk8g <span class=\"o\">(</span>org.apache.kafka.clients.Metadata<span class=\"o\">)</span></code></pre></figure><h3 id=\"update-the-endpoint\">Update the Endpoint:</h3><p>Create an updated config file which has the endpoint of Source Aurora endpoint and the <code class=\"language-html highlighter-rouge\">snapshot mode = schema only recovery</code> .</p><p>And the main important thing is use a different topic for schema changes history. Else you’ll end up with some error like below.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">ERROR Failed due to error: Error processing binlog event <span class=\"o\">(</span>io.debezium.connector.mysql.BinlogReader<span class=\"o\">)</span>org.apache.kafka.connect.errors.ConnectException: Encountered change event <span class=\"k\">for </span>table bhuvi.rohi whose schema isn<span class=\"s1\">'t known to this connector</span></code></pre></figure><p>File: mysql-update.json</p><figure class=\"highlight\"><pre><code class=\"language-json\" data-lang=\"json\"><span class=\"p\">{</span><span class=\"w\"></span><span class=\"nl\">\"connector.class\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"io.debezium.connector.mysql.MySqlConnector\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"snapshot.locking.mode\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"none\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"tasks.max\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"3\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.history.kafka.topic\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"schema-changes.mysql\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"transforms\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"unwrap\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"internal.key.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"transforms.unwrap.add.source.fields\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"ts_ms\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"tombstones.on.delete\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"transforms.unwrap.type\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"io.debezium.transforms.ExtractNewRecordState\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"value.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.whitelist\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"bhuvi\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"key.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.user\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"admin\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.server.id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"1\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.history.kafka.bootstrap.servers\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"BROKER-NODE-IP:9092\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.server.name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"mysql-db01\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.port\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"3306\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"key.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"internal.key.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.hostname\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"SOURCE-AURORA-ENDPOINT\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.password\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"*****\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"internal.value.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"mysql-connector-db01\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"value.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"internal.value.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"snapshot.mode\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"SCHEMA_ONLY_RECOVERY\"</span><span class=\"w\"></span><span class=\"p\">}</span></code></pre></figure><p>Run the below command to update the  MySQL connector.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">curl <span class=\"nt\">-X</span> PUT <span class=\"nt\">-H</span> <span class=\"s2\">\"Accept: application/json\"</span> <span class=\"nt\">-H</span> <span class=\"s2\">\"Content-Type: application/json\"</span> http://localhost:8083/connectors/mysql-connector-db01/config <span class=\"nt\">-d</span> @mysql-update.json</code></pre></figure><p>Then immediately it’ll start reading from the Source Aurora cluster from the binlog position <code class=\"language-html highlighter-rouge\">mysql-bin-changelog.000002 2170</code></p><p>You can see these changes from the <code class=\"language-html highlighter-rouge\">connect-offsets</code> topic.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">kafka-console-consumer <span class=\"nt\">--bootstrap-server</span> localhost:9092 <span class=\"nt\">--topic</span> connect-offsets <span class=\"nt\">--from-beginning</span><span class=\"o\">{</span><span class=\"s2\">\"file\"</span>:<span class=\"s2\">\"mysql-bin-changelog.000006\"</span>,<span class=\"s2\">\"pos\"</span>:154<span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"file\"</span>:<span class=\"s2\">\"mysql-bin-changelog.000002\"</span>,<span class=\"s2\">\"pos\"</span>:2170<span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"ts_sec\"</span>:1577948351,<span class=\"s2\">\"file\"</span>:<span class=\"s2\">\"mysql-bin-changelog.000003\"</span>,<span class=\"s2\">\"pos\"</span>:1207,<span class=\"s2\">\"row\"</span>:1,<span class=\"s2\">\"server_id\"</span>:2115919109,<span class=\"s2\">\"event\"</span>:2<span class=\"o\">}</span></code></pre></figure><p>And we add 2 more rows to the rohi table. You can see those new values from  the <code class=\"language-html highlighter-rouge\">bhuvi.rohi</code> topic.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">kafka-console-consumer <span class=\"nt\">--bootstrap-server</span> localhost:9092 <span class=\"nt\">--topic</span> mysql-db01.bhuvi.rohi <span class=\"nt\">--from-beginning</span><span class=\"o\">{</span><span class=\"s2\">\"id\"</span>:1,<span class=\"s2\">\"fn\"</span>:<span class=\"s2\">\"rohit\"</span>,<span class=\"s2\">\"ln\"</span>:<span class=\"s2\">\"last\"</span>,<span class=\"s2\">\"phone\"</span>:87611,<span class=\"s2\">\"__ts_ms\"</span>:0<span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"id\"</span>:2,<span class=\"s2\">\"fn\"</span>:<span class=\"s2\">\"rohit\"</span>,<span class=\"s2\">\"ln\"</span>:<span class=\"s2\">\"last\"</span>,<span class=\"s2\">\"phone\"</span>:87611,<span class=\"s2\">\"__ts_ms\"</span>:0<span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"id\"</span>:3,<span class=\"s2\">\"fn\"</span>:<span class=\"s2\">\"rohit\"</span>,<span class=\"s2\">\"ln\"</span>:<span class=\"s2\">\"last\"</span>,<span class=\"s2\">\"phone\"</span>:87611,<span class=\"s2\">\"__ts_ms\"</span>:0<span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"id\"</span>:4,<span class=\"s2\">\"fn\"</span>:<span class=\"s2\">\"rohit\"</span>,<span class=\"s2\">\"ln\"</span>:<span class=\"s2\">\"last\"</span>,<span class=\"s2\">\"phone\"</span>:87611,<span class=\"s2\">\"__ts_ms\"</span>:0<span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"id\"</span>:5,<span class=\"s2\">\"fn\"</span>:<span class=\"s2\">\"rohit\"</span>,<span class=\"s2\">\"ln\"</span>:<span class=\"s2\">\"last\"</span>,<span class=\"s2\">\"phone\"</span>:87611,<span class=\"s2\">\"__ts_ms\"</span>:0<span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"id\"</span>:6,<span class=\"s2\">\"fn\"</span>:<span class=\"s2\">\"rohit\"</span>,<span class=\"s2\">\"ln\"</span>:<span class=\"s2\">\"last\"</span>,<span class=\"s2\">\"phone\"</span>:87611,<span class=\"s2\">\"__ts_ms\"</span>:1577948298000<span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"id\"</span>:7,<span class=\"s2\">\"fn\"</span>:<span class=\"s2\">\"rohit\"</span>,<span class=\"s2\">\"ln\"</span>:<span class=\"s2\">\"last\"</span>,<span class=\"s2\">\"phone\"</span>:87611,<span class=\"s2\">\"__ts_ms\"</span>:1577948304000<span class=\"o\">}</span></code></pre></figure><p>Also, you can the new table <code class=\"language-html highlighter-rouge\">testtbl</code> added to the topic.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">kafka-topics <span class=\"nt\">--zookeeper</span> localhost:2181 <span class=\"nt\">--list</span>connect-configsconnect-offsetsconnect-statusdefault_ksql_processing_logmysql-db01mysql-db01.bhuvi.rohimysql-db01.bhuvi.testtblreplica-schema-changes.mysqlschema-changes.mysql</code></pre></figure><h3 id=\"debezium-series-blogs\">Debezium Series blogs:</h3><ol>  <li><a href=\"https://thedataguy.in/build-production-grade-debezium-with-confluent-kafka-cluster/\">Build Production Grade Debezium Cluster With Confluent Kafka</a></li>  <li><a href=\"https://thedataguy.in/monitor-debezium-mysql-connector-with-prometheus-and-grafana/\">Monitor Debezium MySQL Connector With Prometheus And Grafana</a></li>  <li><a href=\"https://thedataguy.in/debezium-mysql-snapshot-from-read-replica-with-gtid/\">Debezium MySQL Snapshot From Read Replica With GTID</a></li>  <li><a href=\"https://thedataguy.in/debezium-mysql-snapshot-from-read-replica-and-resume-from-master/\">Debezium MySQL Snapshot From Read Replica And Resume From Master</a></li>  <li><a href=\"https://thedataguy.in/debezium-mysql-snapshot-for-aws-rds-aurora-from-backup-snaphot/\">Debezium MySQL Snapshot For AWS RDS Aurora From Backup Snaphot</a></li>  <li><a href=\"https://medium.com/searce/realtime-cdc-from-mysql-using-aws-msk-with-debezium-28da5a4ca873\">RealTime CDC From MySQL Using AWS MSK With Debezium</a></li></ol>",
            "url": "/2020/01/02/debezium-mysql-snapshot-for-aws-rds-aurora-from-backup-snaphot",
            "image": "/assets/Debezium MySQL Snapshot For AWS RDS Aurora From Backup Snaphot.jpg",
            
            
            "tags": ["aws","rds","aurora","kafka","debezium"],
            
            "date_published": "2020-01-02T08:43:00+00:00",
            "date_modified": "2020-01-02T08:43:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2019/12/31/debezium-mysql-snapshot-from-read-replica-and-resume-from-master",
            "title": "Debezium MySQL Snapshot From Read Replica And Resume From Master",
            "summary": "Debezium MySQL connector to take snapshot from Read replica and then point it to the Master node without GTID. We can resume the CDC with binlog information from slave status.",
            "content_text": "In my previous post, I have shown you how to take the snapshot from Read Replica with GTID for Debezium  MySQL connector. GTID concept is awesome, but still many of us using the replication without GTID. For these cases, we can take a snapshot from Read replica and then manually push the Master binlog information to the offsets topic. Injecting manual entry for offsets topic is already documented in Debezium. I’m just guiding you the way to take snapshot from Read replica without GTID.Requirements:  Setup master slave replication.  The slave must have log-slave-updates=ON else connector will fail to read from beginning onwards.  Debezium connector should be able to access the Read replica with a user that is having necessary permissions.  Install Debezium connector.Use a different name for Slave binlog:  Note: If you are already having a Master slave setup then ignore this step.By default, MySQL use mysql-bin as a prefix for all the mysql binlog files. We should not have the same binlog name for both the master and the slave. If you are setting up a new master-slave replication then make this change in my.cnf file.master#log_bin = /var/log/mysql/mysql-bin.logslave#log_bin = /var/log/mysql/mysql-slave-bin.logSample data:Create a new database to test this sync and insert some values.create database bhuvi;use bhuvi;create table rohi (id int,fn varchar(10),ln varchar(10),phone int);insert into rohi values (1, 'rohit', 'last',87611);insert into rohi values (2, 'rohit', 'last',87611);insert into rohi values (3, 'rohit', 'last',87611);insert into rohi values (4, 'rohit', 'last',87611);insert into rohi values (5, 'rohit', 'last',87611);Create the MySQL Connector Config:File Name: mysql.json{\"name\": \"mysql-connector-db01\",\"config\": {\"name\": \"mysql-connector-db01\",\"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\",\"database.server.id\": \"1\",\"tasks.max\": \"1\",\"database.history.kafka.bootstrap.servers\": \"YOUR-BOOTSTRAP-SERVER:9092\",\"database.history.kafka.topic\": \"schema-changes.mysql\",\"database.server.name\": \"mysql-db01\",\"database.hostname\": \"IP-OF-READER-NODE\",\"database.port\": \"3306\",\"database.user\": \"bhuvi\",\"database.password\": \"****\",\"database.whitelist\": \"bhuvi\",\"snapshot.mode\": \"initial\",\"snapshot.locking.mode\": \"none\",\"key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\"key.converter.schemas.enable\": \"false\",\"value.converter.schemas.enable\": \"false\",\"internal.key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\"internal.value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\"internal.key.converter.schemas.enable\": \"false\",\"internal.value.converter.schemas.enable\": \"false\",\"transforms\": \"unwrap\",\"transforms.unwrap.add.source.fields\": \"ts_ms\",\"tombstones.on.delete\": \"false\",\"transforms.unwrap.type\": \"io.debezium.transforms.ExtractNewRecordState}}Run the below command to register the mysql connector.curl -X POST -H \"Accept: application/json\" -H \"Content-Type: application/json\" http://localhost:8083/connectors -d @mysql.jsonOnce the snapshot has been done, then it’ll push the binlog information of the Slave while taking the snapshot. And then it’ll start to continue to do CDC for the upcoming data. You will see the first record in your connect-offsets topic as like below.{\"file\":\"ip-172-31-25-99-bin.000002\",\"pos\":7240}Then for continuous replication, it’ll start adding the record to this topic along with some more addition metadata like, server id, timestamp.{\"ts_sec\":1577764293,\"file\":\"ip-172-31-25-99-bin.000002\",\"pos\":7305,\"row\":1,\"server_id\":1,\"event\":2}You can monitor the snapshot progress from JMX.curl localhost:7071 | grep debezium_metrics_SecondsBehindMasterdebezium_metrics_SecondsBehindMaster{context=\"binlog\",name=\"mysql-db01\",plugin=\"mysql\",} 299.577536699E9Sometimes the metrics take a few more minutes to update. So once you are able to see the last binlog information from the connet-offsets and from JMX the lag &lt;10, then the snapshot is done.Switch to Master:Before switching to the master, we need to stop the slave instance to get the consistent binlog information of Master from the Read replica. And then stop the Debezium connector to update binlog information manually in the connect-offsets topic.mysql-slave&gt; stop slave;Debezium-connector-node# systemctl stop confluent-connect-distributedTo simulate the real-time scenario, we can add 1 new row in our MySQL table. So this will never replicate to your slave. But once you switch the node, it should start reading from this row.mysql-master&gt; insert into rohi values (6, 'rohit', 'last','87611');Also create a new table and insert one new row to this new table.mysql-master&gt; create table testtbl (id int);mysql-master&gt; insert into testtbl values (1);Once the switchover has been done, then it should read the 6'th row that we inserted and a new topic should be created for the testtblGet the last binlog info from offsets:Install kafkacat in you broker node. (it’s available from confluent repo)apt-get install kafkacatRun the below command get the last read binlog info.kafkacat -b localhost:9092 -C -t connect-offsets  -f 'Partition(%p) %k %s\\n'  -b - Broker  -C consumer  -t Topic  -f lag takes arguments specifying both the format of the output and the fields to include.You will get something like this.Partition(0) [\"mysql-connector-db01\",{\"server\":\"mysql-db01\"}] {\"file\":\"ip-172-31-25-99-bin.000002\",\"pos\":7240}Partition(0) [\"mysql-connector-db01\",{\"server\":\"mysql-db01\"}] {\"ts_sec\":1577764293,\"file\":\"ip-172-31-25-99-bin.000002\",\"pos\":7305,\"row\":1,\"server_id\":1,\"event\":2}  Partition(0) - The Partition where the information is location.  mysql-connector-db01  Connector Name  \"server\":\"mysql-db01\" Server name that the connect has.  \"ts-sec\":1577764293,\"file\":\"ip-172-31-25-99-bin.000002\",\"pos\":7305,\"row\":1,\"server_id\":1,\"event\":2 - Binlog informationNow we’ll manually push a new record inside this topic with the same information but just replace the binlog file name and its position. We need to continue the CDC where it stopped, so the get the exact starting binlog information we’ll use slave status from the Read replica.mysql-slave&gt; show slave status\\G                   Slave_IO_State:                      Master_Host: 172.31.36.115                      Master_User: bhuvi                      Master_Port: 3306                    Connect_Retry: 60                  Master_Log_File: mysql-bin.000003              Read_Master_Log_Pos: 7759                   Relay_Log_File: ip-172-31-25-99-relay-bin.000009                    Relay_Log_Pos: 7646              Exec_Master_Log_Pos: 7759Make a note of Master-log-file and Exec-Master-Log-Pos from the slave status. Now inject a new record to the offets topic.echo '[\"mysql-connector-db01\",{\"server\":\"mysql-db01\"}]|{\"file\":\"mysql-bin.000003\",\"pos\":7759}' |\\kafkacat -P -b localhost:9092 -t connect-offsets -K\\ | -p 0  -b Broker  -P Producer  -K Delimiter  -p PartitionIf you read the data from this topic, you’ll see the manually injected record.kafka-console-consumer --bootstrap-server localhost:9092 --topic connect-offsets --from-beginning{\"file\":\"ip-172-31-25-99-bin.000002\",\"pos\":7240}{\"ts_sec\":1577764293,\"file\":\"ip-172-31-25-99-bin.000002\",\"pos\":7305,\"row\":1,\"server_id\":1,\"event\":2}{\"file\":\"mysql-bin.000003\",\"pos\":7759}Once you start the Debezium MySQL connector, then it’ll start reading from the slave but it’ll start looking for the binlog file mysql-bin.000003 If you use the same binlog file name for both master and slave, then it’ll be a problem. So we can do any one of the following method to solve this.  Use different naming conversion for both Master and Slave binlog files.  Delete all the binlog files from the Slave using Reset master command.  If the binlog file in slave is having a file named as mysql-bin.000003 then delete this file alone.  If the binlog file in slave is having a file names as mysql-bin.000003 then rename this file as mysql-bin.000003.old  Disclaimer: Please consider with your DBA before performing any of the above steps. I recommend using step 1 or 4.Start the debezium connector:Debezium-connector-node#  systemctl start confluent-connect-distributedvYou in your connector log file, you can see there is an error indicating that the Debezium is not able to find the binlog file called mysql-bin.000003.\\[2019-12-31 03:55:17,128\\] INFO WorkerSourceTask{id=mysql-connector-db01-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask)\\[2019-12-31 03:55:17,131\\] ERROR WorkerSourceTask{id=mysql-connector-db01-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask)org.apache.kafka.connect.errors.ConnectException: The connector is trying to read binlog starting at binlog file 'mysql-bin.000003', pos=7759, skipping 2 events plus 1 rows, but this is no longer available on the server. Reconfigure the connector to use a snapshot when needed.at io.debezium.connector.mysql.MySqlConnectorTask.start(MySqlConnectorTask.java:132)at io.debezium.connector.common.BaseSourceTask.start(BaseSourceTask.java:49)at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:208)at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:177)at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:227)at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)at java.base/java.lang.Thread.run(Thread.java:834)\\[2019-12-31 03:55:17,132\\] ERROR WorkerSourceTask{id=mysql-connector-db01-0} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask)\\[2019-12-31 03:55:17,132\\] INFO Stopping MySQL connector task (io.debezium.connector.mysql.MySqlConnectorTask)Now we need to update the existing MySQL connector’s config and just change the \"database.hostname\" parameter.  Note: this JSON file format is different from the one which we used to register the connector. So make sure the syntax.File Name: mysql-update.json{\"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\",\"snapshot.locking.mode\": \"none\",\"tasks.max\": \"3\",\"database.history.kafka.topic\": \"schema-changes.mysql\",\"transforms\": \"unwrap\",\"internal.key.converter.schemas.enable\": \"false\",\"transforms.unwrap.add.source.fields\": \"ts_ms\",\"tombstones.on.delete\": \"false\",\"transforms.unwrap.type\": \"io.debezium.transforms.ExtractNewRecordState\",\"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\"database.whitelist\": \"bhuvi\",\"key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\"database.user\": \"bhuvi\",\"database.server.id\": \"1\",\"database.history.kafka.bootstrap.servers\": \"YOUR-KAFKA-BOOTSTRAP-SERVER:9092\",\"database.server.name\": \"mysql-db01\",\"database.port\": \"3306\",\"key.converter.schemas.enable\": \"false\",\"internal.key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\"database.hostname\": \"MASTER-IP-ADDRESS\",\"database.password\": \"****\",\"internal.value.converter.schemas.enable\": \"false\",\"name\": \"mysql-connector-db01\",\"value.converter.schemas.enable\": \"false\",\"internal.value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\"snapshot.mode\": \"initial\"}Run the below command to update the config file.curl -X PUT -H \"Accept: application/json\" -H \"Content-Type: application/json\" http://localhost:8083/connectors/mysql-connector-db01/config -d @mysql-update.jsonOnce the update is done, immediately it’ll start connecting to the master and start reading the binlog file mysql-bin.000003 from position 7759.We inserted a new record to the rohi table. If you read this topic then you can see the row has been read. Also start inserting few more rows to this table with id 7 and 8.kafka-console-consumer --bootstrap-server localhost:9092 --topic mysql-db01.bhuvi.rohi --from-beginning{\"id\":6,\"fn\":\"rohit\",\"ln\":\"last\",\"phone\":87611,\"__ts_ms\":1577788740000}{\"id\":7,\"fn\":\"rohit\",\"ln\":\"last\",\"phone\":87611,\"__ts_ms\":1577788764000}{\"id\":8,\"fn\":\"rohit\",\"ln\":\"last\",\"phone\":87611,\"__ts_ms\":1577788767000}Also, it should added the testtbl to the kafka topic.kafka-topics --zookeeper localhost:2181 --listconnect-configsconnect-offsetsconnect-statusdefault_ksql_processing_logmy_connect_offsetsmysql-db01mysql-db01.bhuvi.rohimysql-db01.bhuvi.testtblschema-changes.mysqlOnce your switchover is done, resume the replication on your slave.Debezium Series blogs:  Build Production Grade Debezium Cluster With Confluent Kafka  Monitor Debezium MySQL Connector With Prometheus And Grafana  Debezium MySQL Snapshot From Read Replica With GTID  Debezium MySQL Snapshot From Read Replica And Resume From Master  Debezium MySQL Snapshot For AWS RDS Aurora From Backup Snaphot  RealTime CDC From MySQL Using AWS MSK With Debezium",
            "content_html": "<p>In my <a href=\"https://thedataguy.in/debezium-mysql-snapshot-from-read-replica-with-gtid/\">previous post</a>, I have shown you how to take the snapshot from Read Replica with GTID for Debezium  MySQL connector. GTID concept is awesome, but still many of us using the replication without GTID. For these cases, we can take a snapshot from Read replica and then manually push the Master binlog information to the offsets topic. Injecting manual entry for offsets topic is <a href=\"https://debezium.io/documentation/faq/#how_to_change_the_offsets_of_the_source_database\">already documented in Debezium</a>. I’m just guiding you the way to take snapshot from Read replica without GTID.</p><h2 id=\"requirements\">Requirements:</h2><ul>  <li>Setup master slave replication.</li>  <li>The slave must have <code class=\"language-html highlighter-rouge\">log-slave-updates=ON</code> else connector will fail to read from beginning onwards.</li>  <li>Debezium connector should be able to access the Read replica with a user that is having necessary permissions.</li>  <li>Install Debezium connector.</li></ul><h2 id=\"use-a-different-name-for-slave-binlog\">Use a different name for Slave binlog:</h2><blockquote>  <p><strong>Note</strong>: If you are already having a Master slave setup then ignore this step.</p></blockquote><p>By default, MySQL use <code class=\"language-html highlighter-rouge\">mysql-bin</code> as a prefix for all the mysql binlog files. We should not have the same binlog name for both the master and the slave. If you are setting up a new master-slave replication then make this change in <code class=\"language-html highlighter-rouge\">my.cnf</code> file.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">master#log_bin <span class=\"o\">=</span> /var/log/mysql/mysql-bin.logslave#log_bin <span class=\"o\">=</span> /var/log/mysql/mysql-slave-bin.log</code></pre></figure><h2 id=\"sample-data\">Sample data:</h2><p>Create a new database to test this sync and insert some values.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">create database bhuvi<span class=\"p\">;</span>use bhuvi<span class=\"p\">;</span>create table rohi <span class=\"o\">(</span><span class=\"nb\">id </span>int,fn varchar<span class=\"o\">(</span>10<span class=\"o\">)</span>,<span class=\"nb\">ln </span>varchar<span class=\"o\">(</span>10<span class=\"o\">)</span>,phone int<span class=\"o\">)</span><span class=\"p\">;</span>insert into rohi values <span class=\"o\">(</span>1, <span class=\"s1\">'rohit'</span>, <span class=\"s1\">'last'</span>,87611<span class=\"o\">)</span><span class=\"p\">;</span>insert into rohi values <span class=\"o\">(</span>2, <span class=\"s1\">'rohit'</span>, <span class=\"s1\">'last'</span>,87611<span class=\"o\">)</span><span class=\"p\">;</span>insert into rohi values <span class=\"o\">(</span>3, <span class=\"s1\">'rohit'</span>, <span class=\"s1\">'last'</span>,87611<span class=\"o\">)</span><span class=\"p\">;</span>insert into rohi values <span class=\"o\">(</span>4, <span class=\"s1\">'rohit'</span>, <span class=\"s1\">'last'</span>,87611<span class=\"o\">)</span><span class=\"p\">;</span>insert into rohi values <span class=\"o\">(</span>5, <span class=\"s1\">'rohit'</span>, <span class=\"s1\">'last'</span>,87611<span class=\"o\">)</span><span class=\"p\">;</span></code></pre></figure><h3 id=\"create-the-mysql-connector-config\">Create the MySQL Connector Config:</h3><p>File Name: mysql.json</p><figure class=\"highlight\"><pre><code class=\"language-json\" data-lang=\"json\"><span class=\"p\">{</span><span class=\"w\"></span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"mysql-connector-db01\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"config\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"></span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"mysql-connector-db01\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"connector.class\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"io.debezium.connector.mysql.MySqlConnector\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.server.id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"1\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"tasks.max\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"1\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.history.kafka.bootstrap.servers\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"YOUR-BOOTSTRAP-SERVER:9092\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.history.kafka.topic\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"schema-changes.mysql\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.server.name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"mysql-db01\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.hostname\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"IP-OF-READER-NODE\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.port\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"3306\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.user\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"bhuvi\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.password\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"****\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.whitelist\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"bhuvi\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"snapshot.mode\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"initial\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"snapshot.locking.mode\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"none\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"key.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"value.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"key.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"value.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"internal.key.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"internal.value.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"internal.key.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"internal.value.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"transforms\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"unwrap\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"transforms.unwrap.add.source.fields\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"ts_ms\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"tombstones.on.delete\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"transforms.unwrap.type\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"io.debezium.transforms.ExtractNewRecordState}}</span></code></pre></figure><p>Run the below command to register the mysql connector.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">curl <span class=\"nt\">-X</span> POST <span class=\"nt\">-H</span> <span class=\"s2\">\"Accept: application/json\"</span> <span class=\"nt\">-H</span> <span class=\"s2\">\"Content-Type: application/json\"</span> http://localhost:8083/connectors <span class=\"nt\">-d</span> @mysql.json</code></pre></figure><p>Once the snapshot has been done, then it’ll push the binlog information of the Slave while taking the snapshot. And then it’ll start to continue to do CDC for the upcoming data. You will see the first record in your <code class=\"language-html highlighter-rouge\">connect-offsets</code> topic as like below.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"o\">{</span><span class=\"s2\">\"file\"</span>:<span class=\"s2\">\"ip-172-31-25-99-bin.000002\"</span>,<span class=\"s2\">\"pos\"</span>:7240<span class=\"o\">}</span></code></pre></figure><p>Then for continuous replication, it’ll start adding the record to this topic along with some more addition metadata like, server id, timestamp.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"o\">{</span><span class=\"s2\">\"ts_sec\"</span>:1577764293,<span class=\"s2\">\"file\"</span>:<span class=\"s2\">\"ip-172-31-25-99-bin.000002\"</span>,<span class=\"s2\">\"pos\"</span>:7305,<span class=\"s2\">\"row\"</span>:1,<span class=\"s2\">\"server_id\"</span>:1,<span class=\"s2\">\"event\"</span>:2<span class=\"o\">}</span></code></pre></figure><p>You can monitor the snapshot progress from JMX.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">curl localhost:7071 | <span class=\"nb\">grep </span>debezium_metrics_SecondsBehindMasterdebezium_metrics_SecondsBehindMaster<span class=\"o\">{</span><span class=\"nv\">context</span><span class=\"o\">=</span><span class=\"s2\">\"binlog\"</span>,name<span class=\"o\">=</span><span class=\"s2\">\"mysql-db01\"</span>,plugin<span class=\"o\">=</span><span class=\"s2\">\"mysql\"</span>,<span class=\"o\">}</span> 299.577536699E9</code></pre></figure><p>Sometimes the metrics take a few more minutes to update. So once you are able to see the last binlog information from the <code class=\"language-html highlighter-rouge\">connet-offsets</code> and from JMX the <code class=\"language-html highlighter-rouge\">lag <span class=\"nt\">&lt;10</span></code>, then the snapshot is done.</p><h2 id=\"switch-to-master\">Switch to Master:</h2><p>Before switching to the master, we need to stop the slave instance to get the consistent binlog information of Master from the Read replica. And then stop the Debezium connector to update binlog information manually in the <code class=\"language-html highlighter-rouge\">connect-offsets</code> topic.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"n\">mysql</span><span class=\"o\">-</span><span class=\"n\">slave</span><span class=\"o\">&gt;</span> <span class=\"n\">stop</span> <span class=\"n\">slave</span><span class=\"p\">;</span></code></pre></figure><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">Debezium-connector-node# systemctl stop confluent-connect-distributed</code></pre></figure><p>To simulate the real-time scenario, we can add 1 new row in our MySQL table. So this will never replicate to your slave. But once you switch the node, it should start reading from this row.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"n\">mysql</span><span class=\"o\">-</span><span class=\"n\">master</span><span class=\"o\">&gt;</span> <span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">rohi</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"s1\">'rohit'</span><span class=\"p\">,</span> <span class=\"s1\">'last'</span><span class=\"p\">,</span><span class=\"s1\">'87611'</span><span class=\"p\">);</span></code></pre></figure><p>Also create a new table and insert one new row to this new table.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"n\">mysql</span><span class=\"o\">-</span><span class=\"n\">master</span><span class=\"o\">&gt;</span> <span class=\"k\">create</span> <span class=\"k\">table</span> <span class=\"n\">testtbl</span> <span class=\"p\">(</span><span class=\"n\">id</span> <span class=\"nb\">int</span><span class=\"p\">);</span><span class=\"n\">mysql</span><span class=\"o\">-</span><span class=\"n\">master</span><span class=\"o\">&gt;</span> <span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">testtbl</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">);</span></code></pre></figure><p>Once the switchover has been done, then it should read the <code class=\"language-html highlighter-rouge\">6'th row</code> that we inserted and a new topic should be created for the <code class=\"language-html highlighter-rouge\">testtbl</code></p><h2 id=\"get-the-last-binlog-info-from-offsets\">Get the last binlog info from offsets:</h2><p>Install <code class=\"language-html highlighter-rouge\">kafkacat</code> in you broker node. (it’s available from <a href=\"https://docs.confluent.io/current/app-development/kafkacat-usage.html\">confluent repo</a>)</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">apt-get <span class=\"nb\">install </span>kafkacat</code></pre></figure><p>Run the below command get the last read binlog info.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">kafkacat <span class=\"nt\">-b</span> localhost:9092 <span class=\"nt\">-C</span> <span class=\"nt\">-t</span> connect-offsets  <span class=\"nt\">-f</span> <span class=\"s1\">'Partition(%p) %k %s\\n'</span></code></pre></figure><ul>  <li><strong>-b</strong> - Broker</li>  <li><strong>-C</strong> consumer</li>  <li><strong>-t</strong> Topic</li>  <li><strong>-f</strong> lag takes arguments specifying both the format of the output and the fields to include.</li></ul><p>You will get something like this.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">Partition<span class=\"o\">(</span>0<span class=\"o\">)</span> <span class=\"o\">[</span><span class=\"s2\">\"mysql-connector-db01\"</span>,<span class=\"o\">{</span><span class=\"s2\">\"server\"</span>:<span class=\"s2\">\"mysql-db01\"</span><span class=\"o\">}]</span> <span class=\"o\">{</span><span class=\"s2\">\"file\"</span>:<span class=\"s2\">\"ip-172-31-25-99-bin.000002\"</span>,<span class=\"s2\">\"pos\"</span>:7240<span class=\"o\">}</span>Partition<span class=\"o\">(</span>0<span class=\"o\">)</span> <span class=\"o\">[</span><span class=\"s2\">\"mysql-connector-db01\"</span>,<span class=\"o\">{</span><span class=\"s2\">\"server\"</span>:<span class=\"s2\">\"mysql-db01\"</span><span class=\"o\">}]</span> <span class=\"o\">{</span><span class=\"s2\">\"ts_sec\"</span>:1577764293,<span class=\"s2\">\"file\"</span>:<span class=\"s2\">\"ip-172-31-25-99-bin.000002\"</span>,<span class=\"s2\">\"pos\"</span>:7305,<span class=\"s2\">\"row\"</span>:1,<span class=\"s2\">\"server_id\"</span>:1,<span class=\"s2\">\"event\"</span>:2<span class=\"o\">}</span></code></pre></figure><ul>  <li><code class=\"language-html highlighter-rouge\">Partition(0)</code> - The Partition where the information is location.</li>  <li><code class=\"language-html highlighter-rouge\">mysql-connector-db01</code>  Connector Name</li>  <li><code class=\"language-html highlighter-rouge\">\"server\":\"mysql-db01\"</code> Server name that the connect has.</li>  <li><code class=\"language-html highlighter-rouge\">\"ts-sec\":1577764293,\"file\":\"ip-172-31-25-99-bin.000002\",\"pos\":7305,\"row\":1,\"server_id\":1,\"event\":2</code> - Binlog information</li></ul><p>Now we’ll manually push a new record inside this topic with the same information but just replace the binlog file name and its position. We need to continue the CDC where it stopped, so the get the exact starting binlog information we’ll use <code class=\"language-html highlighter-rouge\">slave status</code> from the Read replica.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"n\">mysql</span><span class=\"o\">-</span><span class=\"n\">slave</span><span class=\"o\">&gt;</span> <span class=\"k\">show</span> <span class=\"n\">slave</span> <span class=\"n\">status</span><span class=\"err\">\\</span><span class=\"k\">G</span>                   <span class=\"n\">Slave_IO_State</span><span class=\"p\">:</span>                      <span class=\"n\">Master_Host</span><span class=\"p\">:</span> <span class=\"mi\">172</span><span class=\"p\">.</span><span class=\"mi\">31</span><span class=\"p\">.</span><span class=\"mi\">36</span><span class=\"p\">.</span><span class=\"mi\">115</span>                      <span class=\"n\">Master_User</span><span class=\"p\">:</span> <span class=\"n\">bhuvi</span>                      <span class=\"n\">Master_Port</span><span class=\"p\">:</span> <span class=\"mi\">3306</span>                    <span class=\"n\">Connect_Retry</span><span class=\"p\">:</span> <span class=\"mi\">60</span>                  <span class=\"n\">Master_Log_File</span><span class=\"p\">:</span> <span class=\"n\">mysql</span><span class=\"o\">-</span><span class=\"n\">bin</span><span class=\"p\">.</span><span class=\"mi\">000003</span>              <span class=\"n\">Read_Master_Log_Pos</span><span class=\"p\">:</span> <span class=\"mi\">7759</span>                   <span class=\"n\">Relay_Log_File</span><span class=\"p\">:</span> <span class=\"n\">ip</span><span class=\"o\">-</span><span class=\"mi\">172</span><span class=\"o\">-</span><span class=\"mi\">31</span><span class=\"o\">-</span><span class=\"mi\">25</span><span class=\"o\">-</span><span class=\"mi\">99</span><span class=\"o\">-</span><span class=\"n\">relay</span><span class=\"o\">-</span><span class=\"n\">bin</span><span class=\"p\">.</span><span class=\"mi\">000009</span>                    <span class=\"n\">Relay_Log_Pos</span><span class=\"p\">:</span> <span class=\"mi\">7646</span>              <span class=\"n\">Exec_Master_Log_Pos</span><span class=\"p\">:</span> <span class=\"mi\">7759</span></code></pre></figure><p>Make a note of <code class=\"language-html highlighter-rouge\">Master-log-file</code> and <code class=\"language-html highlighter-rouge\">Exec-Master-Log-Pos</code> from the slave status. Now inject a new record to the offets topic.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nb\">echo</span> <span class=\"s1\">'[\"mysql-connector-db01\",{\"server\":\"mysql-db01\"}]|{\"file\":\"mysql-bin.000003\",\"pos\":7759}'</span> |<span class=\"se\">\\</span>kafkacat <span class=\"nt\">-P</span> <span class=\"nt\">-b</span> localhost:9092 <span class=\"nt\">-t</span> connect-offsets <span class=\"nt\">-K</span><span class=\"se\">\\ </span>| <span class=\"nt\">-p</span> 0</code></pre></figure><ul>  <li><strong>-b</strong> Broker</li>  <li><strong>-P</strong> Producer</li>  <li><strong>-K</strong> Delimiter</li>  <li><strong>-p</strong> Partition</li></ul><p>If you read the data from this topic, you’ll see the manually injected record.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">kafka-console-consumer <span class=\"nt\">--bootstrap-server</span> localhost:9092 <span class=\"nt\">--topic</span> connect-offsets <span class=\"nt\">--from-beginning</span><span class=\"o\">{</span><span class=\"s2\">\"file\"</span>:<span class=\"s2\">\"ip-172-31-25-99-bin.000002\"</span>,<span class=\"s2\">\"pos\"</span>:7240<span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"ts_sec\"</span>:1577764293,<span class=\"s2\">\"file\"</span>:<span class=\"s2\">\"ip-172-31-25-99-bin.000002\"</span>,<span class=\"s2\">\"pos\"</span>:7305,<span class=\"s2\">\"row\"</span>:1,<span class=\"s2\">\"server_id\"</span>:1,<span class=\"s2\">\"event\"</span>:2<span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"file\"</span>:<span class=\"s2\">\"mysql-bin.000003\"</span>,<span class=\"s2\">\"pos\"</span>:7759<span class=\"o\">}</span></code></pre></figure><p>Once you start the Debezium MySQL connector, then it’ll start reading from the slave but it’ll start looking for the binlog file <code class=\"language-html highlighter-rouge\">mysql-bin.000003</code> If you use the same binlog file name for both master and slave, then it’ll be a problem. So we can do any one of the following method to solve this.</p><ol>  <li>Use different naming conversion for both Master and Slave binlog files.</li>  <li>Delete all the binlog files from the Slave using <code class=\"language-html highlighter-rouge\">Reset master</code> command.</li>  <li>If the binlog file in slave is having a file named as <code class=\"language-html highlighter-rouge\">mysql-bin.000003</code> then delete this file alone.</li>  <li>If the binlog file in slave is having a file names as <code class=\"language-html highlighter-rouge\">mysql-bin.000003</code> then rename this file as <code class=\"language-html highlighter-rouge\">mysql-bin.000003.old</code></li></ol><blockquote>  <p><strong>Disclaimer</strong>: Please consider with your DBA before performing any of the above steps. I recommend using step 1 or 4.</p></blockquote><h3 id=\"start-the-debezium-connector\">Start the debezium connector:</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">Debezium-connector-node#  systemctl start confluent-connect-distributedv</code></pre></figure><p>You in your connector log file, you can see there is an error indicating that the Debezium is not able to find the binlog file called <code class=\"language-html highlighter-rouge\">mysql-bin.000003</code>.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"se\">\\[</span>2019-12-31 03:55:17,128<span class=\"se\">\\]</span> INFO WorkerSourceTask<span class=\"o\">{</span><span class=\"nb\">id</span><span class=\"o\">=</span>mysql-connector-db01-0<span class=\"o\">}</span> flushing 0 outstanding messages <span class=\"k\">for </span>offset commit <span class=\"o\">(</span>org.apache.kafka.connect.runtime.WorkerSourceTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2019-12-31 03:55:17,131<span class=\"se\">\\]</span> ERROR WorkerSourceTask<span class=\"o\">{</span><span class=\"nb\">id</span><span class=\"o\">=</span>mysql-connector-db01-0<span class=\"o\">}</span> Task threw an uncaught and unrecoverable exception <span class=\"o\">(</span>org.apache.kafka.connect.runtime.WorkerTask<span class=\"o\">)</span>org.apache.kafka.connect.errors.ConnectException: The connector is trying to <span class=\"nb\">read </span>binlog starting at binlog file <span class=\"s1\">'mysql-bin.000003'</span>, <span class=\"nv\">pos</span><span class=\"o\">=</span>7759, skipping 2 events plus 1 rows, but this is no longer available on the server. Reconfigure the connector to use a snapshot when needed.at io.debezium.connector.mysql.MySqlConnectorTask.start<span class=\"o\">(</span>MySqlConnectorTask.java:132<span class=\"o\">)</span>at io.debezium.connector.common.BaseSourceTask.start<span class=\"o\">(</span>BaseSourceTask.java:49<span class=\"o\">)</span>at org.apache.kafka.connect.runtime.WorkerSourceTask.execute<span class=\"o\">(</span>WorkerSourceTask.java:208<span class=\"o\">)</span>at org.apache.kafka.connect.runtime.WorkerTask.doRun<span class=\"o\">(</span>WorkerTask.java:177<span class=\"o\">)</span>at org.apache.kafka.connect.runtime.WorkerTask.run<span class=\"o\">(</span>WorkerTask.java:227<span class=\"o\">)</span>at java.base/java.util.concurrent.Executors<span class=\"nv\">$RunnableAdapter</span>.call<span class=\"o\">(</span>Executors.java:515<span class=\"o\">)</span>at java.base/java.util.concurrent.FutureTask.run<span class=\"o\">(</span>FutureTask.java:264<span class=\"o\">)</span>at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker<span class=\"o\">(</span>ThreadPoolExecutor.java:1128<span class=\"o\">)</span>at java.base/java.util.concurrent.ThreadPoolExecutor<span class=\"nv\">$Worker</span>.run<span class=\"o\">(</span>ThreadPoolExecutor.java:628<span class=\"o\">)</span>at java.base/java.lang.Thread.run<span class=\"o\">(</span>Thread.java:834<span class=\"o\">)</span><span class=\"se\">\\[</span>2019-12-31 03:55:17,132<span class=\"se\">\\]</span> ERROR WorkerSourceTask<span class=\"o\">{</span><span class=\"nb\">id</span><span class=\"o\">=</span>mysql-connector-db01-0<span class=\"o\">}</span> Task is being killed and will not recover <span class=\"k\">until </span>manually restarted <span class=\"o\">(</span>org.apache.kafka.connect.runtime.WorkerTask<span class=\"o\">)</span><span class=\"se\">\\[</span>2019-12-31 03:55:17,132<span class=\"se\">\\]</span> INFO Stopping MySQL connector task <span class=\"o\">(</span>io.debezium.connector.mysql.MySqlConnectorTask<span class=\"o\">)</span></code></pre></figure><p>Now we need to update the existing MySQL connector’s config and just change the <code class=\"language-html highlighter-rouge\">\"database.hostname\"</code> parameter.</p><blockquote>  <p><strong>Note</strong>: this JSON file format is different from the one which we used to register the connector. So make sure the syntax.</p></blockquote><p><strong><em>File Name</em></strong>: mysql-update.json</p><figure class=\"highlight\"><pre><code class=\"language-json\" data-lang=\"json\"><span class=\"p\">{</span><span class=\"w\"></span><span class=\"nl\">\"connector.class\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"io.debezium.connector.mysql.MySqlConnector\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"snapshot.locking.mode\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"none\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"tasks.max\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"3\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.history.kafka.topic\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"schema-changes.mysql\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"transforms\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"unwrap\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"internal.key.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"transforms.unwrap.add.source.fields\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"ts_ms\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"tombstones.on.delete\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"transforms.unwrap.type\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"io.debezium.transforms.ExtractNewRecordState\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"value.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.whitelist\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"bhuvi\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"key.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.user\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"bhuvi\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.server.id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"1\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.history.kafka.bootstrap.servers\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"YOUR-KAFKA-BOOTSTRAP-SERVER:9092\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.server.name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"mysql-db01\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.port\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"3306\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"key.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"internal.key.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.hostname\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"MASTER-IP-ADDRESS\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"database.password\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"****\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"internal.value.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"mysql-connector-db01\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"value.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"internal.value.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\"></span><span class=\"nl\">\"snapshot.mode\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"initial\"</span><span class=\"w\"></span><span class=\"p\">}</span></code></pre></figure><p>Run the below command to update the config file.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">curl <span class=\"nt\">-X</span> PUT <span class=\"nt\">-H</span> <span class=\"s2\">\"Accept: application/json\"</span> <span class=\"nt\">-H</span> <span class=\"s2\">\"Content-Type: application/json\"</span> http://localhost:8083/connectors/mysql-connector-db01/config <span class=\"nt\">-d</span> @mysql-update.json</code></pre></figure><p>Once the update is done, immediately it’ll start connecting to the master and start reading the binlog file <code class=\"language-html highlighter-rouge\">mysql-bin.000003</code> from position <code class=\"language-html highlighter-rouge\">7759</code>.</p><p>We inserted a new record to the <code class=\"language-html highlighter-rouge\">rohi</code> table. If you read this topic then you can see the row has been read. Also start inserting few more rows to this table with id 7 and 8.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">kafka-console-consumer <span class=\"nt\">--bootstrap-server</span> localhost:9092 <span class=\"nt\">--topic</span> mysql-db01.bhuvi.rohi <span class=\"nt\">--from-beginning</span><span class=\"o\">{</span><span class=\"s2\">\"id\"</span>:6,<span class=\"s2\">\"fn\"</span>:<span class=\"s2\">\"rohit\"</span>,<span class=\"s2\">\"ln\"</span>:<span class=\"s2\">\"last\"</span>,<span class=\"s2\">\"phone\"</span>:87611,<span class=\"s2\">\"__ts_ms\"</span>:1577788740000<span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"id\"</span>:7,<span class=\"s2\">\"fn\"</span>:<span class=\"s2\">\"rohit\"</span>,<span class=\"s2\">\"ln\"</span>:<span class=\"s2\">\"last\"</span>,<span class=\"s2\">\"phone\"</span>:87611,<span class=\"s2\">\"__ts_ms\"</span>:1577788764000<span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"id\"</span>:8,<span class=\"s2\">\"fn\"</span>:<span class=\"s2\">\"rohit\"</span>,<span class=\"s2\">\"ln\"</span>:<span class=\"s2\">\"last\"</span>,<span class=\"s2\">\"phone\"</span>:87611,<span class=\"s2\">\"__ts_ms\"</span>:1577788767000<span class=\"o\">}</span></code></pre></figure><p>Also, it should added the <code class=\"language-html highlighter-rouge\">testtbl</code> to the kafka topic.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">kafka-topics <span class=\"nt\">--zookeeper</span> localhost:2181 <span class=\"nt\">--list</span>connect-configsconnect-offsetsconnect-statusdefault_ksql_processing_logmy_connect_offsetsmysql-db01mysql-db01.bhuvi.rohimysql-db01.bhuvi.testtblschema-changes.mysql</code></pre></figure><p>Once your switchover is done, resume the replication on your slave.</p><h3 id=\"debezium-series-blogs\">Debezium Series blogs:</h3><ol>  <li><a href=\"https://thedataguy.in/build-production-grade-debezium-with-confluent-kafka-cluster/\">Build Production Grade Debezium Cluster With Confluent Kafka</a></li>  <li><a href=\"https://thedataguy.in/monitor-debezium-mysql-connector-with-prometheus-and-grafana/\">Monitor Debezium MySQL Connector With Prometheus And Grafana</a></li>  <li><a href=\"https://thedataguy.in/debezium-mysql-snapshot-from-read-replica-with-gtid/\">Debezium MySQL Snapshot From Read Replica With GTID</a></li>  <li><a href=\"https://thedataguy.in/debezium-mysql-snapshot-from-read-replica-and-resume-from-master/\">Debezium MySQL Snapshot From Read Replica And Resume From Master</a></li>  <li><a href=\"https://thedataguy.in/debezium-mysql-snapshot-for-aws-rds-aurora-from-backup-snaphot/\">Debezium MySQL Snapshot For AWS RDS Aurora From Backup Snaphot</a></li>  <li><a href=\"https://medium.com/searce/realtime-cdc-from-mysql-using-aws-msk-with-debezium-28da5a4ca873\">RealTime CDC From MySQL Using AWS MSK With Debezium</a></li></ol>",
            "url": "/2019/12/31/debezium-mysql-snapshot-from-read-replica-and-resume-from-master",
            "image": "/assets/Debezium MySQL Snapshot From Read Replica Without GTID - Custom Binlog.jpg",
            
            
            "tags": ["kafka","mysql","debezium","replication"],
            
            "date_published": "2019-12-31T12:10:00+00:00",
            "date_modified": "2019-12-31T12:10:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2019/12/28/debezium-mysql-snapshot-from-read-replica-with-gtid",
            "title": "Debezium MySQL Snapshot From Read Replica With GTID",
            "summary": "Take Debezium mysql connector snapshot from read replica using GTID. You can use this method to sync historical data for Debezium using read replica.",
            "content_text": "When you installed the Debezium MySQL connector, then it’ll start read your historical data and push all of them into the Kafka topics. This setting can we changed via snapshot.mode parameter in the connector. But if you are going to start a new sync, then Debezium will load the existing data its called Snapshot. Unfortunately, if you have a busy transactional MySQL database, then it may lead to some performance issues. And your DBA will never agree to read the data from Master Node.[Disclaimer: I’m a DBA :) ]. So I was thinking of figuring out to take the snapshot from the Read Replica, once the snapshot is done, then start read the realtime data from the Master. I found this useful information in a StackOverflow answer.  If your binlog uses GTID, you should be able to make a CDC tool like Debezium read the snapshot from the replica, then when that’s done, switch to the master to read the binlog. But if you don’t use GTID, that’s a little more tricky. The tool would have to know the binlog position on the master corresponding to the snapshot on the replica.  Source: https://stackoverflow.com/a/58250791/6885516Then I tried to implement in a realtime scenario and verified the statement is true. Yes, we made this in our system. Here is the step by step details from our PoC.Requirements:  Master and Slave should be enabled with GTID.  Debezium Connector Node can talk to both master and slave.  log-slave-updates must be enabled on the slave(anyhow for GTID its requires).  A user account for Debezium with respective permissions.  Install Debezium connector.Sample data:Create a new database to test this sync and insert some values.create database bhuvi;use bhuvi;create table rohi (id int,fn varchar(10),ln varchar(10),phone int);insert into rohi values (1, 'rohit', 'last',87611);insert into rohi values (2, 'rohit', 'last',87611);insert into rohi values (3, 'rohit', 'last',87611);insert into rohi values (4, 'rohit', 'last',87611);insert into rohi values (5, 'rohit', 'last',87611);Create the MySQL Connector Config:File Name: mysql.json{    \"name\": \"mysql-connector-db01\",    \"config\": {        \"name\": \"mysql-connector-db01\",        \"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\",        \"database.server.id\": \"1\",        \"tasks.max\": \"1\",        \"database.history.kafka.bootstrap.servers\": \"YOUR-BOOTSTRAP-SERVER:9092\",        \"database.history.kafka.topic\": \"schema-changes.mysql\",        \"database.server.name\": \"mysql-db01\",        \"database.hostname\": \"IP-OF-READER-NODE\",        \"database.port\": \"3306\",        \"database.user\": \"bhuvi\",        \"database.password\": \"****\",        \"database.whitelist\": \"bhuvi\",        \"snapshot.mode\": \"initial\",        \"snapshot.locking.mode\": \"none\",        \"key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",        \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",        \"key.converter.schemas.enable\": \"false\",        \"value.converter.schemas.enable\": \"false\",        \"internal.key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",        \"internal.value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",        \"internal.key.converter.schemas.enable\": \"false\",        \"internal.value.converter.schemas.enable\": \"false\",        \"transforms\": \"unwrap\",       \t\"transforms.unwrap.add.source.fields\": \"ts_ms\",  \t\t\"tombstones.on.delete\": \"false\",  \t\t\"transforms.unwrap.type\": \"io.debezium.transforms.ExtractNewRecordState    }}Watch the status of the connector:Open three terminal windows and start listening to the following topics.NOTE: change the bootstrap-server as per your cluster’s IP.  connect-configs  connect-statusFrom Terminal-1kafka-console-consumer --bootstrap-server localhost:9092 --topic connect-configs --from-beginningFrom Terminal-2kafka-console-consumer --bootstrap-server localhost:9092 --topic connect-status --from-beginningInstall the Connector:curl -X POST -H \"Accept: application/json\" -H \"Content-Type: application/json\" http://localhost:8083/connectors -d @mysql.jsonOnce you installed, from your connect-configs topic, you will get the following output.{\"properties\":{\"connector.class\":\"io.debezium.connector.mysql.MySqlConnector\",\"snapshot.locking.mode\":\"none\",\"database.user\":\"bhuvi\",\"database.server.id\":\"1\"\"tasks.max\":\"1\",\"database.history.kafka.bootstrap.servers\":\"172.31.40.132:9092\",\"database.history.kafka.topic\":\"schema-changes.mysql\"\"database.server.name\":\"mysql-db01\",\"internal.key.converter.schemas.enable\":\"false\",\"database.port\":\"3306\",\"key.converter.schemas.enable\":\"false\"\"internal.key.converter\":\"org.apache.kafka.connect.json.JsonConverter\",\"task.class\":\"io.debezium.connector.mysql.MySqlConnectorTask\"\"database.hostname\":\"172.31.25.99\",\"database.password\":\"*****\",\"internal.value.converter.schemas.enable\":\"false\",\"name\":\"mysql-connector-db01\"\"value.converter.schemas.enable\":\"false\",\"internal.value.converter\":\"org.apache.kafka.connect.json.JsonConverter\"\"value.converter\":\"org.apache.kafka.connect.json.JsonConverter\",\"database.whitelist\":\"bhuvi\",\"key.converter\":\"org.apache.kafka.connect.json.JsonConverter\"\"snapshot.mode\":\"initial\"}}{\"tasks\":1}And then from your connect-statustopic, you’ll get the status of your MySQL connector.{\"state\":\"RUNNING\",\"trace\":null,\"worker_id\":\"172.31.36.115:8083\",\"generation\":2}{\"state\":\"RUNNING\",\"trace\":null,\"worker_id\":\"172.31.36.115:8083\",\"generation\":3}Snapshot Status from the log file:By default, the Kafka connector’s logs will go to syslog. You can customize this log location. So wherever you have the log file, you can see the snapshot progress there.[2019-12-28 11:06:04,246] INFO Step 7: scanning contents of 1 tables while still in transaction (io.debezium.connector.mysql.SnapshotReader)[2019-12-28 11:06:04,252] INFO Step 7: - scanning table 'bhuvi.rohi' (1 of 1 tables) (io.debezium.connector.mysql.SnapshotReader)[2019-12-28 11:06:04,252] INFO For table 'bhuvi.rohi' using select statement: 'SELECT * FROM `bhuvi`.`rohi`' (io.debezium.connector.mysql.SnapshotReader)[2019-12-28 11:06:04,264] INFO Step 7: - Completed scanning a total of 31 rows from table 'bhuvi.rohi' after 00:00:00.012(io.debezium.connector.mysql.SnapshotReader)[2019-12-28 11:06:04,265] INFO Step 7: scanned 5 rows in 1 tables in 00:00:00.018 (io.debezium.connector.mysql.SnapshotReader)[2019-12-28 11:06:04,265] INFO Step 8: committing transaction (io.debezium.connector.mysql.SnapshotReader)[2019-12-28 11:06:04,267] INFO Completed snapshot in 00:00:01.896 (io.debezium.connector.mysql.SnapshotReader)[2019-12-28 11:06:04,348] WARN [Producer clientId=connector-producer-mysql-connector-db01-0] Error while fetching metadata with correlation id 7 :{mysql-db01.bhuvi.rohi=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)[2019-12-28 11:06:04,460] INFO Transitioning from the snapshot reader to the binlog reader (io.debezium.connector.mysql.ChainedReader)[2019-12-28 11:06:04,492] INFO GTID set purged on server: 88726004-2734-11ea-ae86-0e7687279b85:1-7 (io.debezium.connector.mysql.BinlogReader)[2019-12-28 11:06:04,492] INFO Attempting to generate a filtered GTID set (io.debezium.connector.mysql.MySqlTaskContext)[2019-12-28 11:06:04,492] INFO GTID set from previous recorded offset: 88726004-2734-11ea-ae86-0e7687279b85:1-11(io.debezium.connector.mysql.MySqlTaskContext)[2019-12-28 11:06:04,492] INFO GTID set available on server: 88726004-2734-11ea-ae86-0e7687279b85:1-11 (io.debezium.connector.mysql.MySqlTaskContext)[2019-12-28 11:06:04,492] INFO Final merged GTID set to use when connecting to MySQL: 88726004-2734-11ea-ae86-0e7687279b85:1-11(io.debezium.connector.mysql.MySqlTaskContext)[2019-12-28 11:06:04,492] INFO Registering binlog reader with GTID set: 88726004-2734-11ea-ae86-0e7687279b85:1-11 (io.debezium.connector.mysql.BinlogReader)Snapshot Complete:Once your’ snapshot process is done, then the connect-offsets topic will have the binlog information of till where it’s consumed.{\"file\":\"ip-172-31-25-99-bin.000001\",\"pos\":1234,\"gtids\":\"88726004-2734-11ea-ae86-0e7687279b85:1-11\"}Then it’ll start applying the ongoing replication changes as well.{\"ts_sec\":1577531225,\"file\":\"ip-172-31-25-99-bin.000001\",\"pos\":1299,\"gtids\":\"88726004-2734-11ea-ae86-0e7687279b85:1-11\",\"row\":1,\"server_id\":1,\"event\":2}Now we have verified that the database’s snapshot has been done. Its time to swap the nodes. We’ll start consuming from the Master.If you enable the Monitoring for the Debezium connector, then you see the lag from the JMX or Premetheus metrics.Reference: Configuring monitoring for Debezium MySQL Connector.curl localhost:7071 | grep debezium_metrics_SecondsBehindMasterdebezium_metrics_SecondsBehindMaster{context=\"binlog\",name=\"mysql-db01\",plugin=\"mysql\",} 299.577536699E9Sometimes the metrics take a few more minutes to update. So once you are able to see the last binlog information from the connet-offsets and the lag &lt;10, then the snapshot is done.Switch to Master:The main important thing is to STOP the slave thread in your Read replica. This will prevent the changing the GTID in your connect-offsets topic. mysql-slave&gt; STOP SLAVE;To simulate the sync, we can add 1 new row in our MySQL table.  So this will never replicate to your slave. But once you switch the node, it should start reading from this row.mysql-master&gt; insert into rohi values (6, 'rohit', 'last','87611');We need to update the existing MySQL connector’s config and just change the \"database.hostname\" parameter.  Note: this JSON file format is different from the one which we used to register the connector. So make sure the syntax.File Name: mysql-update.json{\t\"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\",\t\"snapshot.locking.mode\": \"none\",\t\"tasks.max\": \"3\",\t\"database.history.kafka.topic\": \"schema-changes.mysql\",\t\"transforms\": \"unwrap\",\t\"internal.key.converter.schemas.enable\": \"false\",\t\"transforms.unwrap.add.source.fields\": \"ts_ms\",\t\"tombstones.on.delete\": \"false\",\t\"transforms.unwrap.type\": \"io.debezium.transforms.ExtractNewRecordState\",\t\"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\t\"database.whitelist\": \"bhuvi\",\t\"key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\t\"database.user\": \"bhuvi\",\t\"database.server.id\": \"1\",\t\"database.history.kafka.bootstrap.servers\": \"YOUR-KAFKA-BOOTSTRAP-SERVER:9092\",\t\"database.server.name\": \"mysql-db01\",\t\"database.port\": \"3306\",\t\"key.converter.schemas.enable\": \"false\",\t\"internal.key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\t\"database.hostname\": \"MASTER-IP-ADDRESS\",\t\"database.password\": \"****\",\t\"internal.value.converter.schemas.enable\": \"false\",\t\"name\": \"mysql-connector-db01\",\t\"value.converter.schemas.enable\": \"false\",\t\"internal.value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\t\"snapshot.mode\": \"initial\"}Run the below command to update the config file.curl -X PUT -H \"Accept: application/json\" -H \"Content-Type: application/json\" http://localhost:8083/connectors/mysql-connector-db01/config -d @mysql-update.jsonOnce its updated, from the connect-offsets topic, you can see that the Debezium starts reading the data from the Next GTID.{\"ts_sec\":1577531276,\"file\":\"mysql-bin.000008\",\"pos\":1937,\"gtids\":\"88726004-2734-11ea-ae86-0e7687279b85:1-13\",\"row\":1,\"server_id\":1,\"event\":2}Also from your topic, you can see the last row has been pushed.kafka-console-consumer --bootstrap-server localhost:9092 --topic mysql-db01.bhuvi.rohi --from-beginning    {\"id\":1,\"fn\":\"rohit\",\"ln\":\"last\",\"phone\":87611,\"__ts_ms\":0}{\"id\":2,\"fn\":\"rohit\",\"ln\":\"last\",\"phone\":87611,\"__ts_ms\":0}{\"id\":3,\"fn\":\"rohit\",\"ln\":\"last\",\"phone\":87611,\"__ts_ms\":0}{\"id\":4,\"fn\":\"rohit\",\"ln\":\"last\",\"phone\":87611,\"__ts_ms\":0}{\"id\":5,\"fn\":\"rohit\",\"ln\":\"last\",\"phone\":87611,\"__ts_ms\":0}{\"id\":6,\"fn\":\"rohit\",\"ln\":\"last\",\"phone\":87611,\"__ts_ms\":1577531276000}This method helped us to sync the historical data from the Read replica to the Kafka topic without affecting the transactions on the Master node. Still, we are exploring this for more scenarios.  I’ll keep posting new articles about this.Debezium Series blogs:  Build Production Grade Debezium Cluster With Confluent Kafka  Monitor Debezium MySQL Connector With Prometheus And Grafana  Debezium MySQL Snapshot From Read Replica With GTID  Debezium MySQL Snapshot From Read Replica And Resume From Master  Debezium MySQL Snapshot For AWS RDS Aurora From Backup Snaphot  RealTime CDC From MySQL Using AWS MSK With Debezium",
            "content_html": "<p>When you installed the Debezium MySQL connector, then it’ll start read your historical data and push all of them into the Kafka topics. This setting can we changed via <code class=\"language-html highlighter-rouge\">snapshot.mode</code> parameter in the connector. But if you are going to start a new sync, then Debezium will load the existing data its called Snapshot. Unfortunately, if you have a busy transactional MySQL database, then it may lead to some performance issues. And your DBA will never agree to read the data from Master Node.[Disclaimer: I’m a DBA :) ]. So I was thinking of figuring out to take the snapshot from the Read Replica, once the snapshot is done, then start read the realtime data from the Master. I found this useful information in a StackOverflow answer.</p><blockquote>  <p>If your binlog uses GTID, you should be able to make a CDC tool like Debezium read the snapshot from the replica, then when that’s done, switch to the master to read the binlog. But if you don’t use GTID, that’s a little more tricky. The tool would have to know the binlog position on the master corresponding to the snapshot on the replica.</p>  <p><strong>Source</strong>: <a href=\"https://stackoverflow.com/a/58250791/6885516\" title=\"https://stackoverflow.com/a/58250791/6885516\">https://stackoverflow.com/a/58250791/6885516</a></p></blockquote><p>Then I tried to implement in a realtime scenario and verified the statement is true. Yes, we made this in our system. Here is the step by step details from our PoC.</p><h2 id=\"requirements\">Requirements:</h2><ul>  <li>Master and Slave should be enabled with GTID.</li>  <li>Debezium Connector Node can talk to both master and slave.</li>  <li><code class=\"language-html highlighter-rouge\">log-slave-updates</code> must be enabled on the slave(anyhow for GTID its requires).</li>  <li>A user account for Debezium with respective permissions.</li>  <li>Install Debezium connector.</li></ul><h2 id=\"sample-data\">Sample data:</h2><p>Create a new database to test this sync and insert some values.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">create</span> <span class=\"k\">database</span> <span class=\"n\">bhuvi</span><span class=\"p\">;</span><span class=\"n\">use</span> <span class=\"n\">bhuvi</span><span class=\"p\">;</span><span class=\"k\">create</span> <span class=\"k\">table</span> <span class=\"n\">rohi</span> <span class=\"p\">(</span><span class=\"n\">id</span> <span class=\"nb\">int</span><span class=\"p\">,</span><span class=\"n\">fn</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">),</span><span class=\"n\">ln</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">),</span><span class=\"n\">phone</span> <span class=\"nb\">int</span><span class=\"p\">);</span><span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">rohi</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">'rohit'</span><span class=\"p\">,</span> <span class=\"s1\">'last'</span><span class=\"p\">,</span><span class=\"mi\">87611</span><span class=\"p\">);</span><span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">rohi</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"s1\">'rohit'</span><span class=\"p\">,</span> <span class=\"s1\">'last'</span><span class=\"p\">,</span><span class=\"mi\">87611</span><span class=\"p\">);</span><span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">rohi</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"s1\">'rohit'</span><span class=\"p\">,</span> <span class=\"s1\">'last'</span><span class=\"p\">,</span><span class=\"mi\">87611</span><span class=\"p\">);</span><span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">rohi</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"s1\">'rohit'</span><span class=\"p\">,</span> <span class=\"s1\">'last'</span><span class=\"p\">,</span><span class=\"mi\">87611</span><span class=\"p\">);</span><span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">rohi</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"s1\">'rohit'</span><span class=\"p\">,</span> <span class=\"s1\">'last'</span><span class=\"p\">,</span><span class=\"mi\">87611</span><span class=\"p\">);</span></code></pre></figure><h3 id=\"create-the-mysql-connector-config\">Create the MySQL Connector Config:</h3><p>File Name: mysql.json</p><figure class=\"highlight\"><pre><code class=\"language-json\" data-lang=\"json\"><span class=\"p\">{</span><span class=\"w\">    </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"mysql-connector-db01\"</span><span class=\"p\">,</span><span class=\"w\">    </span><span class=\"nl\">\"config\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">        </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"mysql-connector-db01\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"connector.class\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"io.debezium.connector.mysql.MySqlConnector\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"database.server.id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"1\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"tasks.max\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"1\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"database.history.kafka.bootstrap.servers\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"YOUR-BOOTSTRAP-SERVER:9092\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"database.history.kafka.topic\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"schema-changes.mysql\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"database.server.name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"mysql-db01\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"database.hostname\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"IP-OF-READER-NODE\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"database.port\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"3306\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"database.user\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"bhuvi\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"database.password\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"****\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"database.whitelist\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"bhuvi\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"snapshot.mode\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"initial\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"snapshot.locking.mode\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"none\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"key.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"value.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"key.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"value.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"internal.key.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"internal.value.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"internal.key.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"internal.value.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"transforms\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"unwrap\"</span><span class=\"p\">,</span><span class=\"w\">       \t</span><span class=\"nl\">\"transforms.unwrap.add.source.fields\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"ts_ms\"</span><span class=\"p\">,</span><span class=\"w\">  \t\t</span><span class=\"nl\">\"tombstones.on.delete\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\">  \t\t</span><span class=\"nl\">\"transforms.unwrap.type\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"io.debezium.transforms.ExtractNewRecordState    }}</span></code></pre></figure><h3 id=\"watch-the-status-of-the-connector\">Watch the status of the connector:</h3><p>Open three terminal windows and start listening to the following topics.</p><p>NOTE: change the bootstrap-server as per your cluster’s IP.</p><ol>  <li>connect-configs</li>  <li>connect-status</li></ol><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">From Terminal-1kafka-console-consumer <span class=\"nt\">--bootstrap-server</span> localhost:9092 <span class=\"nt\">--topic</span> connect-configs <span class=\"nt\">--from-beginning</span>From Terminal-2kafka-console-consumer <span class=\"nt\">--bootstrap-server</span> localhost:9092 <span class=\"nt\">--topic</span> connect-status <span class=\"nt\">--from-beginning</span></code></pre></figure><h3 id=\"install-the-connector\">Install the Connector:</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">curl <span class=\"nt\">-X</span> POST <span class=\"nt\">-H</span> <span class=\"s2\">\"Accept: application/json\"</span> <span class=\"nt\">-H</span> <span class=\"s2\">\"Content-Type: application/json\"</span> http://localhost:8083/connectors <span class=\"nt\">-d</span> @mysql.json</code></pre></figure><p>Once you installed, from your <code class=\"language-html highlighter-rouge\">connect-configs</code> topic, you will get the following output.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"o\">{</span><span class=\"s2\">\"properties\"</span>:<span class=\"o\">{</span><span class=\"s2\">\"connector.class\"</span>:<span class=\"s2\">\"io.debezium.connector.mysql.MySqlConnector\"</span>,<span class=\"s2\">\"snapshot.locking.mode\"</span>:<span class=\"s2\">\"none\"</span>,<span class=\"s2\">\"database.user\"</span>:<span class=\"s2\">\"bhuvi\"</span>,<span class=\"s2\">\"database.server.id\"</span>:<span class=\"s2\">\"1\"\"tasks.max\"</span>:<span class=\"s2\">\"1\"</span>,<span class=\"s2\">\"database.history.kafka.bootstrap.servers\"</span>:<span class=\"s2\">\"172.31.40.132:9092\"</span>,<span class=\"s2\">\"database.history.kafka.topic\"</span>:<span class=\"s2\">\"schema-changes.mysql\"\"database.server.name\"</span>:<span class=\"s2\">\"mysql-db01\"</span>,<span class=\"s2\">\"internal.key.converter.schemas.enable\"</span>:<span class=\"s2\">\"false\"</span>,<span class=\"s2\">\"database.port\"</span>:<span class=\"s2\">\"3306\"</span>,<span class=\"s2\">\"key.converter.schemas.enable\"</span>:<span class=\"s2\">\"false\"\"internal.key.converter\"</span>:<span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span>,<span class=\"s2\">\"task.class\"</span>:<span class=\"s2\">\"io.debezium.connector.mysql.MySqlConnectorTask\"\"database.hostname\"</span>:<span class=\"s2\">\"172.31.25.99\"</span>,<span class=\"s2\">\"database.password\"</span>:<span class=\"s2\">\"*****\"</span>,<span class=\"s2\">\"internal.value.converter.schemas.enable\"</span>:<span class=\"s2\">\"false\"</span>,<span class=\"s2\">\"name\"</span>:<span class=\"s2\">\"mysql-connector-db01\"\"value.converter.schemas.enable\"</span>:<span class=\"s2\">\"false\"</span>,<span class=\"s2\">\"internal.value.converter\"</span>:<span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"\"value.converter\"</span>:<span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span>,<span class=\"s2\">\"database.whitelist\"</span>:<span class=\"s2\">\"bhuvi\"</span>,<span class=\"s2\">\"key.converter\"</span>:<span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"\"snapshot.mode\"</span>:<span class=\"s2\">\"initial\"</span><span class=\"o\">}}</span><span class=\"o\">{</span><span class=\"s2\">\"tasks\"</span>:1<span class=\"o\">}</span></code></pre></figure><p>And then from your <code class=\"language-html highlighter-rouge\">connect-status</code>topic, you’ll get the status of your MySQL connector.</p><figure class=\"highlight\"><pre><code class=\"language-json\" data-lang=\"json\"><span class=\"p\">{</span><span class=\"nl\">\"state\"</span><span class=\"p\">:</span><span class=\"s2\">\"RUNNING\"</span><span class=\"p\">,</span><span class=\"nl\">\"trace\"</span><span class=\"p\">:</span><span class=\"kc\">null</span><span class=\"p\">,</span><span class=\"nl\">\"worker_id\"</span><span class=\"p\">:</span><span class=\"s2\">\"172.31.36.115:8083\"</span><span class=\"p\">,</span><span class=\"nl\">\"generation\"</span><span class=\"p\">:</span><span class=\"mi\">2</span><span class=\"p\">}</span><span class=\"w\"></span><span class=\"p\">{</span><span class=\"nl\">\"state\"</span><span class=\"p\">:</span><span class=\"s2\">\"RUNNING\"</span><span class=\"p\">,</span><span class=\"nl\">\"trace\"</span><span class=\"p\">:</span><span class=\"kc\">null</span><span class=\"p\">,</span><span class=\"nl\">\"worker_id\"</span><span class=\"p\">:</span><span class=\"s2\">\"172.31.36.115:8083\"</span><span class=\"p\">,</span><span class=\"nl\">\"generation\"</span><span class=\"p\">:</span><span class=\"mi\">3</span><span class=\"p\">}</span></code></pre></figure><h3 id=\"snapshot-status-from-the-log-file\">Snapshot Status from the log file:</h3><p>By default, the Kafka connector’s logs will go to syslog. You can customize this log location. So wherever you have the log file, you can see the snapshot progress there.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"o\">[</span>2019-12-28 11:06:04,246] INFO Step 7: scanning contents of 1 tables <span class=\"k\">while </span>still <span class=\"k\">in </span>transaction <span class=\"o\">(</span>io.debezium.connector.mysql.SnapshotReader<span class=\"o\">)</span><span class=\"o\">[</span>2019-12-28 11:06:04,252] INFO Step 7: - scanning table <span class=\"s1\">'bhuvi.rohi'</span> <span class=\"o\">(</span>1 of 1 tables<span class=\"o\">)</span> <span class=\"o\">(</span>io.debezium.connector.mysql.SnapshotReader<span class=\"o\">)</span><span class=\"o\">[</span>2019-12-28 11:06:04,252] INFO For table <span class=\"s1\">'bhuvi.rohi'</span> using <span class=\"k\">select </span>statement: <span class=\"s1\">'SELECT * FROM `bhuvi`.`rohi`'</span> <span class=\"o\">(</span>io.debezium.connector.mysql.SnapshotReader<span class=\"o\">)</span><span class=\"o\">[</span>2019-12-28 11:06:04,264] INFO Step 7: - Completed scanning a total of 31 rows from table <span class=\"s1\">'bhuvi.rohi'</span> after 00:00:00.012<span class=\"o\">(</span>io.debezium.connector.mysql.SnapshotReader<span class=\"o\">)</span><span class=\"o\">[</span>2019-12-28 11:06:04,265] INFO Step 7: scanned 5 rows <span class=\"k\">in </span>1 tables <span class=\"k\">in </span>00:00:00.018 <span class=\"o\">(</span>io.debezium.connector.mysql.SnapshotReader<span class=\"o\">)</span><span class=\"o\">[</span>2019-12-28 11:06:04,265] INFO Step 8: committing transaction <span class=\"o\">(</span>io.debezium.connector.mysql.SnapshotReader<span class=\"o\">)</span><span class=\"o\">[</span>2019-12-28 11:06:04,267] INFO Completed snapshot <span class=\"k\">in </span>00:00:01.896 <span class=\"o\">(</span>io.debezium.connector.mysql.SnapshotReader<span class=\"o\">)</span><span class=\"o\">[</span>2019-12-28 11:06:04,348] WARN <span class=\"o\">[</span>Producer <span class=\"nv\">clientId</span><span class=\"o\">=</span>connector-producer-mysql-connector-db01-0] Error <span class=\"k\">while </span>fetching metadata with correlation <span class=\"nb\">id </span>7 :<span class=\"o\">{</span>mysql-db01.bhuvi.rohi<span class=\"o\">=</span>LEADER_NOT_AVAILABLE<span class=\"o\">}</span> <span class=\"o\">(</span>org.apache.kafka.clients.NetworkClient<span class=\"o\">)</span><span class=\"o\">[</span>2019-12-28 11:06:04,460] INFO Transitioning from the snapshot reader to the binlog reader <span class=\"o\">(</span>io.debezium.connector.mysql.ChainedReader<span class=\"o\">)</span><span class=\"o\">[</span>2019-12-28 11:06:04,492] INFO GTID <span class=\"nb\">set </span>purged on server: 88726004-2734-11ea-ae86-0e7687279b85:1-7 <span class=\"o\">(</span>io.debezium.connector.mysql.BinlogReader<span class=\"o\">)</span><span class=\"o\">[</span>2019-12-28 11:06:04,492] INFO Attempting to generate a filtered GTID <span class=\"nb\">set</span> <span class=\"o\">(</span>io.debezium.connector.mysql.MySqlTaskContext<span class=\"o\">)</span><span class=\"o\">[</span>2019-12-28 11:06:04,492] INFO GTID <span class=\"nb\">set </span>from previous recorded offset: 88726004-2734-11ea-ae86-0e7687279b85:1-11<span class=\"o\">(</span>io.debezium.connector.mysql.MySqlTaskContext<span class=\"o\">)</span><span class=\"o\">[</span>2019-12-28 11:06:04,492] INFO GTID <span class=\"nb\">set </span>available on server: 88726004-2734-11ea-ae86-0e7687279b85:1-11 <span class=\"o\">(</span>io.debezium.connector.mysql.MySqlTaskContext<span class=\"o\">)</span><span class=\"o\">[</span>2019-12-28 11:06:04,492] INFO Final merged GTID <span class=\"nb\">set </span>to use when connecting to MySQL: 88726004-2734-11ea-ae86-0e7687279b85:1-11<span class=\"o\">(</span>io.debezium.connector.mysql.MySqlTaskContext<span class=\"o\">)</span><span class=\"o\">[</span>2019-12-28 11:06:04,492] INFO Registering binlog reader with GTID <span class=\"nb\">set</span>: 88726004-2734-11ea-ae86-0e7687279b85:1-11 <span class=\"o\">(</span>io.debezium.connector.mysql.BinlogReader<span class=\"o\">)</span></code></pre></figure><h2 id=\"snapshot-complete\">Snapshot Complete:</h2><p>Once your’ snapshot process is done, then the <code class=\"language-html highlighter-rouge\">connect-offsets</code> topic will have the binlog information of till where it’s consumed.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"o\">{</span><span class=\"s2\">\"file\"</span>:<span class=\"s2\">\"ip-172-31-25-99-bin.000001\"</span>,<span class=\"s2\">\"pos\"</span>:1234,<span class=\"s2\">\"gtids\"</span>:<span class=\"s2\">\"88726004-2734-11ea-ae86-0e7687279b85:1-11\"</span><span class=\"o\">}</span></code></pre></figure><p>Then it’ll start applying the ongoing replication changes as well.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"o\">{</span><span class=\"s2\">\"ts_sec\"</span>:1577531225,<span class=\"s2\">\"file\"</span>:<span class=\"s2\">\"ip-172-31-25-99-bin.000001\"</span>,<span class=\"s2\">\"pos\"</span>:1299,<span class=\"s2\">\"gtids\"</span>:<span class=\"s2\">\"88726004-2734-11ea-ae86-0e7687279b85:1-11\"</span>,<span class=\"s2\">\"row\"</span>:1,<span class=\"s2\">\"server_id\"</span>:1,<span class=\"s2\">\"event\"</span>:2<span class=\"o\">}</span></code></pre></figure><p>Now we have verified that the database’s snapshot has been done. Its time to swap the nodes. We’ll start consuming from the Master.</p><p>If you enable the Monitoring for the Debezium connector, then you see the lag from the JMX or Premetheus metrics.</p><p><strong>Reference</strong>: <a href=\"https://thedataguy.in/monitor-debezium-mysql-connector-with-prometheus-and-grafana/\">Configuring monitoring for Debezium MySQL Connector</a>.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">curl localhost:7071 | <span class=\"nb\">grep </span>debezium_metrics_SecondsBehindMasterdebezium_metrics_SecondsBehindMaster<span class=\"o\">{</span><span class=\"nv\">context</span><span class=\"o\">=</span><span class=\"s2\">\"binlog\"</span>,name<span class=\"o\">=</span><span class=\"s2\">\"mysql-db01\"</span>,plugin<span class=\"o\">=</span><span class=\"s2\">\"mysql\"</span>,<span class=\"o\">}</span> 299.577536699E9</code></pre></figure><p>Sometimes the metrics take a few more minutes to update. So once you are able to see the last binlog information from the <code class=\"language-html highlighter-rouge\">connet-offsets</code> and the <code class=\"language-html highlighter-rouge\">lag <span class=\"nt\">&lt;10</span></code>, then the snapshot is done.</p><h2 id=\"switch-to-master\">Switch to Master:</h2><p>The main important thing is to STOP the slave thread in your Read replica. This will prevent the changing the GTID in your <code class=\"language-html highlighter-rouge\">connect-offsets</code> topic.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"> <span class=\"n\">mysql</span><span class=\"o\">-</span><span class=\"n\">slave</span><span class=\"o\">&gt;</span> <span class=\"n\">STOP</span> <span class=\"n\">SLAVE</span><span class=\"p\">;</span></code></pre></figure><p>To simulate the sync, we can add 1 new row in our MySQL table.  So this will never replicate to your slave. But once you switch the node, it should start reading from this row.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">mysql-master&gt; insert into rohi values <span class=\"o\">(</span>6, <span class=\"s1\">'rohit'</span>, <span class=\"s1\">'last'</span>,<span class=\"s1\">'87611'</span><span class=\"o\">)</span><span class=\"p\">;</span></code></pre></figure><p>We need to update the existing MySQL connector’s config and just change the <code class=\"language-html highlighter-rouge\">\"database.hostname\"</code> parameter.</p><blockquote>  <p><strong>Note</strong>: this JSON file format is different from the one which we used to register the connector. So make sure the syntax.</p></blockquote><p><strong><em>File Name</em></strong>: mysql-update.json</p><figure class=\"highlight\"><pre><code class=\"language-json\" data-lang=\"json\"><span class=\"p\">{</span><span class=\"w\">\t</span><span class=\"nl\">\"connector.class\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"io.debezium.connector.mysql.MySqlConnector\"</span><span class=\"p\">,</span><span class=\"w\">\t</span><span class=\"nl\">\"snapshot.locking.mode\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"none\"</span><span class=\"p\">,</span><span class=\"w\">\t</span><span class=\"nl\">\"tasks.max\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"3\"</span><span class=\"p\">,</span><span class=\"w\">\t</span><span class=\"nl\">\"database.history.kafka.topic\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"schema-changes.mysql\"</span><span class=\"p\">,</span><span class=\"w\">\t</span><span class=\"nl\">\"transforms\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"unwrap\"</span><span class=\"p\">,</span><span class=\"w\">\t</span><span class=\"nl\">\"internal.key.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\">\t</span><span class=\"nl\">\"transforms.unwrap.add.source.fields\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"ts_ms\"</span><span class=\"p\">,</span><span class=\"w\">\t</span><span class=\"nl\">\"tombstones.on.delete\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\">\t</span><span class=\"nl\">\"transforms.unwrap.type\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"io.debezium.transforms.ExtractNewRecordState\"</span><span class=\"p\">,</span><span class=\"w\">\t</span><span class=\"nl\">\"value.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\">\t</span><span class=\"nl\">\"database.whitelist\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"bhuvi\"</span><span class=\"p\">,</span><span class=\"w\">\t</span><span class=\"nl\">\"key.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\">\t</span><span class=\"nl\">\"database.user\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"bhuvi\"</span><span class=\"p\">,</span><span class=\"w\">\t</span><span class=\"nl\">\"database.server.id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"1\"</span><span class=\"p\">,</span><span class=\"w\">\t</span><span class=\"nl\">\"database.history.kafka.bootstrap.servers\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"YOUR-KAFKA-BOOTSTRAP-SERVER:9092\"</span><span class=\"p\">,</span><span class=\"w\">\t</span><span class=\"nl\">\"database.server.name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"mysql-db01\"</span><span class=\"p\">,</span><span class=\"w\">\t</span><span class=\"nl\">\"database.port\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"3306\"</span><span class=\"p\">,</span><span class=\"w\">\t</span><span class=\"nl\">\"key.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\">\t</span><span class=\"nl\">\"internal.key.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\">\t</span><span class=\"nl\">\"database.hostname\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"MASTER-IP-ADDRESS\"</span><span class=\"p\">,</span><span class=\"w\">\t</span><span class=\"nl\">\"database.password\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"****\"</span><span class=\"p\">,</span><span class=\"w\">\t</span><span class=\"nl\">\"internal.value.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\">\t</span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"mysql-connector-db01\"</span><span class=\"p\">,</span><span class=\"w\">\t</span><span class=\"nl\">\"value.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\">\t</span><span class=\"nl\">\"internal.value.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\">\t</span><span class=\"nl\">\"snapshot.mode\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"initial\"</span><span class=\"w\"></span><span class=\"p\">}</span></code></pre></figure><p>Run the below command to update the config file.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">curl <span class=\"nt\">-X</span> PUT <span class=\"nt\">-H</span> <span class=\"s2\">\"Accept: application/json\"</span> <span class=\"nt\">-H</span> <span class=\"s2\">\"Content-Type: application/json\"</span> http://localhost:8083/connectors/mysql-connector-db01/config <span class=\"nt\">-d</span> @mysql-update.json</code></pre></figure><p>Once its updated, from the <code class=\"language-html highlighter-rouge\">connect-offsets</code> topic, you can see that the Debezium starts reading the data from the Next GTID.</p><figure class=\"highlight\"><pre><code class=\"language-json\" data-lang=\"json\"><span class=\"p\">{</span><span class=\"nl\">\"ts_sec\"</span><span class=\"p\">:</span><span class=\"mi\">1577531276</span><span class=\"p\">,</span><span class=\"nl\">\"file\"</span><span class=\"p\">:</span><span class=\"s2\">\"mysql-bin.000008\"</span><span class=\"p\">,</span><span class=\"nl\">\"pos\"</span><span class=\"p\">:</span><span class=\"mi\">1937</span><span class=\"p\">,</span><span class=\"nl\">\"gtids\"</span><span class=\"p\">:</span><span class=\"s2\">\"88726004-2734-11ea-ae86-0e7687279b85:1-13\"</span><span class=\"p\">,</span><span class=\"nl\">\"row\"</span><span class=\"p\">:</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"nl\">\"server_id\"</span><span class=\"p\">:</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"nl\">\"event\"</span><span class=\"p\">:</span><span class=\"mi\">2</span><span class=\"p\">}</span></code></pre></figure><p>Also from your topic, you can see the last row has been pushed.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">kafka-console-consumer <span class=\"nt\">--bootstrap-server</span> localhost:9092 <span class=\"nt\">--topic</span> mysql-db01.bhuvi.rohi <span class=\"nt\">--from-beginning</span>    <span class=\"o\">{</span><span class=\"s2\">\"id\"</span>:1,<span class=\"s2\">\"fn\"</span>:<span class=\"s2\">\"rohit\"</span>,<span class=\"s2\">\"ln\"</span>:<span class=\"s2\">\"last\"</span>,<span class=\"s2\">\"phone\"</span>:87611,<span class=\"s2\">\"__ts_ms\"</span>:0<span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"id\"</span>:2,<span class=\"s2\">\"fn\"</span>:<span class=\"s2\">\"rohit\"</span>,<span class=\"s2\">\"ln\"</span>:<span class=\"s2\">\"last\"</span>,<span class=\"s2\">\"phone\"</span>:87611,<span class=\"s2\">\"__ts_ms\"</span>:0<span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"id\"</span>:3,<span class=\"s2\">\"fn\"</span>:<span class=\"s2\">\"rohit\"</span>,<span class=\"s2\">\"ln\"</span>:<span class=\"s2\">\"last\"</span>,<span class=\"s2\">\"phone\"</span>:87611,<span class=\"s2\">\"__ts_ms\"</span>:0<span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"id\"</span>:4,<span class=\"s2\">\"fn\"</span>:<span class=\"s2\">\"rohit\"</span>,<span class=\"s2\">\"ln\"</span>:<span class=\"s2\">\"last\"</span>,<span class=\"s2\">\"phone\"</span>:87611,<span class=\"s2\">\"__ts_ms\"</span>:0<span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"id\"</span>:5,<span class=\"s2\">\"fn\"</span>:<span class=\"s2\">\"rohit\"</span>,<span class=\"s2\">\"ln\"</span>:<span class=\"s2\">\"last\"</span>,<span class=\"s2\">\"phone\"</span>:87611,<span class=\"s2\">\"__ts_ms\"</span>:0<span class=\"o\">}</span><span class=\"o\">{</span><span class=\"s2\">\"id\"</span>:6,<span class=\"s2\">\"fn\"</span>:<span class=\"s2\">\"rohit\"</span>,<span class=\"s2\">\"ln\"</span>:<span class=\"s2\">\"last\"</span>,<span class=\"s2\">\"phone\"</span>:87611,<span class=\"s2\">\"__ts_ms\"</span>:1577531276000<span class=\"o\">}</span></code></pre></figure><p>This method helped us to sync the historical data from the Read replica to the Kafka topic without affecting the transactions on the Master node. Still, we are exploring this for more scenarios.  I’ll keep posting new articles about this.</p><h3 id=\"debezium-series-blogs\">Debezium Series blogs:</h3><ol>  <li><a href=\"https://thedataguy.in/build-production-grade-debezium-with-confluent-kafka-cluster/\">Build Production Grade Debezium Cluster With Confluent Kafka</a></li>  <li><a href=\"https://thedataguy.in/monitor-debezium-mysql-connector-with-prometheus-and-grafana/\">Monitor Debezium MySQL Connector With Prometheus And Grafana</a></li>  <li><a href=\"https://thedataguy.in/debezium-mysql-snapshot-from-read-replica-with-gtid/\">Debezium MySQL Snapshot From Read Replica With GTID</a></li>  <li><a href=\"https://thedataguy.in/debezium-mysql-snapshot-from-read-replica-and-resume-from-master/\">Debezium MySQL Snapshot From Read Replica And Resume From Master</a></li>  <li><a href=\"https://thedataguy.in/debezium-mysql-snapshot-for-aws-rds-aurora-from-backup-snaphot/\">Debezium MySQL Snapshot For AWS RDS Aurora From Backup Snaphot</a></li>  <li><a href=\"https://medium.com/searce/realtime-cdc-from-mysql-using-aws-msk-with-debezium-28da5a4ca873\">RealTime CDC From MySQL Using AWS MSK With Debezium</a></li></ol>",
            "url": "/2019/12/28/debezium-mysql-snapshot-from-read-replica-with-gtid",
            "image": "/assets/Debezium MySQL Snapshot From Read Replica With GTID.jpg",
            
            
            "tags": ["kafka","debezium","mysql"],
            
            "date_published": "2019-12-28T11:17:00+00:00",
            "date_modified": "2019-12-28T11:17:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2019/12/25/redshift-kill-all-locking-sessions-on-a-table",
            "title": "RedShift Kill All Locking Sessions On A Table",
            "summary": "Redshift kill all locking sessions on a particular table using a stored procedure. It'll collect all locking sessions pid and kill them in one shot.",
            "content_text": "In any relational database, if you didn’t close the session properly, then it’ll lock your DDL queries. It’s applicable to RedShift as well. A few days back I got a scenario that we have to run some DROP TABLE commands to create some lookup tables. But every time while triggering this DDL it got stuck. Then we realize there were some sessions that are still open and those sessions are causing this locking. There we 30+ sessions. I know we can fix this by properly closing the session from the application side. But in some emergency cases, we need to kill all open sessions or locking session in Redshift.Then my DBA brain was telling me to create a stored procedure to get all the locking sessions and kill them in one shot. I never recommend running this all the time. But if you are a DBA or RedShift Admin, then you need to have these kinds of handy toolkits.CREATE OR replace PROCEDURE sp_superkill(table_name VARCHAR(100)) LANGUAGE plpgsql AS   $$   DECLARE     list RECORD;     terminate_query      VARCHAR(50000);     drop_query VARCHAR(50000);   BEGIN     FOR list IN     SELECT a.datname,            c.relname,            a.procpid     FROM   pg_stat_activity a     join   pg_locks l     ON     l.pid = a.procpid     join   pg_class c     ON     c.oid = l.relation     WHERE  c.relname=table_name     LOOP     terminate_query:= 'select pg_terminate_backend('||list.procpid||')';     RAISE info 'Killing pid [%]', list.procpid;     EXECUTE terminate_query;   END LOOP;     --drop_query:='drop table '||table_name; --Add DDL If you want  --EXECUTE drop_query; --Add DDL If you wantEND; $$;Testing the Procedure:Open multiple sessions for a table and don’t close them.START TRANSACTIONS;Select col1 from my_table limit 1;Do open a few more sessions. Then run the stored procedure.call sp_superkill('my_table');INFO:  Killing pid [11734]INFO:  Killing pid [11735]INFO:  Killing pid [11738]INFO:  Killing pid [11739]CALL",
            "content_html": "<p>In any relational database, if you didn’t close the session properly, then it’ll lock your DDL queries. It’s applicable to RedShift as well. A few days back I got a scenario that we have to run some DROP TABLE commands to create some lookup tables. But every time while triggering this DDL it got stuck. Then we realize there were some sessions that are still open and those sessions are causing this locking. There we 30+ sessions. I know we can fix this by properly closing the session from the application side. But in some emergency cases, we need to kill all open sessions or locking session in Redshift.</p><p>Then my DBA brain was telling me to create a stored procedure to get all the locking sessions and kill them in one shot. I never recommend running this all the time. But if you are a DBA or RedShift Admin, then you need to have these kinds of handy toolkits.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">CREATE</span> <span class=\"k\">OR</span> <span class=\"k\">replace</span> <span class=\"k\">PROCEDURE</span> <span class=\"n\">sp_superkill</span><span class=\"p\">(</span><span class=\"k\">table_name</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">))</span> <span class=\"k\">LANGUAGE</span> <span class=\"n\">plpgsql</span> <span class=\"k\">AS</span>   <span class=\"err\">$$</span>   <span class=\"k\">DECLARE</span>     <span class=\"n\">list</span> <span class=\"n\">RECORD</span><span class=\"p\">;</span>     <span class=\"n\">terminate_query</span>      <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">50000</span><span class=\"p\">);</span>     <span class=\"n\">drop_query</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">50000</span><span class=\"p\">);</span>   <span class=\"k\">BEGIN</span>     <span class=\"k\">FOR</span> <span class=\"n\">list</span> <span class=\"k\">IN</span>     <span class=\"k\">SELECT</span> <span class=\"n\">a</span><span class=\"p\">.</span><span class=\"n\">datname</span><span class=\"p\">,</span>            <span class=\"k\">c</span><span class=\"p\">.</span><span class=\"n\">relname</span><span class=\"p\">,</span>            <span class=\"n\">a</span><span class=\"p\">.</span><span class=\"n\">procpid</span>     <span class=\"k\">FROM</span>   <span class=\"n\">pg_stat_activity</span> <span class=\"n\">a</span>     <span class=\"k\">join</span>   <span class=\"n\">pg_locks</span> <span class=\"n\">l</span>     <span class=\"k\">ON</span>     <span class=\"n\">l</span><span class=\"p\">.</span><span class=\"n\">pid</span> <span class=\"o\">=</span> <span class=\"n\">a</span><span class=\"p\">.</span><span class=\"n\">procpid</span>     <span class=\"k\">join</span>   <span class=\"n\">pg_class</span> <span class=\"k\">c</span>     <span class=\"k\">ON</span>     <span class=\"k\">c</span><span class=\"p\">.</span><span class=\"n\">oid</span> <span class=\"o\">=</span> <span class=\"n\">l</span><span class=\"p\">.</span><span class=\"n\">relation</span>     <span class=\"k\">WHERE</span>  <span class=\"k\">c</span><span class=\"p\">.</span><span class=\"n\">relname</span><span class=\"o\">=</span><span class=\"k\">table_name</span>     <span class=\"n\">LOOP</span>     <span class=\"n\">terminate_query</span><span class=\"p\">:</span><span class=\"o\">=</span> <span class=\"s1\">'select pg_terminate_backend('</span><span class=\"o\">||</span><span class=\"n\">list</span><span class=\"p\">.</span><span class=\"n\">procpid</span><span class=\"o\">||</span><span class=\"s1\">')'</span><span class=\"p\">;</span>     <span class=\"n\">RAISE</span> <span class=\"n\">info</span> <span class=\"s1\">'Killing pid [%]'</span><span class=\"p\">,</span> <span class=\"n\">list</span><span class=\"p\">.</span><span class=\"n\">procpid</span><span class=\"p\">;</span>     <span class=\"k\">EXECUTE</span> <span class=\"n\">terminate_query</span><span class=\"p\">;</span>   <span class=\"k\">END</span> <span class=\"n\">LOOP</span><span class=\"p\">;</span>     <span class=\"c1\">--drop_query:='drop table '||table_name; --Add DDL If you want</span>  <span class=\"c1\">--EXECUTE drop_query; --Add DDL If you want</span><span class=\"k\">END</span><span class=\"p\">;</span> <span class=\"err\">$$</span><span class=\"p\">;</span></code></pre></figure><h2 id=\"testing-the-procedure\">Testing the Procedure:</h2><p>Open multiple sessions for a table and don’t close them.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">START TRANSACTIONS<span class=\"p\">;</span>Select col1 from my_table limit 1<span class=\"p\">;</span></code></pre></figure><p>Do open a few more sessions. Then run the stored procedure.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">call sp_superkill<span class=\"o\">(</span><span class=\"s1\">'my_table'</span><span class=\"o\">)</span><span class=\"p\">;</span>INFO:  Killing pid <span class=\"o\">[</span>11734]INFO:  Killing pid <span class=\"o\">[</span>11735]INFO:  Killing pid <span class=\"o\">[</span>11738]INFO:  Killing pid <span class=\"o\">[</span>11739]CALL</code></pre></figure>",
            "url": "/2019/12/25/redshift-kill-all-locking-sessions-on-a-table",
            "image": "/assets/RedShift Kill All Locking Sessions On A Table.png",
            
            
            "tags": ["aws","redshift","sql"],
            
            "date_published": "2019-12-25T23:30:00+00:00",
            "date_modified": "2019-12-25T23:30:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2019/12/24/monitor-debezium-mysql-connector-with-prometheus-and-grafana",
            "title": "Monitor Debezium MySQL Connector With Prometheus And Grafana",
            "summary": "Setup JMX exporter monitoring for debezium MySQL connector with Prometheus and grafana. Download the common JSON dashboard template.",
            "content_text": "Debezium is providing out of the box CDC solution from various databases. In my last blog post, I have published how to configure the Debezium MySQL connector. This is the next part of that post. Once we deployed the debezium, to we need some kind of monitoring to keep track of whats happening in the debezium connector. Luckily Debezium has its own metrics that are already integrated with the connectors. We just need to capture them using the JMX exporter agent. Here I have written how to monitor Debezium MySQL connector with Prometheus and Grafana. But the dashboard is having the basic metrics only. You can build your own dashboard for more detailed monitoring.Reference: List of Debezium monitoring metricsInstall JMX exporter in Kafka Distributed connector:All the connectors are managed by the Kafka connect(Distributed or standalone). In our previous blog, we used Distributed Kafka connect service. So we are going to modify the distributed service binary file.Download the JMX exporter.    mkdir/opt/jmx/    cd /opt/jmx/    wget https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.12.0/jmx_prometheus_javaagent-0.12.0.jar    mv jmx_prometheus_javaagent-0.12.0.jar jmx-exporter.jar    Create config file.vi /opt/jmx/config.yml    startDelaySeconds: 0ssl: falselowercaseOutputName: falselowercaseOutputLabelNames: falserules:- pattern : \"kafka.connect&lt;type=connect-worker-metrics&gt;([^:]+):\"  name: \"kafka_connect_connect_worker_metrics_$1\"- pattern : \"kafka.connect&lt;type=connect-metrics, client-id=([^:]+)&gt;&lt;&gt;([^:]+)\"  name: \"kafka_connect_connect_metrics_$2\"  labels:    client: \"$1\"- pattern: \"debezium.([^:]+)&lt;type=connector-metrics, context=([^,]+), server=([^,]+), key=([^&gt;]+)&gt;&lt;&gt;RowsScanned\"  name: \"debezium_metrics_RowsScanned\"  labels:    plugin: \"$1\"    name: \"$3\"    context: \"$2\"    table: \"$4\"- pattern: \"debezium.([^:]+)&lt;type=connector-metrics, context=([^,]+), server=([^&gt;]+)&gt;([^:]+)\"  name: \"debezium_metrics_$4\"  labels:    plugin: \"$1\"    name: \"$3\"    context: \"$2\"Add the JMX export to the Kafka connect binary File.    vi /usr/bin/connect-distributed        -- Find this line below export CLASSPATH    exec $(dirname $0)/kafka-run-class $EXTRA_ARGS org.apache.kafka.connect.cli.ConnectDistributed \"$@\"        --Replace with    exec $(dirname $0)/kafka-run-class $EXTRA_ARGS -javaagent:/opt/jmx/jmx-exporter.jar=7071:/opt/jmx/config.yml org.apache.kafka.connect.cli.ConnectDistributed \"$@\"    Restart the Distributed Connect Service.    systemctl restart confluent-connect-distributed    Verify the JMX Agent installation.    netstat -tulpn | grep 7071    tcp6       0      0 :::7071                 :::*                    LISTEN      2885/java    Get the debezium metrics.    curl localhost:7071 | grep debezium    :-debezium_metrics_NumberOfDisconnects{context=\"binlog\",name=\"mysql-db01\",plugin=\"mysql\",} 0.    You can these metrics in your browser as well.    http://ip-of-the-connector-vm:7071/metrics    Install PrometheusIm using a separate server for Prometheus and Grafana.Create a user for Prometheus:    sudo useradd --no-create-home --shell /bin/false prometheus    Create Directories for Prometheus:    sudo mkdir /etc/prometheus    sudo mkdir /var/lib/prometheus    sudo chown prometheus:prometheus /etc/prometheus    sudo chown prometheus:prometheus /var/lib/prometheus    Download the Prometheus binary files:    cd /tmp    wget https://github.com/prometheus/prometheus/releases/download/v2.15.0/prometheus-2.15.0.linux-amd64.tar.gz    tar -zxvf prometheus-2.15.0.linux-amd64.tar.gz    Copy the binary files to respective locations:    cd prometheus-2.15.0.linux-amd64    cp prometheus /usr/local/bin/    cp promtool /usr/local/bin/    sudo chown prometheus:prometheus /usr/local/bin/prometheus    sudo chown prometheus:prometheus /usr/local/bin/promtool    cp -r consoles /etc/prometheus    cp -r console_libraries /etc/prometheus    sudo chown -R prometheus:prometheus /etc/prometheus/consoles    sudo chown -R prometheus:prometheus /etc/prometheus/console_libraries    Create a Prometheus config file:vi  /etc/prometheus/prometheus.yml    global:  scrape_interval: 15sscrape_configs:  - job_name: 'prometheus'    scrape_interval: 5s    static_configs:      - targets: ['localhost:9090']Set permission for config file:    sudo chown prometheus:prometheus /etc/prometheus/prometheus.yml    Create a Prometheus systemctl file:    vi /etc/systemd/system/prometheus.service        [Unit]    Description=Prometheus    Wants=network-online.target    After=network-online.target        [Service]    User=prometheus    Group=prometheus    Type=simple    ExecStart=/usr/local/bin/prometheus \\        --config.file /etc/prometheus/prometheus.yml \\        --storage.tsdb.path /var/lib/prometheus/ \\        --web.console.templates=/etc/prometheus/consoles \\        --web.console.libraries=/etc/prometheus/console_libraries        [Install]    WantedBy=multi-user.target    Start the Prometheus Service:    sudo systemctl daemon-reload    sudo systemctl start prometheus    sudo systemctl enable prometheus    Add Debezium MySQL connector metrics to Prometheus:vi  /etc/prometheus/prometheus.yml  - job_name: debezium    scrape_interval: 5s    static_configs:      - targets:          - debezium-node-ip:7071Restart the Prometheus service:    sudo systemctl restart prometheus    Check the status:In your browser Open the below URL.    http://IP_of-prometheus-ec2:9090/graph    Install Grafana:    wget https://dl.grafana.com/oss/release/grafana_6.5.2_amd64.deb    sudo dpkg -i grafana_6.5.2_amd64.deb    sudo systemctl daemon-reload    sudo systemctl start grafana-server    It’ll start listening to the port 3000. The default username and password admin/admin. You can change once you logged in.    http://grafana-server-ip:3000    Add the Debezium MySQL Dashboard:This dashboard is taken from the official Debezium’s example repo. But they gave this for MSSQL Server. With some changes and fixes, we can use the same for MySQL and other databases. I made it as a template.In grafana add the Prometheus datasource.    http://grafana-ip:3000/datasources    Click on Add Data source, select Prometheus.  Name: Prometheus  URL: localhost:9090 (I have installed grafana and Prometheus on the same server, If you have different server for Prometheus, use that IP instead of localhost).Click on Save &amp; Test.You’ll get a pop-up message that its is connected.Now go to the dashboards page and import the Template JSON.    http://grafan-ip:3000/dashboards    Click on Import button.Copy the Template JSON file from here. Paste it or download the JSON file and choose the upload button. Now the dashboard is ready. You can see a few basic metrics.Contribution:Debezium is a great platform for who wants to do real-time analytics. But in terms of monitoring, still, I feel it should get more contribution. This template is just a kickstart. We can build a more detailed monitoring dashboard for the debezium connectors. Please feel free to contribute to repo. Pull requests are welcome. Lets make the debezium more powerful.Debezium Series blogs:  Build Production Grade Debezium Cluster With Confluent Kafka  Monitor Debezium MySQL Connector With Prometheus And Grafana  Debezium MySQL Snapshot From Read Replica With GTID  Debezium MySQL Snapshot From Read Replica And Resume From Master  Debezium MySQL Snapshot For AWS RDS Aurora From Backup Snaphot  RealTime CDC From MySQL Using AWS MSK With Debezium",
            "content_html": "<p>Debezium is providing out of the box CDC solution from various databases. In my last blog post, I have published how to configure the Debezium MySQL connector. This is the next part of that post. Once we deployed the debezium, to we need some kind of monitoring to keep track of whats happening in the debezium connector. Luckily Debezium has its own metrics that are already integrated with the connectors. We just need to capture them using the JMX exporter agent. Here I have written how to monitor Debezium MySQL connector with Prometheus and Grafana. But the dashboard is having the basic metrics only. You can build your own dashboard for more detailed monitoring.</p><p><strong>Reference</strong>: <a href=\"https://debezium.io/documentation/reference/1.0/assemblies/cdc-mysql-connector/as_deploy-the-mysql-connector.html#mysql-connector-monitoring-metrics_debezium\">List of Debezium monitoring metrics</a></p><h2 id=\"install-jmx-exporter-in-kafka-distributed-connector\">Install JMX exporter in Kafka Distributed connector:</h2><p>All the connectors are managed by the Kafka connect(Distributed or standalone). In our previous blog, we used Distributed Kafka connect service. So we are going to modify the distributed service binary file.<br />Download the JMX exporter.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    <span class=\"nb\">mkdir</span>/opt/jmx/    <span class=\"nb\">cd</span> /opt/jmx/    wget https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.12.0/jmx_prometheus_javaagent-0.12.0.jar    <span class=\"nb\">mv </span>jmx_prometheus_javaagent-0.12.0.jar jmx-exporter.jar    </code></pre></figure><p>Create config file.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">vi /opt/jmx/config.yml    startDelaySeconds: 0ssl: <span class=\"nb\">false</span>lowercaseOutputName: <span class=\"nb\">false</span>lowercaseOutputLabelNames: <span class=\"nb\">false</span>rules:- pattern : <span class=\"s2\">\"kafka.connect&lt;type=connect-worker-metrics&gt;([^:]+):\"</span>  name: <span class=\"s2\">\"kafka_connect_connect_worker_metrics_</span><span class=\"nv\">$1</span><span class=\"s2\">\"</span>- pattern : <span class=\"s2\">\"kafka.connect&lt;type=connect-metrics, client-id=([^:]+)&gt;&lt;&gt;([^:]+)\"</span>  name: <span class=\"s2\">\"kafka_connect_connect_metrics_</span><span class=\"nv\">$2</span><span class=\"s2\">\"</span>  labels:    client: <span class=\"s2\">\"</span><span class=\"nv\">$1</span><span class=\"s2\">\"</span>- pattern: <span class=\"s2\">\"debezium.([^:]+)&lt;type=connector-metrics, context=([^,]+), server=([^,]+), key=([^&gt;]+)&gt;&lt;&gt;RowsScanned\"</span>  name: <span class=\"s2\">\"debezium_metrics_RowsScanned\"</span>  labels:    plugin: <span class=\"s2\">\"</span><span class=\"nv\">$1</span><span class=\"s2\">\"</span>    name: <span class=\"s2\">\"</span><span class=\"nv\">$3</span><span class=\"s2\">\"</span>    context: <span class=\"s2\">\"</span><span class=\"nv\">$2</span><span class=\"s2\">\"</span>    table: <span class=\"s2\">\"</span><span class=\"nv\">$4</span><span class=\"s2\">\"</span>- pattern: <span class=\"s2\">\"debezium.([^:]+)&lt;type=connector-metrics, context=([^,]+), server=([^&gt;]+)&gt;([^:]+)\"</span>  name: <span class=\"s2\">\"debezium_metrics_</span><span class=\"nv\">$4</span><span class=\"s2\">\"</span>  labels:    plugin: <span class=\"s2\">\"</span><span class=\"nv\">$1</span><span class=\"s2\">\"</span>    name: <span class=\"s2\">\"</span><span class=\"nv\">$3</span><span class=\"s2\">\"</span>    context: <span class=\"s2\">\"</span><span class=\"nv\">$2</span><span class=\"s2\">\"</span></code></pre></figure><p>Add the JMX export to the Kafka connect binary File.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    vi /usr/bin/connect-distributed        <span class=\"nt\">--</span> Find this line below <span class=\"nb\">export </span>CLASSPATH    <span class=\"nb\">exec</span> <span class=\"si\">$(</span><span class=\"nb\">dirname</span> <span class=\"nv\">$0</span><span class=\"si\">)</span>/kafka-run-class <span class=\"nv\">$EXTRA_ARGS</span> org.apache.kafka.connect.cli.ConnectDistributed <span class=\"s2\">\"</span><span class=\"nv\">$@</span><span class=\"s2\">\"</span>        <span class=\"nt\">--Replace</span> with    <span class=\"nb\">exec</span> <span class=\"si\">$(</span><span class=\"nb\">dirname</span> <span class=\"nv\">$0</span><span class=\"si\">)</span>/kafka-run-class <span class=\"nv\">$EXTRA_ARGS</span> <span class=\"nt\">-javaagent</span>:/opt/jmx/jmx-exporter.jar<span class=\"o\">=</span>7071:/opt/jmx/config.yml org.apache.kafka.connect.cli.ConnectDistributed <span class=\"s2\">\"</span><span class=\"nv\">$@</span><span class=\"s2\">\"</span>    </code></pre></figure><p>Restart the Distributed Connect Service.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    systemctl restart confluent-connect-distributed    </code></pre></figure><p>Verify the JMX Agent installation.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    netstat <span class=\"nt\">-tulpn</span> | <span class=\"nb\">grep </span>7071    tcp6       0      0 :::7071                 :::<span class=\"k\">*</span>                    LISTEN      2885/java    </code></pre></figure><p>Get the debezium metrics.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    curl localhost:7071 | <span class=\"nb\">grep </span>debezium    :-debezium_metrics_NumberOfDisconnects<span class=\"o\">{</span><span class=\"nv\">context</span><span class=\"o\">=</span><span class=\"s2\">\"binlog\"</span>,name<span class=\"o\">=</span><span class=\"s2\">\"mysql-db01\"</span>,plugin<span class=\"o\">=</span><span class=\"s2\">\"mysql\"</span>,<span class=\"o\">}</span> 0.    </code></pre></figure><p>You can these metrics in your browser as well.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    http://ip-of-the-connector-vm:7071/metrics    </code></pre></figure><h2 id=\"install-prometheus\">Install Prometheus</h2><p>Im using a separate server for Prometheus and Grafana.</p><p>Create a user for Prometheus:</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    <span class=\"nb\">sudo </span>useradd <span class=\"nt\">--no-create-home</span> <span class=\"nt\">--shell</span> /bin/false prometheus    </code></pre></figure><p>Create Directories for Prometheus:</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    <span class=\"nb\">sudo mkdir</span> /etc/prometheus    <span class=\"nb\">sudo mkdir</span> /var/lib/prometheus    <span class=\"nb\">sudo chown </span>prometheus:prometheus /etc/prometheus    <span class=\"nb\">sudo chown </span>prometheus:prometheus /var/lib/prometheus    </code></pre></figure><p>Download the Prometheus binary files:</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    <span class=\"nb\">cd</span> /tmp    wget https://github.com/prometheus/prometheus/releases/download/v2.15.0/prometheus-2.15.0.linux-amd64.tar.gz    <span class=\"nb\">tar</span> <span class=\"nt\">-zxvf</span> prometheus-2.15.0.linux-amd64.tar.gz    </code></pre></figure><p>Copy the binary files to respective locations:</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    <span class=\"nb\">cd </span>prometheus-2.15.0.linux-amd64    <span class=\"nb\">cp </span>prometheus /usr/local/bin/    <span class=\"nb\">cp </span>promtool /usr/local/bin/    <span class=\"nb\">sudo chown </span>prometheus:prometheus /usr/local/bin/prometheus    <span class=\"nb\">sudo chown </span>prometheus:prometheus /usr/local/bin/promtool    <span class=\"nb\">cp</span> <span class=\"nt\">-r</span> consoles /etc/prometheus    <span class=\"nb\">cp</span> <span class=\"nt\">-r</span> console_libraries /etc/prometheus    <span class=\"nb\">sudo chown</span> <span class=\"nt\">-R</span> prometheus:prometheus /etc/prometheus/consoles    <span class=\"nb\">sudo chown</span> <span class=\"nt\">-R</span> prometheus:prometheus /etc/prometheus/console_libraries    </code></pre></figure><p>Create a Prometheus config file:</p><figure class=\"highlight\"><pre><code class=\"language-yml\" data-lang=\"yml\"><span class=\"s\">vi  /etc/prometheus/prometheus.yml</span>    <span class=\"na\">global</span><span class=\"pi\">:</span>  <span class=\"na\">scrape_interval</span><span class=\"pi\">:</span> <span class=\"s\">15s</span><span class=\"na\">scrape_configs</span><span class=\"pi\">:</span>  <span class=\"pi\">-</span> <span class=\"na\">job_name</span><span class=\"pi\">:</span> <span class=\"s1\">'</span><span class=\"s\">prometheus'</span>    <span class=\"na\">scrape_interval</span><span class=\"pi\">:</span> <span class=\"s\">5s</span>    <span class=\"na\">static_configs</span><span class=\"pi\">:</span>      <span class=\"pi\">-</span> <span class=\"na\">targets</span><span class=\"pi\">:</span> <span class=\"pi\">[</span><span class=\"s1\">'</span><span class=\"s\">localhost:9090'</span><span class=\"pi\">]</span></code></pre></figure><p>Set permission for config file:</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    <span class=\"nb\">sudo chown </span>prometheus:prometheus /etc/prometheus/prometheus.yml    </code></pre></figure><p>Create a Prometheus systemctl file:</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    vi /etc/systemd/system/prometheus.service        <span class=\"o\">[</span>Unit]    <span class=\"nv\">Description</span><span class=\"o\">=</span>Prometheus    <span class=\"nv\">Wants</span><span class=\"o\">=</span>network-online.target    <span class=\"nv\">After</span><span class=\"o\">=</span>network-online.target        <span class=\"o\">[</span>Service]    <span class=\"nv\">User</span><span class=\"o\">=</span>prometheus    <span class=\"nv\">Group</span><span class=\"o\">=</span>prometheus    <span class=\"nv\">Type</span><span class=\"o\">=</span>simple    <span class=\"nv\">ExecStart</span><span class=\"o\">=</span>/usr/local/bin/prometheus <span class=\"se\">\\</span>        <span class=\"nt\">--config</span>.file /etc/prometheus/prometheus.yml <span class=\"se\">\\</span>        <span class=\"nt\">--storage</span>.tsdb.path /var/lib/prometheus/ <span class=\"se\">\\</span>        <span class=\"nt\">--web</span>.console.templates<span class=\"o\">=</span>/etc/prometheus/consoles <span class=\"se\">\\</span>        <span class=\"nt\">--web</span>.console.libraries<span class=\"o\">=</span>/etc/prometheus/console_libraries        <span class=\"o\">[</span>Install]    <span class=\"nv\">WantedBy</span><span class=\"o\">=</span>multi-user.target    </code></pre></figure><p>Start the Prometheus Service:</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    <span class=\"nb\">sudo </span>systemctl daemon-reload    <span class=\"nb\">sudo </span>systemctl start prometheus    <span class=\"nb\">sudo </span>systemctl <span class=\"nb\">enable </span>prometheus    </code></pre></figure><p>Add Debezium MySQL connector metrics to Prometheus:</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">vi  /etc/prometheus/prometheus.yml  - job_name: debezium    scrape_interval: 5s    static_configs:      - targets:          - debezium-node-ip:7071</code></pre></figure><p>Restart the Prometheus service:</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    <span class=\"nb\">sudo </span>systemctl restart prometheus    </code></pre></figure><p>Check the status:</p><p>In your browser Open the below URL.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    http://IP_of-prometheus-ec2:9090/graph    </code></pre></figure><p><img src=\"/assets/Monitor Debezium MySQL Connector With Prometheus And Grafana-2.jpg\" alt=\"\" /></p><h2 id=\"install-grafana\">Install Grafana:</h2><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    wget https://dl.grafana.com/oss/release/grafana_6.5.2_amd64.deb    <span class=\"nb\">sudo </span>dpkg <span class=\"nt\">-i</span> grafana_6.5.2_amd64.deb    <span class=\"nb\">sudo </span>systemctl daemon-reload    <span class=\"nb\">sudo </span>systemctl start grafana-server    </code></pre></figure><p>It’ll start listening to the port 3000. The default username and password <code class=\"language-html highlighter-rouge\">admin/admin</code>. You can change once you logged in.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    http://grafana-server-ip:3000    </code></pre></figure><p>Add the Debezium MySQL Dashboard:</p><p>This dashboard is taken from the official Debezium’s example repo. But they gave this for MSSQL Server. With some changes and fixes, we can use the same for MySQL and other databases. I made it as a template.<br />In grafana add the Prometheus datasource.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    http://grafana-ip:3000/datasources    </code></pre></figure><p>Click on Add Data source, select Prometheus.</p><ul>  <li><strong>Name</strong>: Prometheus</li>  <li><strong>URL</strong>: localhost:9090 (I have installed grafana and Prometheus on the same server, If you have different server for Prometheus, use that IP instead of localhost).</li></ul><p>Click on Save &amp; Test.</p><p>You’ll get a pop-up message that its is connected.</p><p>Now go to the dashboards page and import the Template JSON.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    http://grafan-ip:3000/dashboards    </code></pre></figure><p>Click on Import button.</p><p>Copy the Template JSON file from <a href=\"https://github.com/BhuviTheDataGuy/Debezium-monitor/blob/master/grafana-templates/debezium-monitor-template.json\"><strong>here</strong></a>. Paste it or download the JSON file and choose the upload button. Now the dashboard is ready. You can see a few basic metrics.</p><p><img src=\"/assets/Monitor Debezium MySQL Connector With Prometheus And Grafana-1.jpg\" alt=\"\" /></p><h2 id=\"contribution\">Contribution:</h2><p>Debezium is a great platform for who wants to do real-time analytics. But in terms of monitoring, still, I feel it should get more contribution. This template is just a kickstart. We can build a more detailed monitoring dashboard for the debezium connectors. Please feel free to contribute to <a href=\"https://github.com/BhuviTheDataGuy/Debezium-monitor/\"><strong>repo</strong></a>. Pull requests are welcome. Lets make the debezium more powerful.</p><h3 id=\"debezium-series-blogs\">Debezium Series blogs:</h3><ol>  <li><a href=\"https://thedataguy.in/build-production-grade-debezium-with-confluent-kafka-cluster/\">Build Production Grade Debezium Cluster With Confluent Kafka</a></li>  <li><a href=\"https://thedataguy.in/monitor-debezium-mysql-connector-with-prometheus-and-grafana/\">Monitor Debezium MySQL Connector With Prometheus And Grafana</a></li>  <li><a href=\"https://thedataguy.in/debezium-mysql-snapshot-from-read-replica-with-gtid/\">Debezium MySQL Snapshot From Read Replica With GTID</a></li>  <li><a href=\"https://thedataguy.in/debezium-mysql-snapshot-from-read-replica-and-resume-from-master/\">Debezium MySQL Snapshot From Read Replica And Resume From Master</a></li>  <li><a href=\"https://thedataguy.in/debezium-mysql-snapshot-for-aws-rds-aurora-from-backup-snaphot/\">Debezium MySQL Snapshot For AWS RDS Aurora From Backup Snaphot</a></li>  <li><a href=\"https://medium.com/searce/realtime-cdc-from-mysql-using-aws-msk-with-debezium-28da5a4ca873\">RealTime CDC From MySQL Using AWS MSK With Debezium</a></li></ol>",
            "url": "/2019/12/24/monitor-debezium-mysql-connector-with-prometheus-and-grafana",
            "image": "/assets/Monitor Debezium MySQL Connector With Prometheus And Grafana.jpg",
            
            
            "tags": ["Kafka","grafana","prometheus","debezium","mysql","jmx","monitoring"],
            
            "date_published": "2019-12-24T06:20:00+00:00",
            "date_modified": "2019-12-24T06:20:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2019/12/19/build-production-grade-debezium-with-confluent-kafka-cluster",
            "title": "Build Production Grade Debezium Cluster With Confluent Kafka",
            "summary": "Configure the Production grade debezium cluster with confluent kafka in aws. Sync data between MySQL and S3 with Debezium MySQL connector.",
            "content_text": "We are living in the DataLake world. Now almost every organizations wants their reporting in Near Real Time. Kafka is of the best streaming platform for realtime reporting. Based on the Kafka connector, RedHat designed the Debezium which is an OpenSource product and high recommended for real time CDC from transnational databases. I referred many blogs to setup this cluster. But I found just basic installation steps. So I setup this cluster for AWS with Production grade and publishing this blog.A shot intro:Debezium is a set of distributed services to capture changes in your databases so that your applications can see those changes and respond to them. Debezium records all row-level changes within each database table in a change event stream, and applications simply read these streams to see the change events in the same order in which they occurred.Basic Tech Terms:  Kafka Broker: Brokers are the core for the kafka streaming, they’ll keep your messages and giving it to the consumers.  Zookeeper: It’ll maintain the cluster status and node status. It’ll help to make the Kafka’s availability.  Producers: The component who will send the messages(data) to the Broker.  Consumers: The component who will get the messages from the Queue for further analytics.  Confluent: Confluent is having their own steaming platform which basically using Apache Kafka under the hood. But it has more features.Here Debezium is our data producer and S3sink is our consumer. For this setup, Im going to stream the MySQL data changes to S3 with customized format.AWS Architecture:Kafka and Zookeepers are installed on the same EC2. We we’ll deploy 3 node confluent Kafka cluster. Each node will be in a different availability zone.  172.31.47.152 - Zone A  172.31.38.158 - Zone B  172.31.46.207 - Zone CFor Producer(debezium) and Consumer(S3sink) will be hosted on the same Ec2. We’ll 3 nodes for this.  172.31.47.12 - Zone A  172.31.38.183 - Zone B  172.31.46.136 - Zone CInstance Type:Kafka nodes are generally needs Memory and Network Optimized. You can choose either Persistent and ephemeral storage. I prefer Persistent SSD Disks for Kafka storage. So add n GB size disk to your Kafka broker nodes. For Normal work loads its better to go with R5 instance Family.Mount the Volume in /kafkadata location.Security Group:Use a new Security group which allows the below ports.Installation:Install the Java and Kafka on all the Broker nodes.-- Install OpenJDKapt-get -y update sudo apt-get -y install default-jre-- Install Confluent Kafka platformwget -qO - https://packages.confluent.io/deb/5.3/archive.key | sudo apt-key add -sudo add-apt-repository \"deb [arch=amd64] https://packages.confluent.io/deb/5.3 stable main\"sudo apt-get update &amp;&amp; sudo apt-get install confluent-platform-2.12Configuration:We need to configure Zookeeper and Kafaka properties, Edit the /etc/kafka/zookeeper.properties on all the kafka nodes-- On Node 1dataDir=/var/lib/zookeeperclientPort=2181maxClientCnxns=0server.1=0.0.0.0:2888:3888server.2=172.31.38.158:2888:3888server.3=172.31.46.207:2888:3888autopurge.snapRetainCount=3autopurge.purgeInterval=24initLimit=5syncLimit=2-- On Node 2dataDir=/var/lib/zookeeperclientPort=2181maxClientCnxns=0server.1=172.31.47.152:2888:3888server.2=0.0.0.0:2888:3888server.3=172.31.46.207:2888:3888autopurge.snapRetainCount=3autopurge.purgeInterval=24initLimit=5syncLimit=2-- On Node 3dataDir=/var/lib/zookeeperclientPort=2181maxClientCnxns=0server.1=172.31.47.152:2888:3888server.2=172.31.38.158:2888:3888server.3=0.0.0.0:2888:3888autopurge.snapRetainCount=3autopurge.purgeInterval=24initLimit=5syncLimit=2We need to assign a unique ID for all the Zookeeper nodes. -- On Node 1 echo \"1\" &gt; /var/lib/zookeeper/myid  --On Node 2 echo \"2\" &gt; /var/lib/zookeeper/myid  --On Node 3 echo \"3\" &gt; /var/lib/zookeeper/myidNow we need to configure Kafka broker. So edit the /etc/kafka/server.properties on all the kafka nodes.--On Node 1broker.id.generation.enable=truedelete.topic.enable=truelisteners=PLAINTEXT://:9092zookeeper.connect=172.31.47.152:2181,172.31.38.158:2181,172.31.46.207:2181log.dirs=/kafkadata/kafkalog.retention.hours=168num.partitions=1--On Node 2broker.id.generation.enable=truedelete.topic.enable=truelisteners=PLAINTEXT://:9092log.dirs=/kafkadata/kafkazookeeper.connect=172.31.47.152:2181,172.31.38.158:2181,172.31.46.207:2181log.retention.hours=168num.partitions=1-- On Node 3broker.id.generation.enable=truedelete.topic.enable=truelisteners=PLAINTEXT://:9092log.dirs=/kafkadata/kafkazookeeper.connect=172.31.47.152:2181,172.31.38.158:2181,172.31.46.207:2181num.partitions=1log.retention.hours=168The next step is optimizing the Java JVM Heap size, In many places kafka will go down due to the less heap size. So Im allocating 50% of the Memory to Heap. But make sure more Heap size also bad. Please refer some documentation to set this value for very heavy systems.vi /usr/bin/kafka-server-startexport KAFKA_HEAP_OPTS=\"-Xmx2G -Xms2G\"The another major problem in the kafka system is the open file descriptors. So we need to allow the kafka to open at least up to 100000 files.vi /etc/pam.d/common-sessionsession required pam_limits.sovi /etc/security/limits.conf*                       soft    nofile          10000*                       hard    nofile          100000cp-kafka                soft    nofile          10000cp-kafka                hard    nofile          100000Here the cp-kafka is the default user for the kafka process.Create Kafka data dir:mkdir -p /kafkadata/kafkachown -R cp-kafka:confluent /kafkadata/kafkachmode 710 /kafkadata/kafkaStart the Kafka cluster:sudo systemctl start confluent-zookeepersudo systemctl start confluent-kafkasudo systemctl start confluent-schema-registryMake sure the Kafka has to automatically starts after the Ec2 restart.sudo systemctl enable confluent-zookeepersudo systemctl enable confluent-kafkasudo systemctl enable confluent-schema-registryNow our kafka cluster is ready. To check the list of system topics run the following command.kafka-topics --list --zookeeper localhost:2181__confluent.support.metricsSetup Debezium:Install the confluent connector and debezium MySQL connector on all the producer nodes.apt-get update sudo apt-get install default-jre wget -qO - https://packages.confluent.io/deb/5.3/archive.key | sudo apt-key add -sudo add-apt-repository \"deb [arch=amd64] https://packages.confluent.io/deb/5.3 stable main\"sudo apt-get update &amp;&amp; sudo apt-get install confluent-hub-client confluent-common confluent-kafka-connect-s3 confluent-kafka-2.12Configuration:Edit the /etc/kafka/connect-distributed.properties on all the producer nodes to make our producer will run on a distributed manner.-- On all the connector nodesbootstrap.servers=172.31.47.152:9092,172.31.38.158:9092,172.31.46.207:9092group.id=debezium-clusterplugin.path=/usr/share/java,/usr/share/confluent-hub-componentsInstall Debezium MySQL Connector:confluent-hub install debezium/debezium-connector-mysql:latestit’ll ask for making some changes just select Y for everything.Run the distributed connector as a service:vi /lib/systemd/system/confluent-connect-distributed.service[Unit]Description=Apache Kafka - connect-distributedDocumentation=http://docs.confluent.io/After=network.target[Service]Type=simpleUser=cp-kafkaGroup=confluentExecStart=/usr/bin/connect-distributed /etc/kafka/connect-distributed.propertiesTimeoutStopSec=180Restart=no[Install]WantedBy=multi-user.targetStart the Service:systemctl enable confluent-connect-distributedsystemctl start confluent-connect-distributedConfigure Debezium MySQL Connector:Create a mysql.json file which contains the MySQL information and other formatting options.{\t\"name\": \"mysql-connector-db01\",\t\"config\": {\t\t\"name\": \"mysql-connector-db01\",\t\t\"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\",\t\t\"database.server.id\": \"1\",\t\t\"tasks.max\": \"3\",\t\t\"database.history.kafka.bootstrap.servers\": \"172.31.47.152:9092,172.31.38.158:9092,172.31.46.207:9092\",\t\t\"database.history.kafka.topic\": \"schema-changes.mysql\",\t\t\"database.server.name\": \"mysql-db01\",\t\t\"database.hostname\": \"172.31.84.129\",\t\t\"database.port\": \"3306\",\t\t\"database.user\": \"bhuvi\",\t\t\"database.password\": \"my_stong_password\",\t\t\"database.whitelist\": \"proddb,test\",\t\t\"internal.key.converter.schemas.enable\": \"false\",\t\t\"key.converter.schemas.enable\": \"false\",\t\t\"internal.key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\t\t\"internal.value.converter.schemas.enable\": \"false\",\t\t\"value.converter.schemas.enable\": \"false\",\t\t\"internal.value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\t\t\"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\t\t\"key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\t\t\"transforms\": \"unwrap\",\t\t\"transforms.unwrap.type\": \"io.debezium.transforms.ExtractNewRecordState\",        \"transforms.unwrap.add.source.fields\": \"ts_ms\",\t\t\"tombstones.on.delete\": false\t}}  “database.history.kafka.bootstrap.servers” - Kafka Servers IP.  “database.whitelist” - List of databases to get the CDC.  key.converter and value.converter and transforms parameters - By default Debezium output will have more detailed information. But I don’t want all of those information. Im only interested in to get the new row and the timestamp when its inserted.If you don’t want to customize anythings then just remove everything after the database.whitelistRegister the MySQL Connector:curl -X POST -H \"Accept: application/json\" -H \"Content-Type: application/json\" http://localhost:8083/connectors -d @mysql.jsonCheck the status:curl GET localhost:8083/connectors/mysql-connector-db01/status{  \"name\": \"mysql-connector-db01\",  \"connector\": {    \"state\": \"RUNNING\",    \"worker_id\": \"172.31.94.191:8083\"  },  \"tasks\": [    {      \"id\": 0,      \"state\": \"RUNNING\",      \"worker_id\": \"172.31.94.191:8083\"    }  ],  \"type\": \"source\"}Test the MySQL Consumer:Now insert something into any tables in proddb or test (because we have whilelisted only these databaes to capture the CDC.use test;create table rohi (id int,fn varchar(10),ln varchar(10),phone int );insert into rohi values (2, 'rohit', 'ayare','87611');We can get these values from the Kafker brokers. Open any one the kafka node and run the below command.I prefer confluent cli for this. By default it’ll not be available, so download manually.curl -L https://cnfl.io/cli | sh -s -- -b /usr/bin/Listen the below topic:  mysql-db01.test.rohi This is the combination of servername.databasename.tablename servername(you mentioned this in as a server name in mysql json file).confluent local consume mysql-db01.test.rohi----The local commands are intended for a single-node development environmentonly, NOT for production usage. https://docs.confluent.io/current/cli/index.html-----{\"id\":1,\"fn\":\"rohit\",\"ln\":\"ayare\",\"phone\":87611,\"__ts_ms\":1576757407000}Setup S3 Sink connector in All Producer Nodes:I want to send this data to S3 bucket. So you must have an EC2 IAM role which has access to the target S3 bucket. Or install awscli and configure access and secret key(but its not recommended)Install S3 Connector:confluent-hub install confluentinc/kafka-connect-s3:latestCreate s3.json file.{\t\"name\": \"s3-sink-db01\",\t\"config\": {\t\t\"connector.class\": \"io.confluent.connect.s3.S3SinkConnector\",\t\t\"storage.class\": \"io.confluent.connect.s3.storage.S3Storage\",\t\t\"s3.bucket.name\": \"bhuvi-datalake\",\t\t\"name\": \"s3-sink-db01\",\t\t\"tasks.max\": \"3\",\t\t\"s3.region\": \"us-east-1\",\t\t\"s3.part.size\": \"5242880\",\t\t\"s3.compression.type\": \"gzip\",\t\t\"timezone\": \"UTC\",\t\t\"locale\": \"en\",\t\t\"flush.size\": \"10000\",\t\t\"rotate.interval.ms\": \"3600000\",\t\t\"topics.regex\": \"mysql-db01.(.*)\",\t\t\"internal.key.converter.schemas.enable\": \"false\",\t\t\"key.converter.schemas.enable\": \"false\",\t\t\"internal.key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\t\t\"format.class\": \"io.confluent.connect.s3.format.json.JsonFormat\",\t\t\"internal.value.converter.schemas.enable\": \"false\",\t\t\"value.converter.schemas.enable\": \"false\",\t\t\"internal.value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\t\t\"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\t\t\"key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\t\t\"partitioner.class\": \"io.confluent.connect.storage.partitioner.HourlyPartitioner\",\t\t\"path.format\": \"YYYY/MM/dd/HH\",\t\t\"partition.duration.ms\": \"3600000\",\t\t\"rotate.schedule.interval.ms\": \"3600000\"\t}}  \"topics.regex\": \"mysql-db01\" - It’ll send the data only from the topics which has mysql-db01 as prefix. In our case all the MySQL databases related topics will start  with this prefix.  \"flush.size\" - The data will uploaded to S3 only after these many number of records stored. Or after \"rotate.schedule.interval.ms\" this duration.Register this S3 sink connector:curl -X POST -H \"Accept: application/json\" -H \"Content-Type: application/json\" http://localhost:8083/connectors -d @s3Check the Status:curl GET localhost:8083/connectors/s3-sink-db01/status{  \"name\": \"s3-sink-db01\",  \"connector\": {    \"state\": \"RUNNING\",    \"worker_id\": \"172.31.94.191:8083\"  },  \"tasks\": [    {      \"id\": 0,      \"state\": \"RUNNING\",      \"worker_id\": \"172.31.94.191:8083\"    },    {      \"id\": 1,      \"state\": \"RUNNING\",      \"worker_id\": \"172.31.94.191:8083\"    },    {      \"id\": 2,      \"state\": \"RUNNING\",      \"worker_id\": \"172.31.94.191:8083\"    }  ],  \"type\": \"sink\"}Test the S3 sync:Insert the 10000 rows into the rohi table. Then check the S3 bucket. It’ll save the data in JSON format with GZIP compression. Also in a HOUR wise partitions.Monitoring:Refer this post to setup monitoring for MySQL Connector.More Tuning:  Replication Factor is the other main parameter to the data durability.  Use internal IP addresses as much as you can.  By default debezium uses 1 Partition per topic. You can configure this based on your work load. But more partitions more through put needed.References:  Setup Kafka in production by confluent  How to choose number of partition  Open file descriptors for Kafka   Kafka best practices in AWS  Debezium documentation  Customize debezium output with SMTDebezium Series blogs:  Build Production Grade Debezium Cluster With Confluent Kafka  Monitor Debezium MySQL Connector With Prometheus And Grafana  Debezium MySQL Snapshot From Read Replica With GTID  Debezium MySQL Snapshot From Read Replica And Resume From Master  Debezium MySQL Snapshot For AWS RDS Aurora From Backup Snaphot  RealTime CDC From MySQL Using AWS MSK With Debezium",
            "content_html": "<p>We are living in the DataLake world. Now almost every organizations wants their reporting in Near Real Time. Kafka is of the best streaming platform for realtime reporting. Based on the Kafka connector, RedHat designed the Debezium which is an OpenSource product and high recommended for real time CDC from transnational databases. I referred many blogs to setup this cluster. But I found just basic installation steps. So I setup this cluster for AWS with Production grade and publishing this blog.</p><h2 id=\"a-shot-intro\">A shot intro:</h2><p>Debezium is a set of distributed services to capture changes in your databases so that your applications can see those changes and respond to them. Debezium records all row-level changes within each database table in a <em>change event stream</em>, and applications simply read these streams to see the change events in the same order in which they occurred.</p><h2 id=\"basic-tech-terms\">Basic Tech Terms:</h2><ul>  <li><strong>Kafka Broker:</strong> Brokers are the core for the kafka streaming, they’ll keep your messages and giving it to the consumers.</li>  <li><strong>Zookeeper</strong>: It’ll maintain the cluster status and node status. It’ll help to make the Kafka’s availability.</li>  <li><strong>Producers:</strong> The component who will send the messages(data) to the Broker.</li>  <li><strong>Consumers:</strong> The component who will get the messages from the Queue for further analytics.</li>  <li><strong>Confluent:</strong> Confluent is having their own steaming platform which basically using Apache Kafka under the hood. But it has more features.</li></ul><p>Here <strong>Debezium</strong> is our data producer and <strong>S3sink</strong> is our consumer. For this setup, Im going to stream the MySQL data changes to S3 with customized format.</p><h2 id=\"aws-architecture\">AWS Architecture:</h2><p><img src=\"/assets/Build Production Grade Dedezium Cluster With Confluent Kafka-3.jpg\" alt=\"\" /></p><p>Kafka and Zookeepers are installed on the same EC2. We we’ll deploy 3 node confluent Kafka cluster. Each node will be in a different availability zone.</p><ul>  <li>172.31.47.152 - Zone A</li>  <li>172.31.38.158 - Zone B</li>  <li>172.31.46.207 - Zone C</li></ul><p>For Producer(debezium) and Consumer(S3sink) will be hosted on the same Ec2. We’ll 3 nodes for this.</p><ul>  <li>172.31.47.12 - Zone A</li>  <li>172.31.38.183 - Zone B</li>  <li>172.31.46.136 - Zone C</li></ul><h2 id=\"instance-type\">Instance Type:</h2><p>Kafka nodes are generally needs Memory and Network Optimized. You can choose either Persistent and ephemeral storage. I prefer Persistent SSD Disks for Kafka storage. So add n GB size disk to your Kafka broker nodes. For Normal work loads its better to go with R5 instance Family.</p><p>Mount the Volume in <code class=\"language-html highlighter-rouge\">/kafkadata</code> location.</p><h2 id=\"security-group\">Security Group:</h2><p>Use a new Security group which allows the below ports.</p><p><img src=\"/assets/Build Production Grade Dedezium Cluster With Confluent Kafka-4.jpg\" alt=\"\" /></p><h2 id=\"installation\">Installation:</h2><p>Install the Java and Kafka on all the Broker nodes.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nt\">--</span> Install OpenJDKapt-get <span class=\"nt\">-y</span> update <span class=\"nb\">sudo </span>apt-get <span class=\"nt\">-y</span> <span class=\"nb\">install </span>default-jre<span class=\"nt\">--</span> Install Confluent Kafka platformwget <span class=\"nt\">-qO</span> - https://packages.confluent.io/deb/5.3/archive.key | <span class=\"nb\">sudo </span>apt-key add -<span class=\"nb\">sudo </span>add-apt-repository <span class=\"s2\">\"deb [arch=amd64] https://packages.confluent.io/deb/5.3 stable main\"</span><span class=\"nb\">sudo </span>apt-get update <span class=\"o\">&amp;&amp;</span> <span class=\"nb\">sudo </span>apt-get <span class=\"nb\">install </span>confluent-platform-2.12</code></pre></figure><h2 id=\"configuration\">Configuration:</h2><p>We need to configure Zookeeper and Kafaka properties, Edit the <code class=\"language-html highlighter-rouge\">/etc/kafka/zookeeper.properties</code> on all the kafka nodes</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nt\">--</span> On Node 1<span class=\"nv\">dataDir</span><span class=\"o\">=</span>/var/lib/zookeeper<span class=\"nv\">clientPort</span><span class=\"o\">=</span>2181<span class=\"nv\">maxClientCnxns</span><span class=\"o\">=</span>0server.1<span class=\"o\">=</span>0.0.0.0:2888:3888server.2<span class=\"o\">=</span>172.31.38.158:2888:3888server.3<span class=\"o\">=</span>172.31.46.207:2888:3888autopurge.snapRetainCount<span class=\"o\">=</span>3autopurge.purgeInterval<span class=\"o\">=</span>24<span class=\"nv\">initLimit</span><span class=\"o\">=</span>5<span class=\"nv\">syncLimit</span><span class=\"o\">=</span>2<span class=\"nt\">--</span> On Node 2<span class=\"nv\">dataDir</span><span class=\"o\">=</span>/var/lib/zookeeper<span class=\"nv\">clientPort</span><span class=\"o\">=</span>2181<span class=\"nv\">maxClientCnxns</span><span class=\"o\">=</span>0server.1<span class=\"o\">=</span>172.31.47.152:2888:3888server.2<span class=\"o\">=</span>0.0.0.0:2888:3888server.3<span class=\"o\">=</span>172.31.46.207:2888:3888autopurge.snapRetainCount<span class=\"o\">=</span>3autopurge.purgeInterval<span class=\"o\">=</span>24<span class=\"nv\">initLimit</span><span class=\"o\">=</span>5<span class=\"nv\">syncLimit</span><span class=\"o\">=</span>2<span class=\"nt\">--</span> On Node 3<span class=\"nv\">dataDir</span><span class=\"o\">=</span>/var/lib/zookeeper<span class=\"nv\">clientPort</span><span class=\"o\">=</span>2181<span class=\"nv\">maxClientCnxns</span><span class=\"o\">=</span>0server.1<span class=\"o\">=</span>172.31.47.152:2888:3888server.2<span class=\"o\">=</span>172.31.38.158:2888:3888server.3<span class=\"o\">=</span>0.0.0.0:2888:3888autopurge.snapRetainCount<span class=\"o\">=</span>3autopurge.purgeInterval<span class=\"o\">=</span>24<span class=\"nv\">initLimit</span><span class=\"o\">=</span>5<span class=\"nv\">syncLimit</span><span class=\"o\">=</span>2</code></pre></figure><p>We need to assign a unique ID for all the Zookeeper nodes.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"> <span class=\"nt\">--</span> On Node 1 <span class=\"nb\">echo</span> <span class=\"s2\">\"1\"</span> <span class=\"o\">&gt;</span> /var/lib/zookeeper/myid  <span class=\"nt\">--On</span> Node 2 <span class=\"nb\">echo</span> <span class=\"s2\">\"2\"</span> <span class=\"o\">&gt;</span> /var/lib/zookeeper/myid  <span class=\"nt\">--On</span> Node 3 <span class=\"nb\">echo</span> <span class=\"s2\">\"3\"</span> <span class=\"o\">&gt;</span> /var/lib/zookeeper/myid</code></pre></figure><p>Now we need to configure Kafka broker. So edit the <code class=\"language-html highlighter-rouge\">/etc/kafka/server.properties</code> on all the kafka nodes.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nt\">--On</span> Node 1broker.id.generation.enable<span class=\"o\">=</span><span class=\"nb\">true</span>delete.topic.enable<span class=\"o\">=</span><span class=\"nb\">true</span><span class=\"nv\">listeners</span><span class=\"o\">=</span>PLAINTEXT://:9092zookeeper.connect<span class=\"o\">=</span>172.31.47.152:2181,172.31.38.158:2181,172.31.46.207:2181log.dirs<span class=\"o\">=</span>/kafkadata/kafkalog.retention.hours<span class=\"o\">=</span>168num.partitions<span class=\"o\">=</span>1<span class=\"nt\">--On</span> Node 2broker.id.generation.enable<span class=\"o\">=</span><span class=\"nb\">true</span>delete.topic.enable<span class=\"o\">=</span><span class=\"nb\">true</span><span class=\"nv\">listeners</span><span class=\"o\">=</span>PLAINTEXT://:9092log.dirs<span class=\"o\">=</span>/kafkadata/kafkazookeeper.connect<span class=\"o\">=</span>172.31.47.152:2181,172.31.38.158:2181,172.31.46.207:2181log.retention.hours<span class=\"o\">=</span>168num.partitions<span class=\"o\">=</span>1<span class=\"nt\">--</span> On Node 3broker.id.generation.enable<span class=\"o\">=</span><span class=\"nb\">true</span>delete.topic.enable<span class=\"o\">=</span><span class=\"nb\">true</span><span class=\"nv\">listeners</span><span class=\"o\">=</span>PLAINTEXT://:9092log.dirs<span class=\"o\">=</span>/kafkadata/kafkazookeeper.connect<span class=\"o\">=</span>172.31.47.152:2181,172.31.38.158:2181,172.31.46.207:2181num.partitions<span class=\"o\">=</span>1log.retention.hours<span class=\"o\">=</span>168</code></pre></figure><p>The next step is optimizing the <code class=\"language-html highlighter-rouge\">Java JVM Heap</code> size, In many places kafka will go down due to the less heap size. So Im allocating 50% of the Memory to Heap. But make sure more Heap size also bad. Please refer some documentation to set this value for very heavy systems.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">vi /usr/bin/kafka-server-start<span class=\"nb\">export </span><span class=\"nv\">KAFKA_HEAP_OPTS</span><span class=\"o\">=</span><span class=\"s2\">\"-Xmx2G -Xms2G\"</span></code></pre></figure><p>The another major problem in the kafka system is the open file descriptors. So we need to allow the kafka to open at least up to 100000 files.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">vi /etc/pam.d/common-sessionsession required pam_limits.sovi /etc/security/limits.conf<span class=\"k\">*</span>                       soft    nofile          10000<span class=\"k\">*</span>                       hard    nofile          100000cp-kafka                soft    nofile          10000cp-kafka                hard    nofile          100000</code></pre></figure><p>Here the <code class=\"language-html highlighter-rouge\">cp-kafka</code> is the default user for the kafka process.</p><h3 id=\"create-kafka-data-dir\">Create Kafka data dir:</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nb\">mkdir</span> <span class=\"nt\">-p</span> /kafkadata/kafka<span class=\"nb\">chown</span> <span class=\"nt\">-R</span> cp-kafka:confluent /kafkadata/kafkachmode 710 /kafkadata/kafka</code></pre></figure><h3 id=\"start-the-kafka-cluster\">Start the Kafka cluster:</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nb\">sudo </span>systemctl start confluent-zookeeper<span class=\"nb\">sudo </span>systemctl start confluent-kafka<span class=\"nb\">sudo </span>systemctl start confluent-schema-registry</code></pre></figure><p>Make sure the Kafka has to automatically starts after the Ec2 restart.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nb\">sudo </span>systemctl <span class=\"nb\">enable </span>confluent-zookeeper<span class=\"nb\">sudo </span>systemctl <span class=\"nb\">enable </span>confluent-kafka<span class=\"nb\">sudo </span>systemctl <span class=\"nb\">enable </span>confluent-schema-registry</code></pre></figure><p>Now our kafka cluster is ready. To check the list of system topics run the following command.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">kafka-topics <span class=\"nt\">--list</span> <span class=\"nt\">--zookeeper</span> localhost:2181__confluent.support.metrics</code></pre></figure><h2 id=\"setup-debezium\">Setup Debezium:</h2><p>Install the confluent connector and debezium MySQL connector on all the producer nodes.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">apt-get update <span class=\"nb\">sudo </span>apt-get <span class=\"nb\">install </span>default-jre wget <span class=\"nt\">-qO</span> - https://packages.confluent.io/deb/5.3/archive.key | <span class=\"nb\">sudo </span>apt-key add -<span class=\"nb\">sudo </span>add-apt-repository <span class=\"s2\">\"deb [arch=amd64] https://packages.confluent.io/deb/5.3 stable main\"</span><span class=\"nb\">sudo </span>apt-get update <span class=\"o\">&amp;&amp;</span> <span class=\"nb\">sudo </span>apt-get <span class=\"nb\">install </span>confluent-hub-client confluent-common confluent-kafka-connect-s3 confluent-kafka-2.12</code></pre></figure><h3 id=\"configuration-1\">Configuration:</h3><p>Edit the <code class=\"language-html highlighter-rouge\">/etc/kafka/connect-distributed.properties</code> on all the producer nodes to make our producer will run on a distributed manner.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nt\">--</span> On all the connector nodesbootstrap.servers<span class=\"o\">=</span>172.31.47.152:9092,172.31.38.158:9092,172.31.46.207:9092group.id<span class=\"o\">=</span>debezium-clusterplugin.path<span class=\"o\">=</span>/usr/share/java,/usr/share/confluent-hub-components</code></pre></figure><h3 id=\"install-debezium-mysql-connector\">Install Debezium MySQL Connector:</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">confluent-hub <span class=\"nb\">install </span>debezium/debezium-connector-mysql:latest</code></pre></figure><p>it’ll ask for making some changes just select <code class=\"language-html highlighter-rouge\">Y</code> for everything.</p><h3 id=\"run-the-distributed-connector-as-a-service\">Run the distributed connector as a service:</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">vi /lib/systemd/system/confluent-connect-distributed.service<span class=\"o\">[</span>Unit]<span class=\"nv\">Description</span><span class=\"o\">=</span>Apache Kafka - connect-distributed<span class=\"nv\">Documentation</span><span class=\"o\">=</span>http://docs.confluent.io/<span class=\"nv\">After</span><span class=\"o\">=</span>network.target<span class=\"o\">[</span>Service]<span class=\"nv\">Type</span><span class=\"o\">=</span>simple<span class=\"nv\">User</span><span class=\"o\">=</span>cp-kafka<span class=\"nv\">Group</span><span class=\"o\">=</span>confluent<span class=\"nv\">ExecStart</span><span class=\"o\">=</span>/usr/bin/connect-distributed /etc/kafka/connect-distributed.properties<span class=\"nv\">TimeoutStopSec</span><span class=\"o\">=</span>180<span class=\"nv\">Restart</span><span class=\"o\">=</span>no<span class=\"o\">[</span>Install]<span class=\"nv\">WantedBy</span><span class=\"o\">=</span>multi-user.target</code></pre></figure><h3 id=\"start-the-service\">Start the Service:</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">systemctl <span class=\"nb\">enable </span>confluent-connect-distributedsystemctl start confluent-connect-distributed</code></pre></figure><h2 id=\"configure-debezium-mysql-connector\">Configure Debezium MySQL Connector:</h2><p>Create a <code class=\"language-html highlighter-rouge\">mysql.json</code> file which contains the MySQL information and other formatting options.</p><figure class=\"highlight\"><pre><code class=\"language-json\" data-lang=\"json\"><span class=\"p\">{</span><span class=\"w\">\t</span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"mysql-connector-db01\"</span><span class=\"p\">,</span><span class=\"w\">\t</span><span class=\"nl\">\"config\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\t\t</span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"mysql-connector-db01\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"connector.class\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"io.debezium.connector.mysql.MySqlConnector\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"database.server.id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"1\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"tasks.max\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"3\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"database.history.kafka.bootstrap.servers\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"172.31.47.152:9092,172.31.38.158:9092,172.31.46.207:9092\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"database.history.kafka.topic\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"schema-changes.mysql\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"database.server.name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"mysql-db01\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"database.hostname\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"172.31.84.129\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"database.port\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"3306\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"database.user\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"bhuvi\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"database.password\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"my_stong_password\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"database.whitelist\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"proddb,test\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"internal.key.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"key.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"internal.key.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"internal.value.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"value.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"internal.value.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"value.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"key.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"transforms\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"unwrap\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"transforms.unwrap.type\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"io.debezium.transforms.ExtractNewRecordState\"</span><span class=\"p\">,</span><span class=\"w\">        </span><span class=\"nl\">\"transforms.unwrap.add.source.fields\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"ts_ms\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"tombstones.on.delete\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">false</span><span class=\"w\">\t</span><span class=\"p\">}</span><span class=\"w\"></span><span class=\"p\">}</span></code></pre></figure><ul>  <li>“database.history.kafka.bootstrap.servers” - Kafka Servers IP.</li>  <li>“database.whitelist” - List of databases to get the CDC.</li>  <li>key.converter and value.converter and transforms parameters - By default Debezium output will have more detailed information. But I don’t want all of those information. Im only interested in to get the new row and the timestamp when its inserted.</li></ul><p>If you don’t want to customize anythings then just remove everything after the <code class=\"language-html highlighter-rouge\">database.whitelist</code></p><h3 id=\"register-the-mysql-connector\">Register the MySQL Connector:</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">curl <span class=\"nt\">-X</span> POST <span class=\"nt\">-H</span> <span class=\"s2\">\"Accept: application/json\"</span> <span class=\"nt\">-H</span> <span class=\"s2\">\"Content-Type: application/json\"</span> http://localhost:8083/connectors <span class=\"nt\">-d</span> @mysql.json</code></pre></figure><h3 id=\"check-the-status\">Check the status:</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">curl GET localhost:8083/connectors/mysql-connector-db01/status<span class=\"o\">{</span>  <span class=\"s2\">\"name\"</span>: <span class=\"s2\">\"mysql-connector-db01\"</span>,  <span class=\"s2\">\"connector\"</span>: <span class=\"o\">{</span>    <span class=\"s2\">\"state\"</span>: <span class=\"s2\">\"RUNNING\"</span>,    <span class=\"s2\">\"worker_id\"</span>: <span class=\"s2\">\"172.31.94.191:8083\"</span>  <span class=\"o\">}</span>,  <span class=\"s2\">\"tasks\"</span>: <span class=\"o\">[</span>    <span class=\"o\">{</span>      <span class=\"s2\">\"id\"</span>: 0,      <span class=\"s2\">\"state\"</span>: <span class=\"s2\">\"RUNNING\"</span>,      <span class=\"s2\">\"worker_id\"</span>: <span class=\"s2\">\"172.31.94.191:8083\"</span>    <span class=\"o\">}</span>  <span class=\"o\">]</span>,  <span class=\"s2\">\"type\"</span>: <span class=\"s2\">\"source\"</span><span class=\"o\">}</span></code></pre></figure><h3 id=\"test-the-mysql-consumer\">Test the MySQL Consumer:</h3><p>Now insert something into any tables in <code class=\"language-html highlighter-rouge\">proddb or test</code> (because we have whilelisted only these databaes to capture the CDC.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"n\">use</span> <span class=\"n\">test</span><span class=\"p\">;</span><span class=\"k\">create</span> <span class=\"k\">table</span> <span class=\"n\">rohi</span> <span class=\"p\">(</span><span class=\"n\">id</span> <span class=\"nb\">int</span><span class=\"p\">,</span><span class=\"n\">fn</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">),</span><span class=\"n\">ln</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">),</span><span class=\"n\">phone</span> <span class=\"nb\">int</span> <span class=\"p\">);</span><span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">rohi</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"s1\">'rohit'</span><span class=\"p\">,</span> <span class=\"s1\">'ayare'</span><span class=\"p\">,</span><span class=\"s1\">'87611'</span><span class=\"p\">);</span></code></pre></figure><p>We can get these values from the Kafker brokers. Open any one the kafka node and run the below command.</p><p>I prefer confluent cli for this. By default it’ll not be available, so download manually.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">curl <span class=\"nt\">-L</span> https://cnfl.io/cli | sh <span class=\"nt\">-s</span> <span class=\"nt\">--</span> <span class=\"nt\">-b</span> /usr/bin/</code></pre></figure><h3 id=\"listen-the-below-topic\">Listen the below topic:</h3><blockquote>  <p><strong>mysql-db01.test.rohi</strong> <br />This is the combination of <code class=\"language-html highlighter-rouge\">servername.databasename.tablename</code> <br />servername(you mentioned this in as a server name in mysql json file).</p></blockquote><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">confluent <span class=\"nb\">local </span>consume mysql-db01.test.rohi<span class=\"nt\">----</span>The <span class=\"nb\">local </span>commands are intended <span class=\"k\">for </span>a single-node development environmentonly, NOT <span class=\"k\">for </span>production usage. https://docs.confluent.io/current/cli/index.html<span class=\"nt\">-----</span><span class=\"o\">{</span><span class=\"s2\">\"id\"</span>:1,<span class=\"s2\">\"fn\"</span>:<span class=\"s2\">\"rohit\"</span>,<span class=\"s2\">\"ln\"</span>:<span class=\"s2\">\"ayare\"</span>,<span class=\"s2\">\"phone\"</span>:87611,<span class=\"s2\">\"__ts_ms\"</span>:1576757407000<span class=\"o\">}</span></code></pre></figure><h2 id=\"setup-s3-sink-connector-in-all-producer-nodes\">Setup S3 Sink connector in All Producer Nodes:</h2><p>I want to send this data to S3 bucket. So you must have an EC2 IAM role which has access to the target S3 bucket. Or install <code class=\"language-html highlighter-rouge\">awscli</code> and configure access and secret key(but its not recommended)</p><h3 id=\"install-s3-connector\">Install S3 Connector:</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">confluent-hub <span class=\"nb\">install </span>confluentinc/kafka-connect-s3:latest</code></pre></figure><p>Create <code class=\"language-html highlighter-rouge\">s3.json</code> file.</p><figure class=\"highlight\"><pre><code class=\"language-json\" data-lang=\"json\"><span class=\"p\">{</span><span class=\"w\">\t</span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"s3-sink-db01\"</span><span class=\"p\">,</span><span class=\"w\">\t</span><span class=\"nl\">\"config\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\t\t</span><span class=\"nl\">\"connector.class\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"io.confluent.connect.s3.S3SinkConnector\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"storage.class\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"io.confluent.connect.s3.storage.S3Storage\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"s3.bucket.name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"bhuvi-datalake\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"s3-sink-db01\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"tasks.max\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"3\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"s3.region\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"us-east-1\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"s3.part.size\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"5242880\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"s3.compression.type\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"gzip\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"timezone\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"UTC\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"locale\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"en\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"flush.size\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"10000\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"rotate.interval.ms\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"3600000\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"topics.regex\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"mysql-db01.(.*)\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"internal.key.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"key.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"internal.key.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"format.class\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"io.confluent.connect.s3.format.json.JsonFormat\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"internal.value.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"value.converter.schemas.enable\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"false\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"internal.value.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"value.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"key.converter\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"org.apache.kafka.connect.json.JsonConverter\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"partitioner.class\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"io.confluent.connect.storage.partitioner.HourlyPartitioner\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"path.format\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"YYYY/MM/dd/HH\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"partition.duration.ms\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"3600000\"</span><span class=\"p\">,</span><span class=\"w\">\t\t</span><span class=\"nl\">\"rotate.schedule.interval.ms\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"3600000\"</span><span class=\"w\">\t</span><span class=\"p\">}</span><span class=\"w\"></span><span class=\"p\">}</span></code></pre></figure><ul>  <li><code class=\"language-html highlighter-rouge\">\"topics.regex\": \"mysql-db01\"</code> - It’ll send the data only from the topics which has <code class=\"language-html highlighter-rouge\">mysql-db01</code> as prefix. In our case all the MySQL databases related topics will start  with this prefix.</li>  <li><code class=\"language-html highlighter-rouge\">\"flush.size\"</code> - The data will uploaded to S3 only after these many number of records stored. Or after <code class=\"language-html highlighter-rouge\">\"rotate.schedule.interval.ms\"</code> this duration.</li></ul><h3 id=\"register-this-s3-sink-connector\">Register this S3 sink connector:</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">curl <span class=\"nt\">-X</span> POST <span class=\"nt\">-H</span> <span class=\"s2\">\"Accept: application/json\"</span> <span class=\"nt\">-H</span> <span class=\"s2\">\"Content-Type: application/json\"</span> http://localhost:8083/connectors <span class=\"nt\">-d</span> @s3</code></pre></figure><h3 id=\"check-the-status-1\">Check the Status:</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">curl GET localhost:8083/connectors/s3-sink-db01/status<span class=\"o\">{</span>  <span class=\"s2\">\"name\"</span>: <span class=\"s2\">\"s3-sink-db01\"</span>,  <span class=\"s2\">\"connector\"</span>: <span class=\"o\">{</span>    <span class=\"s2\">\"state\"</span>: <span class=\"s2\">\"RUNNING\"</span>,    <span class=\"s2\">\"worker_id\"</span>: <span class=\"s2\">\"172.31.94.191:8083\"</span>  <span class=\"o\">}</span>,  <span class=\"s2\">\"tasks\"</span>: <span class=\"o\">[</span>    <span class=\"o\">{</span>      <span class=\"s2\">\"id\"</span>: 0,      <span class=\"s2\">\"state\"</span>: <span class=\"s2\">\"RUNNING\"</span>,      <span class=\"s2\">\"worker_id\"</span>: <span class=\"s2\">\"172.31.94.191:8083\"</span>    <span class=\"o\">}</span>,    <span class=\"o\">{</span>      <span class=\"s2\">\"id\"</span>: 1,      <span class=\"s2\">\"state\"</span>: <span class=\"s2\">\"RUNNING\"</span>,      <span class=\"s2\">\"worker_id\"</span>: <span class=\"s2\">\"172.31.94.191:8083\"</span>    <span class=\"o\">}</span>,    <span class=\"o\">{</span>      <span class=\"s2\">\"id\"</span>: 2,      <span class=\"s2\">\"state\"</span>: <span class=\"s2\">\"RUNNING\"</span>,      <span class=\"s2\">\"worker_id\"</span>: <span class=\"s2\">\"172.31.94.191:8083\"</span>    <span class=\"o\">}</span>  <span class=\"o\">]</span>,  <span class=\"s2\">\"type\"</span>: <span class=\"s2\">\"sink\"</span><span class=\"o\">}</span></code></pre></figure><h3 id=\"test-the-s3-sync\">Test the S3 sync:</h3><p>Insert the 10000 rows into the <code class=\"language-html highlighter-rouge\">rohi</code> table. Then check the S3 bucket. It’ll save the data in JSON format with GZIP compression. Also in a HOUR wise partitions.</p><p><img src=\"/assets/Build Production Grade Dedezium Cluster With Confluent Kafka-1.jpg\" alt=\"\" /></p><p><img src=\"/assets/Build Production Grade Dedezium Cluster With Confluent Kafka-2.jpg\" alt=\"\" /></p><h2 id=\"monitoring\">Monitoring:</h2><p>Refer <a href=\"https://thedataguy.in/monitor-debezium-mysql-connector-with-prometheus-and-grafana/\">this post</a> to setup monitoring for MySQL Connector.</p><h2 id=\"more-tuning\">More Tuning:</h2><ul>  <li>Replication Factor is the other main parameter to the data durability.</li>  <li>Use internal IP addresses as much as you can.</li>  <li>By default debezium uses 1 Partition per topic. You can configure this based on your work load. But more partitions more through put needed.</li></ul><h2 id=\"references\">References:</h2><ol>  <li><a href=\"https://docs.confluent.io/current/kafka/deployment.html\">Setup Kafka in production by confluent</a></li>  <li><a href=\"https://www.confluent.io/blog/how-choose-number-topics-partitions-kafka-cluster/\">How to choose number of partition</a></li>  <li><a href=\"https://log-it.ro/2017/10/16/ubuntu-change-ulimit-kafka-not-ignore/\">Open file descriptors for Kafka </a></li>  <li><a href=\"https://aws.amazon.com/blogs/big-data/best-practices-for-running-apache-kafka-on-aws/\">Kafka best practices in AWS</a></li>  <li><a href=\"https://debezium.io/documentation/reference/1.0/tutorial.html\">Debezium documentation</a></li>  <li><a href=\"https://debezium.io/documentation/reference/1.0/configuration/event-flattening.html\">Customize debezium output with SMT</a></li></ol><h3 id=\"debezium-series-blogs\">Debezium Series blogs:</h3><ol>  <li><a href=\"https://thedataguy.in/build-production-grade-debezium-with-confluent-kafka-cluster/\">Build Production Grade Debezium Cluster With Confluent Kafka</a></li>  <li><a href=\"https://thedataguy.in/monitor-debezium-mysql-connector-with-prometheus-and-grafana/\">Monitor Debezium MySQL Connector With Prometheus And Grafana</a></li>  <li><a href=\"https://thedataguy.in/debezium-mysql-snapshot-from-read-replica-with-gtid/\">Debezium MySQL Snapshot From Read Replica With GTID</a></li>  <li><a href=\"https://thedataguy.in/debezium-mysql-snapshot-from-read-replica-and-resume-from-master/\">Debezium MySQL Snapshot From Read Replica And Resume From Master</a></li>  <li><a href=\"https://thedataguy.in/debezium-mysql-snapshot-for-aws-rds-aurora-from-backup-snaphot/\">Debezium MySQL Snapshot For AWS RDS Aurora From Backup Snaphot</a></li>  <li><a href=\"https://medium.com/searce/realtime-cdc-from-mysql-using-aws-msk-with-debezium-28da5a4ca873\">RealTime CDC From MySQL Using AWS MSK With Debezium</a></li></ol>",
            "url": "/2019/12/19/build-production-grade-debezium-with-confluent-kafka-cluster",
            "image": "/assets/Build Production Grade Dedezium Cluster With Confluent Kafka.jpg",
            
            
            "tags": ["aws","kafka","confluent","debezium","cdc","mysql","s3"],
            
            "date_published": "2019-12-19T15:43:00+00:00",
            "date_modified": "2019-12-19T15:43:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2019/11/22/redshift-unload-multiple-tables-schema-to-s3",
            "title": "RedShift Unload Like A Pro - Multiple Tables And Schemas",
            "summary": "In this blog we used a stored procedure to Unload multiple tables, specific or all the schema or all the tables in all schema to S3.",
            "content_text": "In my previous post, I explained how to unload all the tables in the RedShift database to S3 Bucket. But there was a limitation. We should export all the tables, you can’t specify some list of tables or all the tables in a specific schema. Its because of I can’t give comma separated values in RedShift stored procedure. But after spending few days I found a solution for this.What we’ll achieve here?  Unload many tables to S3.  Unload all the tables in a specific schema.  Unload specific tables in any schema.  All the tables in all the schema.Stored Procedure:You can refer my previous post to understand how it works and the meaning for the variables I used.Arguments Used:  s3_path - Location to export the data.  schema_name - Export the tables in this schema.  table_list - List of tables to export.To understand all other parameters read my previous post.Hardcoded Items:In the stored procedure, I have hardcoded the follow parameters.  IAM ROLE - arn:aws:iam::123123123123:role/myredshiftrole  Delimiter - |You can customize:Also, the following Items are hardcoded in the Unload query. You can get these things as variable or hardcoded as per your convenient.  MAXFILESIZE - 100 MB  PARALLEL  ADDQUOTES  HEADER  GZIPCreate a table for maintain the unload history:CREATE TABLE unload_history(pid          INT IDENTITY(1, 1),u_id         INT,u_timestamp  DATETIME,start_time   DATETIME,end_time     DATETIME,db_name      VARCHAR(100),table_schema VARCHAR(100),table_name   VARCHAR(100),export_query VARCHAR(65000),import_query VARCHAR(65000));Stored Procedure:    CREATE OR replace PROCEDURE unload_pro(s3_location VARCHAR(10000),                                                    schema_list VARCHAR(100),                                                    table_list  VARCHAR(1000))     LANGUAGE plpgsql     AS       $$       DECLARE         list RECORD;         db          VARCHAR(100);         tablename   VARCHAR(100);         tableschema VARCHAR(100);         starttime   datetime;         endtime     datetime;         SQL text;         s3_path      VARCHAR(1000);         iamrole      VARCHAR(100);         delimiter    VARCHAR(10);         max_filesize VARCHAR(100);         un_year      INT;         un_month     INT;         un_day       INT;         unload_query VARCHAR(65000);         copy_query   VARCHAR(65000);         unload_id    INT;         unload_time timestamp;         sc_name  VARCHAR(100);         tbl_list VARCHAR(1000);       BEGIN         -- Pass values for the variables         SELECT extract(year FROM getdate())         INTO   un_year;                  SELECT extract(month FROM getdate())         INTO   un_month;                  SELECT extract(day FROM getdate())         INTO   un_day;                  SELECT DISTINCT(table_catalog)         FROM            information_schema.TABLES         INTO            db;                  SELECT coalesce(max(u_id), 0)+1         FROM   unload_history         INTO   unload_id;                  SELECT getdate()         INTO   unload_time;                  s3_path:=s3_location;                 -- IAM ROLE and the Delimiter is hardcoded here         iamrole:='arn:aws:iam::123123123123:role/myredshiftrole';                 delimiter:='|';                 IF schema_list IS NULL THEN           DROP TABLE IF EXISTS sp_tmp_schemalist;                      CREATE temp TABLE sp_tmp_schemalist (sc_list VARCHAR(100));           INSERT INTO sp_tmp_schemalist           SELECT   nspname           FROM     pg_class pc           join     pg_namespace pn           ON       pc.relnamespace = pn.oid           WHERE    nspname NOT IN ( 'pg_catalog',                                    'information_schema',                                    'pg_toast')           AND      nspname NOT LIKE 'pg_%'           GROUP BY nspname;                      SELECT   listagg(sc_list, ',') within GROUP (ORDER BY sc_list)           FROM     sp_tmp_schemalist           INTO     sc_name;                  ELSE           sc_name:=schema_list;         END IF;                 IF table_list IS NULL THEN           DROP TABLE IF EXISTS sp_tmp_tablelist;                      CREATE temp TABLE sp_tmp_tablelist (tbl_name VARCHAR(100));           INSERT INTO sp_tmp_tablelist           SELECT relname           FROM   pg_class pc           join   pg_namespace pn           ON     pc.relnamespace = pn.oid           WHERE  nspname NOT IN ( 'pg_catalog',                                  'information_schema',                                  'pg_toast' )           AND    relname NOT LIKE 'sp_tmp_%';                      SELECT   listagg(tbl_name, ',') within GROUP (ORDER BY tbl_name)           FROM     sp_tmp_tablelist           INTO     tbl_list;                  ELSE           tbl_list:=table_list;         END IF;                 DROP TABLE IF EXISTS sp_tmp_quote_schema;         DROP TABLE IF EXISTS sp_tmp_quote_table;         DROP TABLE IF EXISTS sp_tmp_token_schema;         DROP TABLE IF EXISTS sp_tmp_token_table;                  CREATE TABLE sp_tmp_quote_schema                      (                                   comma_quote_schema VARCHAR(400)                      );                  CREATE TABLE sp_tmp_quote_table                      (                                   comma_quote_table VARCHAR(400)                      );                          EXECUTE 'INSERT INTO sp_tmp_quote_schema VALUES ('|| quote_literal(sc_name)|| ')';         EXECUTE 'INSERT INTO sp_tmp_quote_table VALUES ('|| quote_literal(tbl_list)|| ')';                 DROP TABLE IF EXISTS sp_tmp_ns;         CREATE temp TABLE sp_tmp_ns (n INT);                 INSERT INTO sp_tmp_ns         SELECT           ROW_NUMBER() OVER () as n        FROM              (SELECT 0 as n UNION SELECT 1) p0,             (SELECT 0 as n UNION SELECT 1) p1,             (SELECT 0 as n UNION SELECT 1) p2,             (SELECT 0 as n UNION SELECT 1) p3,             (SELECT 0 as n UNION SELECT 1) p4,             (SELECT 0 as n UNION SELECT 1) p5,             (SELECT 0 as n UNION SELECT 1) p6,             (SELECT 0 as n UNION SELECT 1) p7,             (SELECT 0 as n UNION SELECT 1) p8,             (SELECT 0 as n UNION SELECT 1) p9,             (SELECT 0 as n UNION SELECT 1) p10;                 SELECT     trim(split_part(b.comma_quote_schema, ',', ns.n)) AS sname         INTO       TABLE sp_tmp_token_schema         FROM       sp_tmp_ns ns         inner join sp_tmp_quote_schema b         ON         ns.n &lt;= regexp_count(b.comma_quote_schema, ',') + 1;                  SELECT     trim(split_part(b.comma_quote_table, ',', ns.n)) AS tname         INTO       TABLE sp_tmp_token_table         FROM       sp_tmp_ns ns         inner join sp_tmp_quote_table b         ON         ns.n &lt;= regexp_count(b.comma_quote_table, ',') + 1;                  FOR list IN         SELECT nspname :: text AS table_schema,                relname :: text AS table_name         FROM   pg_class pc         join   pg_namespace pn         ON     pc.relnamespace = pn.oid         WHERE  trim(relname :: text) IN                (                       SELECT tname                       FROM   sp_tmp_token_table)         AND    relname !='unload_history'         AND    trim(nspname :: text) IN                (                       SELECT sname                       FROM   sp_tmp_token_schema) LOOP         SELECT getdate()         INTO   starttime;                  SQL:='select * from '         ||list.table_schema         ||'.'         ||list.table_name         ||'' ;         RAISE info '[%] Unloading... schema = % and table = %',starttime, list.table_schema, list.table_name;        -- Start unloading the data         unload_query := 'unload ('''         ||SQL         ||''') to '''         ||s3_path         ||un_year         ||'/'         ||un_month         ||'/'         ||un_day         ||'/'         ||db         ||'/'         ||list.table_schema         ||'/'         ||list.table_name         ||'/'         ||list.table_schema         ||'-'         ||list.table_name         ||'_'' iam_role '''         ||iamrole         ||''' delimiter '''         ||delimiter         ||''' MAXFILESIZE 300 MB PARALLEL ADDQUOTES HEADER GZIP';                 EXECUTE unload_query;                 copy_query := 'copy '         ||list.table_schema         ||'.'         ||list.table_name         ||' from '''         ||s3_path         ||un_year         ||'/'         ||un_month         ||'/'         ||un_day         ||'/'         ||db         ||'/'         ||list.table_schema         ||'/'         ||list.table_name         ||'/'' iam_role '''         ||iamrole         ||''' delimiter '''         ||delimiter         ||''' IGNOREHEADER 1 REMOVEQUOTES gzip';                 SELECT getdate()         INTO   endtime;                  SELECT list.table_schema         INTO   tableschema;                  SELECT list.table_name         INTO   tablename;                  -- Insert into the history table         INSERT INTO unload_history                     (                                 u_id,                                 u_timestamp,                                 start_time,                                 end_time,                                 db_name,                                 table_schema,                                 table_name,                                 export_query,                                 import_query                     )                     VALUES                     (                                 unload_id,                                 unload_time,                                 starttime,                                 endtime,                                 db,                                 tableschema,                                 tablename,                                 unload_query,                                 copy_query                     );              END LOOP;       RAISE info ' Unloading of the DB [%] is success !!!' ,db;     END;     $$;I have less than 2048 tables, if you have more than that, just add few more select unions in the below portion.    SELECT           ROW_NUMBER() OVER () as n        FROM              (SELECT 0 as n UNION SELECT 1) p0,             (SELECT 0 as n UNION SELECT 1) p1,             .....             (SELECT 0 as n UNION SELECT 1) p15,             ......             (SELECT 0 as n UNION SELECT 1) p20,Tables in my Redshift database:     table_schema |     table_name    --------------+---------------------     sc1          | tbl1     sc1          | tbl2     sc1          | tbl3     sc2          | tbl4     sc2          | tbl5     sc2          | tbl6     sc3          | tbl7     sc3          | tbl8     sc3          | tbl9     public       | new1     public       | new2     public       | new3     public       | my_tokenized_tables     public       | ns     public       | nsaExample Commands:Export all the tables in the schema sc1:    stg=# call unload_pro('s3://datalake/test/','sc1',NULL);    INFO:  [2019-11-22 04:02:49] Unloading... schema = sc1 and table = tbl2    INFO:  UNLOAD completed, 3 record(s) unloaded successfully.    INFO:  [2019-11-22 04:02:52] Unloading... schema = sc1 and table = tbl1    INFO:  UNLOAD completed, 3 record(s) unloaded successfully.    INFO:  [2019-11-22 04:02:52] Unloading... schema = sc1 and table = tbl3    INFO:  UNLOAD completed, 3 record(s) unloaded successfully.    INFO:   Unloading of the DB [stg] is success !!!    CALLExport all the tables in the schema sc3,public:    stg=# call unload_pro('s3://datalake/test/','sc3,public',NULL);    INFO:  [2019-11-22 04:03:23] Unloading... schema = sc3 and table = tbl7    INFO:  UNLOAD completed, 3 record(s) unloaded successfully.    INFO:  [2019-11-22 04:03:23] Unloading... schema = sc3 and table = tbl9    INFO:  UNLOAD completed, 3 record(s) unloaded successfully.    INFO:  [2019-11-22 04:03:23] Unloading... schema = public and table = new2    INFO:  UNLOAD completed, 0 record(s) unloaded successfully.    INFO:  [2019-11-22 04:03:24] Unloading... schema = public and table = my_tokenized_tables    INFO:  UNLOAD completed, 2 record(s) unloaded successfully.    INFO:  [2019-11-22 04:03:24] Unloading... schema = public and table = nsa    INFO:  UNLOAD completed, 0 record(s) unloaded successfully.    INFO:  [2019-11-22 04:03:24] Unloading... schema = sc3 and table = tbl8    INFO:  UNLOAD completed, 3 record(s) unloaded successfully.    INFO:  [2019-11-22 04:03:25] Unloading... schema = public and table = new1    INFO:  UNLOAD completed, 0 record(s) unloaded successfully.    INFO:  [2019-11-22 04:03:25] Unloading... schema = public and table = new3    INFO:  UNLOAD completed, 0 record(s) unloaded successfully.    INFO:  [2019-11-22 04:03:25] Unloading... schema = public and table = ns    INFO:  UNLOAD completed, 0 record(s) unloaded successfully.    INFO:   Unloading of the DB [stg] is success !!!    CALLExport the tables tbl1, tbl2 in the schema sc1:    stg=# call unload_pro('s3://datalake/test/','sc1','tbl1,tbl2');    INFO:  [2019-11-22 04:04:05] Unloading... schema = sc1 and table = tbl1    INFO:  UNLOAD completed, 3 record(s) unloaded successfully.    INFO:  [2019-11-22 04:04:06] Unloading... schema = sc1 and table = tbl2    INFO:  UNLOAD completed, 3 record(s) unloaded successfully.    INFO:   Unloading of the DB [stg] is success !!!    CALLExport the tbl4, tbl5 without specifying any schema name: (but if you have multiple tables in the same name, all tables will be exported )    stg=# call unload_pro('s3://datalake/test/',NULL,'tbl4,tbl5');    INFO:  [2019-11-22 04:04:42] Unloading... schema = sc2 and table = tbl4    INFO:  UNLOAD completed, 3 record(s) unloaded successfully.    INFO:  [2019-11-22 04:04:42] Unloading... schema = sc2 and table = tbl5    INFO:  UNLOAD completed, 3 record(s) unloaded successfully.    INFO:   Unloading of the DB [stg] is success !!!    CALLExport all the tables in all the schemas:    stg=# call unload_pro('s3://datalake/test/',NULL,NULL);    INFO:  [2019-11-22 04:05:23] Unloading... schema = sc1 and table = tbl2    INFO:  UNLOAD completed, 3 record(s) unloaded successfully.    INFO:  [2019-11-22 04:05:23] Unloading... schema = sc2 and table = tbl4    INFO:  UNLOAD completed, 3 record(s) unloaded successfully.    INFO:  [2019-11-22 04:05:24] Unloading... schema = sc2 and table = tbl6    ...................    ...................    INFO:  [2019-11-22 04:05:28] Unloading... schema = public and table = nsa    INFO:  UNLOAD completed, 0 record(s) unloaded successfully.    INFO:   Unloading of the DB [stg] is success !!!    CALL  From the unload_history table you can get the COPY query to load into any other RedShift cluster.  Caution: You need to install this procedure on all the databases to work seamlessly.Reference Links:  Why are we unload with partitions (yyyy/mm/dd) in S3  Unload all the tables to S3.  Use comma separated string in RedShift stored procedure argument.",
            "content_html": "<p>In my <a href=\"https://thedataguy.in/redshift-unload-all-tables-to-s3/\">previous post</a>, I explained how to unload all the tables in the RedShift database to S3 Bucket. But there was a limitation. We should export all the tables, you can’t specify some list of tables or all the tables in a specific schema. Its because of I can’t give comma separated values in RedShift stored procedure. But after spending few days I found a solution for this.</p><h2 id=\"what-well-achieve-here\">What we’ll achieve here?</h2><ul>  <li>Unload many tables to S3.</li>  <li>Unload all the tables in a specific schema.</li>  <li>Unload specific tables in any schema.</li>  <li>All the tables in all the schema.</li></ul><h2 id=\"stored-procedure\">Stored Procedure:</h2><p>You can refer my previous post to understand how it works and the meaning for the variables I used.</p><p><strong>Arguments Used:</strong></p><ul>  <li>s3_path - Location to export the data.</li>  <li>schema_name - Export the tables in this schema.</li>  <li>table_list - List of tables to export.</li></ul><p>To understand all other parameters read my <a href=\"https://thedataguy.in/redshift-unload-all-tables-to-s3/\">previous post</a>.</p><p><strong>Hardcoded Items:</strong></p><p>In the stored procedure, I have hardcoded the follow parameters.</p><ul>  <li>IAM ROLE - <code class=\"language-html highlighter-rouge\">arn:aws:iam::123123123123:role/myredshiftrole</code></li>  <li>Delimiter - <code class=\"language-html highlighter-rouge\">|</code></li></ul><p><strong>You can customize:</strong></p><p>Also, the following Items are hardcoded in the Unload query. You can get these things as variable or hardcoded as per your convenient.</p><ul>  <li>MAXFILESIZE - 100 MB</li>  <li>PARALLEL</li>  <li>ADDQUOTES</li>  <li>HEADER</li>  <li>GZIP</li></ul><p><strong>Create a table for maintain the unload history:</strong></p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">CREATE</span> <span class=\"k\">TABLE</span> <span class=\"n\">unload_history</span><span class=\"p\">(</span><span class=\"n\">pid</span>          <span class=\"nb\">INT</span> <span class=\"k\">IDENTITY</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span><span class=\"n\">u_id</span>         <span class=\"nb\">INT</span><span class=\"p\">,</span><span class=\"n\">u_timestamp</span>  <span class=\"nb\">DATETIME</span><span class=\"p\">,</span><span class=\"n\">start_time</span>   <span class=\"nb\">DATETIME</span><span class=\"p\">,</span><span class=\"n\">end_time</span>     <span class=\"nb\">DATETIME</span><span class=\"p\">,</span><span class=\"n\">db_name</span>      <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">),</span><span class=\"n\">table_schema</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">),</span><span class=\"k\">table_name</span>   <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">),</span><span class=\"n\">export_query</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">65000</span><span class=\"p\">),</span><span class=\"n\">import_query</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">65000</span><span class=\"p\">)</span><span class=\"p\">);</span></code></pre></figure><p><strong>Stored Procedure:</strong></p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\">    <span class=\"k\">CREATE</span> <span class=\"k\">OR</span> <span class=\"k\">replace</span> <span class=\"k\">PROCEDURE</span> <span class=\"n\">unload_pro</span><span class=\"p\">(</span><span class=\"n\">s3_location</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">10000</span><span class=\"p\">),</span>                                                    <span class=\"n\">schema_list</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">),</span>                                                    <span class=\"n\">table_list</span>  <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">))</span>     <span class=\"k\">LANGUAGE</span> <span class=\"n\">plpgsql</span>     <span class=\"k\">AS</span>       <span class=\"err\">$$</span>       <span class=\"k\">DECLARE</span>         <span class=\"n\">list</span> <span class=\"n\">RECORD</span><span class=\"p\">;</span>         <span class=\"n\">db</span>          <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">);</span>         <span class=\"n\">tablename</span>   <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">);</span>         <span class=\"n\">tableschema</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">);</span>         <span class=\"n\">starttime</span>   <span class=\"nb\">datetime</span><span class=\"p\">;</span>         <span class=\"n\">endtime</span>     <span class=\"nb\">datetime</span><span class=\"p\">;</span>         <span class=\"k\">SQL</span> <span class=\"nb\">text</span><span class=\"p\">;</span>         <span class=\"n\">s3_path</span>      <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">);</span>         <span class=\"n\">iamrole</span>      <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">);</span>         <span class=\"k\">delimiter</span>    <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">);</span>         <span class=\"n\">max_filesize</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">);</span>         <span class=\"n\">un_year</span>      <span class=\"nb\">INT</span><span class=\"p\">;</span>         <span class=\"n\">un_month</span>     <span class=\"nb\">INT</span><span class=\"p\">;</span>         <span class=\"n\">un_day</span>       <span class=\"nb\">INT</span><span class=\"p\">;</span>         <span class=\"n\">unload_query</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">65000</span><span class=\"p\">);</span>         <span class=\"n\">copy_query</span>   <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">65000</span><span class=\"p\">);</span>         <span class=\"n\">unload_id</span>    <span class=\"nb\">INT</span><span class=\"p\">;</span>         <span class=\"n\">unload_time</span> <span class=\"nb\">timestamp</span><span class=\"p\">;</span>         <span class=\"n\">sc_name</span>  <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">);</span>         <span class=\"n\">tbl_list</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">);</span>       <span class=\"k\">BEGIN</span>         <span class=\"c1\">-- Pass values for the variables </span>        <span class=\"k\">SELECT</span> <span class=\"k\">extract</span><span class=\"p\">(</span><span class=\"nb\">year</span> <span class=\"k\">FROM</span> <span class=\"n\">getdate</span><span class=\"p\">())</span>         <span class=\"k\">INTO</span>   <span class=\"n\">un_year</span><span class=\"p\">;</span>                  <span class=\"k\">SELECT</span> <span class=\"k\">extract</span><span class=\"p\">(</span><span class=\"k\">month</span> <span class=\"k\">FROM</span> <span class=\"n\">getdate</span><span class=\"p\">())</span>         <span class=\"k\">INTO</span>   <span class=\"n\">un_month</span><span class=\"p\">;</span>                  <span class=\"k\">SELECT</span> <span class=\"k\">extract</span><span class=\"p\">(</span><span class=\"k\">day</span> <span class=\"k\">FROM</span> <span class=\"n\">getdate</span><span class=\"p\">())</span>         <span class=\"k\">INTO</span>   <span class=\"n\">un_day</span><span class=\"p\">;</span>                  <span class=\"k\">SELECT</span> <span class=\"k\">DISTINCT</span><span class=\"p\">(</span><span class=\"n\">table_catalog</span><span class=\"p\">)</span>         <span class=\"k\">FROM</span>            <span class=\"n\">information_schema</span><span class=\"p\">.</span><span class=\"n\">TABLES</span>         <span class=\"k\">INTO</span>            <span class=\"n\">db</span><span class=\"p\">;</span>                  <span class=\"k\">SELECT</span> <span class=\"n\">coalesce</span><span class=\"p\">(</span><span class=\"k\">max</span><span class=\"p\">(</span><span class=\"n\">u_id</span><span class=\"p\">),</span> <span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"o\">+</span><span class=\"mi\">1</span>         <span class=\"k\">FROM</span>   <span class=\"n\">unload_history</span>         <span class=\"k\">INTO</span>   <span class=\"n\">unload_id</span><span class=\"p\">;</span>                  <span class=\"k\">SELECT</span> <span class=\"n\">getdate</span><span class=\"p\">()</span>         <span class=\"k\">INTO</span>   <span class=\"n\">unload_time</span><span class=\"p\">;</span>                  <span class=\"n\">s3_path</span><span class=\"p\">:</span><span class=\"o\">=</span><span class=\"n\">s3_location</span><span class=\"p\">;</span>                 <span class=\"c1\">-- IAM ROLE and the Delimiter is hardcoded here </span>        <span class=\"n\">iamrole</span><span class=\"p\">:</span><span class=\"o\">=</span><span class=\"s1\">'arn:aws:iam::123123123123:role/myredshiftrole'</span><span class=\"p\">;</span>                 <span class=\"k\">delimiter</span><span class=\"p\">:</span><span class=\"o\">=</span><span class=\"s1\">'|'</span><span class=\"p\">;</span>                 <span class=\"n\">IF</span> <span class=\"n\">schema_list</span> <span class=\"k\">IS</span> <span class=\"k\">NULL</span> <span class=\"k\">THEN</span>           <span class=\"k\">DROP</span> <span class=\"k\">TABLE</span> <span class=\"n\">IF</span> <span class=\"k\">EXISTS</span> <span class=\"n\">sp_tmp_schemalist</span><span class=\"p\">;</span>                      <span class=\"k\">CREATE</span> <span class=\"k\">temp</span> <span class=\"k\">TABLE</span> <span class=\"n\">sp_tmp_schemalist</span> <span class=\"p\">(</span><span class=\"n\">sc_list</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">));</span>           <span class=\"k\">INSERT</span> <span class=\"k\">INTO</span> <span class=\"n\">sp_tmp_schemalist</span>           <span class=\"k\">SELECT</span>   <span class=\"n\">nspname</span>           <span class=\"k\">FROM</span>     <span class=\"n\">pg_class</span> <span class=\"n\">pc</span>           <span class=\"k\">join</span>     <span class=\"n\">pg_namespace</span> <span class=\"n\">pn</span>           <span class=\"k\">ON</span>       <span class=\"n\">pc</span><span class=\"p\">.</span><span class=\"n\">relnamespace</span> <span class=\"o\">=</span> <span class=\"n\">pn</span><span class=\"p\">.</span><span class=\"n\">oid</span>           <span class=\"k\">WHERE</span>    <span class=\"n\">nspname</span> <span class=\"k\">NOT</span> <span class=\"k\">IN</span> <span class=\"p\">(</span> <span class=\"s1\">'pg_catalog'</span><span class=\"p\">,</span>                                    <span class=\"s1\">'information_schema'</span><span class=\"p\">,</span>                                    <span class=\"s1\">'pg_toast'</span><span class=\"p\">)</span>           <span class=\"k\">AND</span>      <span class=\"n\">nspname</span> <span class=\"k\">NOT</span> <span class=\"k\">LIKE</span> <span class=\"s1\">'pg_%'</span>           <span class=\"k\">GROUP</span> <span class=\"k\">BY</span> <span class=\"n\">nspname</span><span class=\"p\">;</span>                      <span class=\"k\">SELECT</span>   <span class=\"n\">listagg</span><span class=\"p\">(</span><span class=\"n\">sc_list</span><span class=\"p\">,</span> <span class=\"s1\">','</span><span class=\"p\">)</span> <span class=\"n\">within</span> <span class=\"k\">GROUP</span> <span class=\"p\">(</span><span class=\"k\">ORDER</span> <span class=\"k\">BY</span> <span class=\"n\">sc_list</span><span class=\"p\">)</span>           <span class=\"k\">FROM</span>     <span class=\"n\">sp_tmp_schemalist</span>           <span class=\"k\">INTO</span>     <span class=\"n\">sc_name</span><span class=\"p\">;</span>                  <span class=\"k\">ELSE</span>           <span class=\"n\">sc_name</span><span class=\"p\">:</span><span class=\"o\">=</span><span class=\"n\">schema_list</span><span class=\"p\">;</span>         <span class=\"k\">END</span> <span class=\"n\">IF</span><span class=\"p\">;</span>                 <span class=\"n\">IF</span> <span class=\"n\">table_list</span> <span class=\"k\">IS</span> <span class=\"k\">NULL</span> <span class=\"k\">THEN</span>           <span class=\"k\">DROP</span> <span class=\"k\">TABLE</span> <span class=\"n\">IF</span> <span class=\"k\">EXISTS</span> <span class=\"n\">sp_tmp_tablelist</span><span class=\"p\">;</span>                      <span class=\"k\">CREATE</span> <span class=\"k\">temp</span> <span class=\"k\">TABLE</span> <span class=\"n\">sp_tmp_tablelist</span> <span class=\"p\">(</span><span class=\"n\">tbl_name</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">));</span>           <span class=\"k\">INSERT</span> <span class=\"k\">INTO</span> <span class=\"n\">sp_tmp_tablelist</span>           <span class=\"k\">SELECT</span> <span class=\"n\">relname</span>           <span class=\"k\">FROM</span>   <span class=\"n\">pg_class</span> <span class=\"n\">pc</span>           <span class=\"k\">join</span>   <span class=\"n\">pg_namespace</span> <span class=\"n\">pn</span>           <span class=\"k\">ON</span>     <span class=\"n\">pc</span><span class=\"p\">.</span><span class=\"n\">relnamespace</span> <span class=\"o\">=</span> <span class=\"n\">pn</span><span class=\"p\">.</span><span class=\"n\">oid</span>           <span class=\"k\">WHERE</span>  <span class=\"n\">nspname</span> <span class=\"k\">NOT</span> <span class=\"k\">IN</span> <span class=\"p\">(</span> <span class=\"s1\">'pg_catalog'</span><span class=\"p\">,</span>                                  <span class=\"s1\">'information_schema'</span><span class=\"p\">,</span>                                  <span class=\"s1\">'pg_toast'</span> <span class=\"p\">)</span>           <span class=\"k\">AND</span>    <span class=\"n\">relname</span> <span class=\"k\">NOT</span> <span class=\"k\">LIKE</span> <span class=\"s1\">'sp_tmp_%'</span><span class=\"p\">;</span>                      <span class=\"k\">SELECT</span>   <span class=\"n\">listagg</span><span class=\"p\">(</span><span class=\"n\">tbl_name</span><span class=\"p\">,</span> <span class=\"s1\">','</span><span class=\"p\">)</span> <span class=\"n\">within</span> <span class=\"k\">GROUP</span> <span class=\"p\">(</span><span class=\"k\">ORDER</span> <span class=\"k\">BY</span> <span class=\"n\">tbl_name</span><span class=\"p\">)</span>           <span class=\"k\">FROM</span>     <span class=\"n\">sp_tmp_tablelist</span>           <span class=\"k\">INTO</span>     <span class=\"n\">tbl_list</span><span class=\"p\">;</span>                  <span class=\"k\">ELSE</span>           <span class=\"n\">tbl_list</span><span class=\"p\">:</span><span class=\"o\">=</span><span class=\"n\">table_list</span><span class=\"p\">;</span>         <span class=\"k\">END</span> <span class=\"n\">IF</span><span class=\"p\">;</span>                 <span class=\"k\">DROP</span> <span class=\"k\">TABLE</span> <span class=\"n\">IF</span> <span class=\"k\">EXISTS</span> <span class=\"n\">sp_tmp_quote_schema</span><span class=\"p\">;</span>         <span class=\"k\">DROP</span> <span class=\"k\">TABLE</span> <span class=\"n\">IF</span> <span class=\"k\">EXISTS</span> <span class=\"n\">sp_tmp_quote_table</span><span class=\"p\">;</span>         <span class=\"k\">DROP</span> <span class=\"k\">TABLE</span> <span class=\"n\">IF</span> <span class=\"k\">EXISTS</span> <span class=\"n\">sp_tmp_token_schema</span><span class=\"p\">;</span>         <span class=\"k\">DROP</span> <span class=\"k\">TABLE</span> <span class=\"n\">IF</span> <span class=\"k\">EXISTS</span> <span class=\"n\">sp_tmp_token_table</span><span class=\"p\">;</span>                  <span class=\"k\">CREATE</span> <span class=\"k\">TABLE</span> <span class=\"n\">sp_tmp_quote_schema</span>                      <span class=\"p\">(</span>                                   <span class=\"n\">comma_quote_schema</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">400</span><span class=\"p\">)</span>                      <span class=\"p\">);</span>                  <span class=\"k\">CREATE</span> <span class=\"k\">TABLE</span> <span class=\"n\">sp_tmp_quote_table</span>                      <span class=\"p\">(</span>                                   <span class=\"n\">comma_quote_table</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">400</span><span class=\"p\">)</span>                      <span class=\"p\">);</span>                          <span class=\"k\">EXECUTE</span> <span class=\"s1\">'INSERT INTO sp_tmp_quote_schema VALUES ('</span><span class=\"o\">||</span> <span class=\"n\">quote_literal</span><span class=\"p\">(</span><span class=\"n\">sc_name</span><span class=\"p\">)</span><span class=\"o\">||</span> <span class=\"s1\">')'</span><span class=\"p\">;</span>         <span class=\"k\">EXECUTE</span> <span class=\"s1\">'INSERT INTO sp_tmp_quote_table VALUES ('</span><span class=\"o\">||</span> <span class=\"n\">quote_literal</span><span class=\"p\">(</span><span class=\"n\">tbl_list</span><span class=\"p\">)</span><span class=\"o\">||</span> <span class=\"s1\">')'</span><span class=\"p\">;</span>                 <span class=\"k\">DROP</span> <span class=\"k\">TABLE</span> <span class=\"n\">IF</span> <span class=\"k\">EXISTS</span> <span class=\"n\">sp_tmp_ns</span><span class=\"p\">;</span>         <span class=\"k\">CREATE</span> <span class=\"k\">temp</span> <span class=\"k\">TABLE</span> <span class=\"n\">sp_tmp_ns</span> <span class=\"p\">(</span><span class=\"n\">n</span> <span class=\"nb\">INT</span><span class=\"p\">);</span>                 <span class=\"k\">INSERT</span> <span class=\"k\">INTO</span> <span class=\"n\">sp_tmp_ns</span>         <span class=\"k\">SELECT</span>           <span class=\"n\">ROW_NUMBER</span><span class=\"p\">()</span> <span class=\"n\">OVER</span> <span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">n</span>        <span class=\"k\">FROM</span>              <span class=\"p\">(</span><span class=\"k\">SELECT</span> <span class=\"mi\">0</span> <span class=\"k\">as</span> <span class=\"n\">n</span> <span class=\"k\">UNION</span> <span class=\"k\">SELECT</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"n\">p0</span><span class=\"p\">,</span>             <span class=\"p\">(</span><span class=\"k\">SELECT</span> <span class=\"mi\">0</span> <span class=\"k\">as</span> <span class=\"n\">n</span> <span class=\"k\">UNION</span> <span class=\"k\">SELECT</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"n\">p1</span><span class=\"p\">,</span>             <span class=\"p\">(</span><span class=\"k\">SELECT</span> <span class=\"mi\">0</span> <span class=\"k\">as</span> <span class=\"n\">n</span> <span class=\"k\">UNION</span> <span class=\"k\">SELECT</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"n\">p2</span><span class=\"p\">,</span>             <span class=\"p\">(</span><span class=\"k\">SELECT</span> <span class=\"mi\">0</span> <span class=\"k\">as</span> <span class=\"n\">n</span> <span class=\"k\">UNION</span> <span class=\"k\">SELECT</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"n\">p3</span><span class=\"p\">,</span>             <span class=\"p\">(</span><span class=\"k\">SELECT</span> <span class=\"mi\">0</span> <span class=\"k\">as</span> <span class=\"n\">n</span> <span class=\"k\">UNION</span> <span class=\"k\">SELECT</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"n\">p4</span><span class=\"p\">,</span>             <span class=\"p\">(</span><span class=\"k\">SELECT</span> <span class=\"mi\">0</span> <span class=\"k\">as</span> <span class=\"n\">n</span> <span class=\"k\">UNION</span> <span class=\"k\">SELECT</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"n\">p5</span><span class=\"p\">,</span>             <span class=\"p\">(</span><span class=\"k\">SELECT</span> <span class=\"mi\">0</span> <span class=\"k\">as</span> <span class=\"n\">n</span> <span class=\"k\">UNION</span> <span class=\"k\">SELECT</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"n\">p6</span><span class=\"p\">,</span>             <span class=\"p\">(</span><span class=\"k\">SELECT</span> <span class=\"mi\">0</span> <span class=\"k\">as</span> <span class=\"n\">n</span> <span class=\"k\">UNION</span> <span class=\"k\">SELECT</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"n\">p7</span><span class=\"p\">,</span>             <span class=\"p\">(</span><span class=\"k\">SELECT</span> <span class=\"mi\">0</span> <span class=\"k\">as</span> <span class=\"n\">n</span> <span class=\"k\">UNION</span> <span class=\"k\">SELECT</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"n\">p8</span><span class=\"p\">,</span>             <span class=\"p\">(</span><span class=\"k\">SELECT</span> <span class=\"mi\">0</span> <span class=\"k\">as</span> <span class=\"n\">n</span> <span class=\"k\">UNION</span> <span class=\"k\">SELECT</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"n\">p9</span><span class=\"p\">,</span>             <span class=\"p\">(</span><span class=\"k\">SELECT</span> <span class=\"mi\">0</span> <span class=\"k\">as</span> <span class=\"n\">n</span> <span class=\"k\">UNION</span> <span class=\"k\">SELECT</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"n\">p10</span><span class=\"p\">;</span>                 <span class=\"k\">SELECT</span>     <span class=\"k\">trim</span><span class=\"p\">(</span><span class=\"n\">split_part</span><span class=\"p\">(</span><span class=\"n\">b</span><span class=\"p\">.</span><span class=\"n\">comma_quote_schema</span><span class=\"p\">,</span> <span class=\"s1\">','</span><span class=\"p\">,</span> <span class=\"n\">ns</span><span class=\"p\">.</span><span class=\"n\">n</span><span class=\"p\">))</span> <span class=\"k\">AS</span> <span class=\"n\">sname</span>         <span class=\"k\">INTO</span>       <span class=\"k\">TABLE</span> <span class=\"n\">sp_tmp_token_schema</span>         <span class=\"k\">FROM</span>       <span class=\"n\">sp_tmp_ns</span> <span class=\"n\">ns</span>         <span class=\"k\">inner</span> <span class=\"k\">join</span> <span class=\"n\">sp_tmp_quote_schema</span> <span class=\"n\">b</span>         <span class=\"k\">ON</span>         <span class=\"n\">ns</span><span class=\"p\">.</span><span class=\"n\">n</span> <span class=\"o\">&lt;=</span> <span class=\"n\">regexp_count</span><span class=\"p\">(</span><span class=\"n\">b</span><span class=\"p\">.</span><span class=\"n\">comma_quote_schema</span><span class=\"p\">,</span> <span class=\"s1\">','</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">;</span>                  <span class=\"k\">SELECT</span>     <span class=\"k\">trim</span><span class=\"p\">(</span><span class=\"n\">split_part</span><span class=\"p\">(</span><span class=\"n\">b</span><span class=\"p\">.</span><span class=\"n\">comma_quote_table</span><span class=\"p\">,</span> <span class=\"s1\">','</span><span class=\"p\">,</span> <span class=\"n\">ns</span><span class=\"p\">.</span><span class=\"n\">n</span><span class=\"p\">))</span> <span class=\"k\">AS</span> <span class=\"n\">tname</span>         <span class=\"k\">INTO</span>       <span class=\"k\">TABLE</span> <span class=\"n\">sp_tmp_token_table</span>         <span class=\"k\">FROM</span>       <span class=\"n\">sp_tmp_ns</span> <span class=\"n\">ns</span>         <span class=\"k\">inner</span> <span class=\"k\">join</span> <span class=\"n\">sp_tmp_quote_table</span> <span class=\"n\">b</span>         <span class=\"k\">ON</span>         <span class=\"n\">ns</span><span class=\"p\">.</span><span class=\"n\">n</span> <span class=\"o\">&lt;=</span> <span class=\"n\">regexp_count</span><span class=\"p\">(</span><span class=\"n\">b</span><span class=\"p\">.</span><span class=\"n\">comma_quote_table</span><span class=\"p\">,</span> <span class=\"s1\">','</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">;</span>                  <span class=\"k\">FOR</span> <span class=\"n\">list</span> <span class=\"k\">IN</span>         <span class=\"k\">SELECT</span> <span class=\"n\">nspname</span> <span class=\"p\">::</span> <span class=\"nb\">text</span> <span class=\"k\">AS</span> <span class=\"n\">table_schema</span><span class=\"p\">,</span>                <span class=\"n\">relname</span> <span class=\"p\">::</span> <span class=\"nb\">text</span> <span class=\"k\">AS</span> <span class=\"k\">table_name</span>         <span class=\"k\">FROM</span>   <span class=\"n\">pg_class</span> <span class=\"n\">pc</span>         <span class=\"k\">join</span>   <span class=\"n\">pg_namespace</span> <span class=\"n\">pn</span>         <span class=\"k\">ON</span>     <span class=\"n\">pc</span><span class=\"p\">.</span><span class=\"n\">relnamespace</span> <span class=\"o\">=</span> <span class=\"n\">pn</span><span class=\"p\">.</span><span class=\"n\">oid</span>         <span class=\"k\">WHERE</span>  <span class=\"k\">trim</span><span class=\"p\">(</span><span class=\"n\">relname</span> <span class=\"p\">::</span> <span class=\"nb\">text</span><span class=\"p\">)</span> <span class=\"k\">IN</span>                <span class=\"p\">(</span>                       <span class=\"k\">SELECT</span> <span class=\"n\">tname</span>                       <span class=\"k\">FROM</span>   <span class=\"n\">sp_tmp_token_table</span><span class=\"p\">)</span>         <span class=\"k\">AND</span>    <span class=\"n\">relname</span> <span class=\"o\">!=</span><span class=\"s1\">'unload_history'</span>         <span class=\"k\">AND</span>    <span class=\"k\">trim</span><span class=\"p\">(</span><span class=\"n\">nspname</span> <span class=\"p\">::</span> <span class=\"nb\">text</span><span class=\"p\">)</span> <span class=\"k\">IN</span>                <span class=\"p\">(</span>                       <span class=\"k\">SELECT</span> <span class=\"n\">sname</span>                       <span class=\"k\">FROM</span>   <span class=\"n\">sp_tmp_token_schema</span><span class=\"p\">)</span> <span class=\"n\">LOOP</span>         <span class=\"k\">SELECT</span> <span class=\"n\">getdate</span><span class=\"p\">()</span>         <span class=\"k\">INTO</span>   <span class=\"n\">starttime</span><span class=\"p\">;</span>                  <span class=\"k\">SQL</span><span class=\"p\">:</span><span class=\"o\">=</span><span class=\"s1\">'select * from '</span>         <span class=\"o\">||</span><span class=\"n\">list</span><span class=\"p\">.</span><span class=\"n\">table_schema</span>         <span class=\"o\">||</span><span class=\"s1\">'.'</span>         <span class=\"o\">||</span><span class=\"n\">list</span><span class=\"p\">.</span><span class=\"k\">table_name</span>         <span class=\"o\">||</span><span class=\"s1\">''</span> <span class=\"p\">;</span>         <span class=\"n\">RAISE</span> <span class=\"n\">info</span> <span class=\"s1\">'[%] Unloading... schema = % and table = %'</span><span class=\"p\">,</span><span class=\"n\">starttime</span><span class=\"p\">,</span> <span class=\"n\">list</span><span class=\"p\">.</span><span class=\"n\">table_schema</span><span class=\"p\">,</span> <span class=\"n\">list</span><span class=\"p\">.</span><span class=\"k\">table_name</span><span class=\"p\">;</span>        <span class=\"c1\">-- Start unloading the data </span>        <span class=\"n\">unload_query</span> <span class=\"p\">:</span><span class=\"o\">=</span> <span class=\"s1\">'unload (</span><span class=\"se\">''</span><span class=\"s1\">'</span>         <span class=\"o\">||</span><span class=\"k\">SQL</span>         <span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">''</span><span class=\"s1\">) to </span><span class=\"se\">''</span><span class=\"s1\">'</span>         <span class=\"o\">||</span><span class=\"n\">s3_path</span>         <span class=\"o\">||</span><span class=\"n\">un_year</span>         <span class=\"o\">||</span><span class=\"s1\">'/'</span>         <span class=\"o\">||</span><span class=\"n\">un_month</span>         <span class=\"o\">||</span><span class=\"s1\">'/'</span>         <span class=\"o\">||</span><span class=\"n\">un_day</span>         <span class=\"o\">||</span><span class=\"s1\">'/'</span>         <span class=\"o\">||</span><span class=\"n\">db</span>         <span class=\"o\">||</span><span class=\"s1\">'/'</span>         <span class=\"o\">||</span><span class=\"n\">list</span><span class=\"p\">.</span><span class=\"n\">table_schema</span>         <span class=\"o\">||</span><span class=\"s1\">'/'</span>         <span class=\"o\">||</span><span class=\"n\">list</span><span class=\"p\">.</span><span class=\"k\">table_name</span>         <span class=\"o\">||</span><span class=\"s1\">'/'</span>         <span class=\"o\">||</span><span class=\"n\">list</span><span class=\"p\">.</span><span class=\"n\">table_schema</span>         <span class=\"o\">||</span><span class=\"s1\">'-'</span>         <span class=\"o\">||</span><span class=\"n\">list</span><span class=\"p\">.</span><span class=\"k\">table_name</span>         <span class=\"o\">||</span><span class=\"s1\">'_</span><span class=\"se\">''</span><span class=\"s1\"> iam_role </span><span class=\"se\">''</span><span class=\"s1\">'</span>         <span class=\"o\">||</span><span class=\"n\">iamrole</span>         <span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">''</span><span class=\"s1\"> delimiter </span><span class=\"se\">''</span><span class=\"s1\">'</span>         <span class=\"o\">||</span><span class=\"k\">delimiter</span>         <span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">''</span><span class=\"s1\"> MAXFILESIZE 300 MB PARALLEL ADDQUOTES HEADER GZIP'</span><span class=\"p\">;</span>                 <span class=\"k\">EXECUTE</span> <span class=\"n\">unload_query</span><span class=\"p\">;</span>                 <span class=\"n\">copy_query</span> <span class=\"p\">:</span><span class=\"o\">=</span> <span class=\"s1\">'copy '</span>         <span class=\"o\">||</span><span class=\"n\">list</span><span class=\"p\">.</span><span class=\"n\">table_schema</span>         <span class=\"o\">||</span><span class=\"s1\">'.'</span>         <span class=\"o\">||</span><span class=\"n\">list</span><span class=\"p\">.</span><span class=\"k\">table_name</span>         <span class=\"o\">||</span><span class=\"s1\">' from </span><span class=\"se\">''</span><span class=\"s1\">'</span>         <span class=\"o\">||</span><span class=\"n\">s3_path</span>         <span class=\"o\">||</span><span class=\"n\">un_year</span>         <span class=\"o\">||</span><span class=\"s1\">'/'</span>         <span class=\"o\">||</span><span class=\"n\">un_month</span>         <span class=\"o\">||</span><span class=\"s1\">'/'</span>         <span class=\"o\">||</span><span class=\"n\">un_day</span>         <span class=\"o\">||</span><span class=\"s1\">'/'</span>         <span class=\"o\">||</span><span class=\"n\">db</span>         <span class=\"o\">||</span><span class=\"s1\">'/'</span>         <span class=\"o\">||</span><span class=\"n\">list</span><span class=\"p\">.</span><span class=\"n\">table_schema</span>         <span class=\"o\">||</span><span class=\"s1\">'/'</span>         <span class=\"o\">||</span><span class=\"n\">list</span><span class=\"p\">.</span><span class=\"k\">table_name</span>         <span class=\"o\">||</span><span class=\"s1\">'/</span><span class=\"se\">''</span><span class=\"s1\"> iam_role </span><span class=\"se\">''</span><span class=\"s1\">'</span>         <span class=\"o\">||</span><span class=\"n\">iamrole</span>         <span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">''</span><span class=\"s1\"> delimiter </span><span class=\"se\">''</span><span class=\"s1\">'</span>         <span class=\"o\">||</span><span class=\"k\">delimiter</span>         <span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">''</span><span class=\"s1\"> IGNOREHEADER 1 REMOVEQUOTES gzip'</span><span class=\"p\">;</span>                 <span class=\"k\">SELECT</span> <span class=\"n\">getdate</span><span class=\"p\">()</span>         <span class=\"k\">INTO</span>   <span class=\"n\">endtime</span><span class=\"p\">;</span>                  <span class=\"k\">SELECT</span> <span class=\"n\">list</span><span class=\"p\">.</span><span class=\"n\">table_schema</span>         <span class=\"k\">INTO</span>   <span class=\"n\">tableschema</span><span class=\"p\">;</span>                  <span class=\"k\">SELECT</span> <span class=\"n\">list</span><span class=\"p\">.</span><span class=\"k\">table_name</span>         <span class=\"k\">INTO</span>   <span class=\"n\">tablename</span><span class=\"p\">;</span>                  <span class=\"c1\">-- Insert into the history table </span>        <span class=\"k\">INSERT</span> <span class=\"k\">INTO</span> <span class=\"n\">unload_history</span>                     <span class=\"p\">(</span>                                 <span class=\"n\">u_id</span><span class=\"p\">,</span>                                 <span class=\"n\">u_timestamp</span><span class=\"p\">,</span>                                 <span class=\"n\">start_time</span><span class=\"p\">,</span>                                 <span class=\"n\">end_time</span><span class=\"p\">,</span>                                 <span class=\"n\">db_name</span><span class=\"p\">,</span>                                 <span class=\"n\">table_schema</span><span class=\"p\">,</span>                                 <span class=\"k\">table_name</span><span class=\"p\">,</span>                                 <span class=\"n\">export_query</span><span class=\"p\">,</span>                                 <span class=\"n\">import_query</span>                     <span class=\"p\">)</span>                     <span class=\"k\">VALUES</span>                     <span class=\"p\">(</span>                                 <span class=\"n\">unload_id</span><span class=\"p\">,</span>                                 <span class=\"n\">unload_time</span><span class=\"p\">,</span>                                 <span class=\"n\">starttime</span><span class=\"p\">,</span>                                 <span class=\"n\">endtime</span><span class=\"p\">,</span>                                 <span class=\"n\">db</span><span class=\"p\">,</span>                                 <span class=\"n\">tableschema</span><span class=\"p\">,</span>                                 <span class=\"n\">tablename</span><span class=\"p\">,</span>                                 <span class=\"n\">unload_query</span><span class=\"p\">,</span>                                 <span class=\"n\">copy_query</span>                     <span class=\"p\">);</span>              <span class=\"k\">END</span> <span class=\"n\">LOOP</span><span class=\"p\">;</span>       <span class=\"n\">RAISE</span> <span class=\"n\">info</span> <span class=\"s1\">' Unloading of the DB [%] is success !!!'</span> <span class=\"p\">,</span><span class=\"n\">db</span><span class=\"p\">;</span>     <span class=\"k\">END</span><span class=\"p\">;</span>     <span class=\"err\">$$</span><span class=\"p\">;</span></code></pre></figure><p>I have less than 2048 tables, if you have more than that, just add few more <code class=\"language-html highlighter-rouge\">select unions</code> in the below portion.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\">    <span class=\"k\">SELECT</span>           <span class=\"n\">ROW_NUMBER</span><span class=\"p\">()</span> <span class=\"n\">OVER</span> <span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">n</span>        <span class=\"k\">FROM</span>              <span class=\"p\">(</span><span class=\"k\">SELECT</span> <span class=\"mi\">0</span> <span class=\"k\">as</span> <span class=\"n\">n</span> <span class=\"k\">UNION</span> <span class=\"k\">SELECT</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"n\">p0</span><span class=\"p\">,</span>             <span class=\"p\">(</span><span class=\"k\">SELECT</span> <span class=\"mi\">0</span> <span class=\"k\">as</span> <span class=\"n\">n</span> <span class=\"k\">UNION</span> <span class=\"k\">SELECT</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"n\">p1</span><span class=\"p\">,</span>             <span class=\"p\">.....</span>             <span class=\"p\">(</span><span class=\"k\">SELECT</span> <span class=\"mi\">0</span> <span class=\"k\">as</span> <span class=\"n\">n</span> <span class=\"k\">UNION</span> <span class=\"k\">SELECT</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"n\">p15</span><span class=\"p\">,</span>             <span class=\"p\">......</span>             <span class=\"p\">(</span><span class=\"k\">SELECT</span> <span class=\"mi\">0</span> <span class=\"k\">as</span> <span class=\"n\">n</span> <span class=\"k\">UNION</span> <span class=\"k\">SELECT</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"n\">p20</span><span class=\"p\">,</span></code></pre></figure><h2 id=\"tables-in-my-redshift-database\">Tables in my Redshift database:</h2><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\">     <span class=\"n\">table_schema</span> <span class=\"o\">|</span>     <span class=\"k\">table_name</span>    <span class=\"c1\">--------------+---------------------</span>     <span class=\"n\">sc1</span>          <span class=\"o\">|</span> <span class=\"n\">tbl1</span>     <span class=\"n\">sc1</span>          <span class=\"o\">|</span> <span class=\"n\">tbl2</span>     <span class=\"n\">sc1</span>          <span class=\"o\">|</span> <span class=\"n\">tbl3</span>     <span class=\"n\">sc2</span>          <span class=\"o\">|</span> <span class=\"n\">tbl4</span>     <span class=\"n\">sc2</span>          <span class=\"o\">|</span> <span class=\"n\">tbl5</span>     <span class=\"n\">sc2</span>          <span class=\"o\">|</span> <span class=\"n\">tbl6</span>     <span class=\"n\">sc3</span>          <span class=\"o\">|</span> <span class=\"n\">tbl7</span>     <span class=\"n\">sc3</span>          <span class=\"o\">|</span> <span class=\"n\">tbl8</span>     <span class=\"n\">sc3</span>          <span class=\"o\">|</span> <span class=\"n\">tbl9</span>     <span class=\"k\">public</span>       <span class=\"o\">|</span> <span class=\"n\">new1</span>     <span class=\"k\">public</span>       <span class=\"o\">|</span> <span class=\"n\">new2</span>     <span class=\"k\">public</span>       <span class=\"o\">|</span> <span class=\"n\">new3</span>     <span class=\"k\">public</span>       <span class=\"o\">|</span> <span class=\"n\">my_tokenized_tables</span>     <span class=\"k\">public</span>       <span class=\"o\">|</span> <span class=\"n\">ns</span>     <span class=\"k\">public</span>       <span class=\"o\">|</span> <span class=\"n\">nsa</span></code></pre></figure><h2 id=\"example-commands\">Example Commands:</h2><p><strong>Export all the tables in the schema sc1:</strong></p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\">    <span class=\"n\">stg</span><span class=\"o\">=#</span> <span class=\"k\">call</span> <span class=\"n\">unload_pro</span><span class=\"p\">(</span><span class=\"s1\">'s3://datalake/test/'</span><span class=\"p\">,</span><span class=\"s1\">'sc1'</span><span class=\"p\">,</span><span class=\"k\">NULL</span><span class=\"p\">);</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"p\">[</span><span class=\"mi\">2019</span><span class=\"o\">-</span><span class=\"mi\">11</span><span class=\"o\">-</span><span class=\"mi\">22</span> <span class=\"mi\">04</span><span class=\"p\">:</span><span class=\"mi\">02</span><span class=\"p\">:</span><span class=\"mi\">49</span><span class=\"p\">]</span> <span class=\"n\">Unloading</span><span class=\"p\">...</span> <span class=\"k\">schema</span> <span class=\"o\">=</span> <span class=\"n\">sc1</span> <span class=\"k\">and</span> <span class=\"k\">table</span> <span class=\"o\">=</span> <span class=\"n\">tbl2</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"n\">UNLOAD</span> <span class=\"n\">completed</span><span class=\"p\">,</span> <span class=\"mi\">3</span> <span class=\"n\">record</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">)</span> <span class=\"n\">unloaded</span> <span class=\"n\">successfully</span><span class=\"p\">.</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"p\">[</span><span class=\"mi\">2019</span><span class=\"o\">-</span><span class=\"mi\">11</span><span class=\"o\">-</span><span class=\"mi\">22</span> <span class=\"mi\">04</span><span class=\"p\">:</span><span class=\"mi\">02</span><span class=\"p\">:</span><span class=\"mi\">52</span><span class=\"p\">]</span> <span class=\"n\">Unloading</span><span class=\"p\">...</span> <span class=\"k\">schema</span> <span class=\"o\">=</span> <span class=\"n\">sc1</span> <span class=\"k\">and</span> <span class=\"k\">table</span> <span class=\"o\">=</span> <span class=\"n\">tbl1</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"n\">UNLOAD</span> <span class=\"n\">completed</span><span class=\"p\">,</span> <span class=\"mi\">3</span> <span class=\"n\">record</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">)</span> <span class=\"n\">unloaded</span> <span class=\"n\">successfully</span><span class=\"p\">.</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"p\">[</span><span class=\"mi\">2019</span><span class=\"o\">-</span><span class=\"mi\">11</span><span class=\"o\">-</span><span class=\"mi\">22</span> <span class=\"mi\">04</span><span class=\"p\">:</span><span class=\"mi\">02</span><span class=\"p\">:</span><span class=\"mi\">52</span><span class=\"p\">]</span> <span class=\"n\">Unloading</span><span class=\"p\">...</span> <span class=\"k\">schema</span> <span class=\"o\">=</span> <span class=\"n\">sc1</span> <span class=\"k\">and</span> <span class=\"k\">table</span> <span class=\"o\">=</span> <span class=\"n\">tbl3</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"n\">UNLOAD</span> <span class=\"n\">completed</span><span class=\"p\">,</span> <span class=\"mi\">3</span> <span class=\"n\">record</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">)</span> <span class=\"n\">unloaded</span> <span class=\"n\">successfully</span><span class=\"p\">.</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>   <span class=\"n\">Unloading</span> <span class=\"k\">of</span> <span class=\"n\">the</span> <span class=\"n\">DB</span> <span class=\"p\">[</span><span class=\"n\">stg</span><span class=\"p\">]</span> <span class=\"k\">is</span> <span class=\"n\">success</span> <span class=\"o\">!!!</span>    <span class=\"k\">CALL</span></code></pre></figure><p><strong>Export all the tables in the schema sc3,public:</strong></p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\">    <span class=\"n\">stg</span><span class=\"o\">=#</span> <span class=\"k\">call</span> <span class=\"n\">unload_pro</span><span class=\"p\">(</span><span class=\"s1\">'s3://datalake/test/'</span><span class=\"p\">,</span><span class=\"s1\">'sc3,public'</span><span class=\"p\">,</span><span class=\"k\">NULL</span><span class=\"p\">);</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"p\">[</span><span class=\"mi\">2019</span><span class=\"o\">-</span><span class=\"mi\">11</span><span class=\"o\">-</span><span class=\"mi\">22</span> <span class=\"mi\">04</span><span class=\"p\">:</span><span class=\"mi\">03</span><span class=\"p\">:</span><span class=\"mi\">23</span><span class=\"p\">]</span> <span class=\"n\">Unloading</span><span class=\"p\">...</span> <span class=\"k\">schema</span> <span class=\"o\">=</span> <span class=\"n\">sc3</span> <span class=\"k\">and</span> <span class=\"k\">table</span> <span class=\"o\">=</span> <span class=\"n\">tbl7</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"n\">UNLOAD</span> <span class=\"n\">completed</span><span class=\"p\">,</span> <span class=\"mi\">3</span> <span class=\"n\">record</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">)</span> <span class=\"n\">unloaded</span> <span class=\"n\">successfully</span><span class=\"p\">.</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"p\">[</span><span class=\"mi\">2019</span><span class=\"o\">-</span><span class=\"mi\">11</span><span class=\"o\">-</span><span class=\"mi\">22</span> <span class=\"mi\">04</span><span class=\"p\">:</span><span class=\"mi\">03</span><span class=\"p\">:</span><span class=\"mi\">23</span><span class=\"p\">]</span> <span class=\"n\">Unloading</span><span class=\"p\">...</span> <span class=\"k\">schema</span> <span class=\"o\">=</span> <span class=\"n\">sc3</span> <span class=\"k\">and</span> <span class=\"k\">table</span> <span class=\"o\">=</span> <span class=\"n\">tbl9</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"n\">UNLOAD</span> <span class=\"n\">completed</span><span class=\"p\">,</span> <span class=\"mi\">3</span> <span class=\"n\">record</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">)</span> <span class=\"n\">unloaded</span> <span class=\"n\">successfully</span><span class=\"p\">.</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"p\">[</span><span class=\"mi\">2019</span><span class=\"o\">-</span><span class=\"mi\">11</span><span class=\"o\">-</span><span class=\"mi\">22</span> <span class=\"mi\">04</span><span class=\"p\">:</span><span class=\"mi\">03</span><span class=\"p\">:</span><span class=\"mi\">23</span><span class=\"p\">]</span> <span class=\"n\">Unloading</span><span class=\"p\">...</span> <span class=\"k\">schema</span> <span class=\"o\">=</span> <span class=\"k\">public</span> <span class=\"k\">and</span> <span class=\"k\">table</span> <span class=\"o\">=</span> <span class=\"n\">new2</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"n\">UNLOAD</span> <span class=\"n\">completed</span><span class=\"p\">,</span> <span class=\"mi\">0</span> <span class=\"n\">record</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">)</span> <span class=\"n\">unloaded</span> <span class=\"n\">successfully</span><span class=\"p\">.</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"p\">[</span><span class=\"mi\">2019</span><span class=\"o\">-</span><span class=\"mi\">11</span><span class=\"o\">-</span><span class=\"mi\">22</span> <span class=\"mi\">04</span><span class=\"p\">:</span><span class=\"mi\">03</span><span class=\"p\">:</span><span class=\"mi\">24</span><span class=\"p\">]</span> <span class=\"n\">Unloading</span><span class=\"p\">...</span> <span class=\"k\">schema</span> <span class=\"o\">=</span> <span class=\"k\">public</span> <span class=\"k\">and</span> <span class=\"k\">table</span> <span class=\"o\">=</span> <span class=\"n\">my_tokenized_tables</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"n\">UNLOAD</span> <span class=\"n\">completed</span><span class=\"p\">,</span> <span class=\"mi\">2</span> <span class=\"n\">record</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">)</span> <span class=\"n\">unloaded</span> <span class=\"n\">successfully</span><span class=\"p\">.</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"p\">[</span><span class=\"mi\">2019</span><span class=\"o\">-</span><span class=\"mi\">11</span><span class=\"o\">-</span><span class=\"mi\">22</span> <span class=\"mi\">04</span><span class=\"p\">:</span><span class=\"mi\">03</span><span class=\"p\">:</span><span class=\"mi\">24</span><span class=\"p\">]</span> <span class=\"n\">Unloading</span><span class=\"p\">...</span> <span class=\"k\">schema</span> <span class=\"o\">=</span> <span class=\"k\">public</span> <span class=\"k\">and</span> <span class=\"k\">table</span> <span class=\"o\">=</span> <span class=\"n\">nsa</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"n\">UNLOAD</span> <span class=\"n\">completed</span><span class=\"p\">,</span> <span class=\"mi\">0</span> <span class=\"n\">record</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">)</span> <span class=\"n\">unloaded</span> <span class=\"n\">successfully</span><span class=\"p\">.</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"p\">[</span><span class=\"mi\">2019</span><span class=\"o\">-</span><span class=\"mi\">11</span><span class=\"o\">-</span><span class=\"mi\">22</span> <span class=\"mi\">04</span><span class=\"p\">:</span><span class=\"mi\">03</span><span class=\"p\">:</span><span class=\"mi\">24</span><span class=\"p\">]</span> <span class=\"n\">Unloading</span><span class=\"p\">...</span> <span class=\"k\">schema</span> <span class=\"o\">=</span> <span class=\"n\">sc3</span> <span class=\"k\">and</span> <span class=\"k\">table</span> <span class=\"o\">=</span> <span class=\"n\">tbl8</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"n\">UNLOAD</span> <span class=\"n\">completed</span><span class=\"p\">,</span> <span class=\"mi\">3</span> <span class=\"n\">record</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">)</span> <span class=\"n\">unloaded</span> <span class=\"n\">successfully</span><span class=\"p\">.</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"p\">[</span><span class=\"mi\">2019</span><span class=\"o\">-</span><span class=\"mi\">11</span><span class=\"o\">-</span><span class=\"mi\">22</span> <span class=\"mi\">04</span><span class=\"p\">:</span><span class=\"mi\">03</span><span class=\"p\">:</span><span class=\"mi\">25</span><span class=\"p\">]</span> <span class=\"n\">Unloading</span><span class=\"p\">...</span> <span class=\"k\">schema</span> <span class=\"o\">=</span> <span class=\"k\">public</span> <span class=\"k\">and</span> <span class=\"k\">table</span> <span class=\"o\">=</span> <span class=\"n\">new1</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"n\">UNLOAD</span> <span class=\"n\">completed</span><span class=\"p\">,</span> <span class=\"mi\">0</span> <span class=\"n\">record</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">)</span> <span class=\"n\">unloaded</span> <span class=\"n\">successfully</span><span class=\"p\">.</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"p\">[</span><span class=\"mi\">2019</span><span class=\"o\">-</span><span class=\"mi\">11</span><span class=\"o\">-</span><span class=\"mi\">22</span> <span class=\"mi\">04</span><span class=\"p\">:</span><span class=\"mi\">03</span><span class=\"p\">:</span><span class=\"mi\">25</span><span class=\"p\">]</span> <span class=\"n\">Unloading</span><span class=\"p\">...</span> <span class=\"k\">schema</span> <span class=\"o\">=</span> <span class=\"k\">public</span> <span class=\"k\">and</span> <span class=\"k\">table</span> <span class=\"o\">=</span> <span class=\"n\">new3</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"n\">UNLOAD</span> <span class=\"n\">completed</span><span class=\"p\">,</span> <span class=\"mi\">0</span> <span class=\"n\">record</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">)</span> <span class=\"n\">unloaded</span> <span class=\"n\">successfully</span><span class=\"p\">.</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"p\">[</span><span class=\"mi\">2019</span><span class=\"o\">-</span><span class=\"mi\">11</span><span class=\"o\">-</span><span class=\"mi\">22</span> <span class=\"mi\">04</span><span class=\"p\">:</span><span class=\"mi\">03</span><span class=\"p\">:</span><span class=\"mi\">25</span><span class=\"p\">]</span> <span class=\"n\">Unloading</span><span class=\"p\">...</span> <span class=\"k\">schema</span> <span class=\"o\">=</span> <span class=\"k\">public</span> <span class=\"k\">and</span> <span class=\"k\">table</span> <span class=\"o\">=</span> <span class=\"n\">ns</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"n\">UNLOAD</span> <span class=\"n\">completed</span><span class=\"p\">,</span> <span class=\"mi\">0</span> <span class=\"n\">record</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">)</span> <span class=\"n\">unloaded</span> <span class=\"n\">successfully</span><span class=\"p\">.</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>   <span class=\"n\">Unloading</span> <span class=\"k\">of</span> <span class=\"n\">the</span> <span class=\"n\">DB</span> <span class=\"p\">[</span><span class=\"n\">stg</span><span class=\"p\">]</span> <span class=\"k\">is</span> <span class=\"n\">success</span> <span class=\"o\">!!!</span>    <span class=\"k\">CALL</span></code></pre></figure><p><strong>Export the tables tbl1, tbl2 in the schema sc1:</strong></p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\">    <span class=\"n\">stg</span><span class=\"o\">=#</span> <span class=\"k\">call</span> <span class=\"n\">unload_pro</span><span class=\"p\">(</span><span class=\"s1\">'s3://datalake/test/'</span><span class=\"p\">,</span><span class=\"s1\">'sc1'</span><span class=\"p\">,</span><span class=\"s1\">'tbl1,tbl2'</span><span class=\"p\">);</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"p\">[</span><span class=\"mi\">2019</span><span class=\"o\">-</span><span class=\"mi\">11</span><span class=\"o\">-</span><span class=\"mi\">22</span> <span class=\"mi\">04</span><span class=\"p\">:</span><span class=\"mi\">04</span><span class=\"p\">:</span><span class=\"mi\">05</span><span class=\"p\">]</span> <span class=\"n\">Unloading</span><span class=\"p\">...</span> <span class=\"k\">schema</span> <span class=\"o\">=</span> <span class=\"n\">sc1</span> <span class=\"k\">and</span> <span class=\"k\">table</span> <span class=\"o\">=</span> <span class=\"n\">tbl1</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"n\">UNLOAD</span> <span class=\"n\">completed</span><span class=\"p\">,</span> <span class=\"mi\">3</span> <span class=\"n\">record</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">)</span> <span class=\"n\">unloaded</span> <span class=\"n\">successfully</span><span class=\"p\">.</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"p\">[</span><span class=\"mi\">2019</span><span class=\"o\">-</span><span class=\"mi\">11</span><span class=\"o\">-</span><span class=\"mi\">22</span> <span class=\"mi\">04</span><span class=\"p\">:</span><span class=\"mi\">04</span><span class=\"p\">:</span><span class=\"mi\">06</span><span class=\"p\">]</span> <span class=\"n\">Unloading</span><span class=\"p\">...</span> <span class=\"k\">schema</span> <span class=\"o\">=</span> <span class=\"n\">sc1</span> <span class=\"k\">and</span> <span class=\"k\">table</span> <span class=\"o\">=</span> <span class=\"n\">tbl2</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"n\">UNLOAD</span> <span class=\"n\">completed</span><span class=\"p\">,</span> <span class=\"mi\">3</span> <span class=\"n\">record</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">)</span> <span class=\"n\">unloaded</span> <span class=\"n\">successfully</span><span class=\"p\">.</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>   <span class=\"n\">Unloading</span> <span class=\"k\">of</span> <span class=\"n\">the</span> <span class=\"n\">DB</span> <span class=\"p\">[</span><span class=\"n\">stg</span><span class=\"p\">]</span> <span class=\"k\">is</span> <span class=\"n\">success</span> <span class=\"o\">!!!</span>    <span class=\"k\">CALL</span></code></pre></figure><p><strong>Export the tbl4, tbl5 without specifying any schema name: (but if you have multiple tables in the same name, all tables will be exported )</strong></p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\">    <span class=\"n\">stg</span><span class=\"o\">=#</span> <span class=\"k\">call</span> <span class=\"n\">unload_pro</span><span class=\"p\">(</span><span class=\"s1\">'s3://datalake/test/'</span><span class=\"p\">,</span><span class=\"k\">NULL</span><span class=\"p\">,</span><span class=\"s1\">'tbl4,tbl5'</span><span class=\"p\">);</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"p\">[</span><span class=\"mi\">2019</span><span class=\"o\">-</span><span class=\"mi\">11</span><span class=\"o\">-</span><span class=\"mi\">22</span> <span class=\"mi\">04</span><span class=\"p\">:</span><span class=\"mi\">04</span><span class=\"p\">:</span><span class=\"mi\">42</span><span class=\"p\">]</span> <span class=\"n\">Unloading</span><span class=\"p\">...</span> <span class=\"k\">schema</span> <span class=\"o\">=</span> <span class=\"n\">sc2</span> <span class=\"k\">and</span> <span class=\"k\">table</span> <span class=\"o\">=</span> <span class=\"n\">tbl4</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"n\">UNLOAD</span> <span class=\"n\">completed</span><span class=\"p\">,</span> <span class=\"mi\">3</span> <span class=\"n\">record</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">)</span> <span class=\"n\">unloaded</span> <span class=\"n\">successfully</span><span class=\"p\">.</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"p\">[</span><span class=\"mi\">2019</span><span class=\"o\">-</span><span class=\"mi\">11</span><span class=\"o\">-</span><span class=\"mi\">22</span> <span class=\"mi\">04</span><span class=\"p\">:</span><span class=\"mi\">04</span><span class=\"p\">:</span><span class=\"mi\">42</span><span class=\"p\">]</span> <span class=\"n\">Unloading</span><span class=\"p\">...</span> <span class=\"k\">schema</span> <span class=\"o\">=</span> <span class=\"n\">sc2</span> <span class=\"k\">and</span> <span class=\"k\">table</span> <span class=\"o\">=</span> <span class=\"n\">tbl5</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"n\">UNLOAD</span> <span class=\"n\">completed</span><span class=\"p\">,</span> <span class=\"mi\">3</span> <span class=\"n\">record</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">)</span> <span class=\"n\">unloaded</span> <span class=\"n\">successfully</span><span class=\"p\">.</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>   <span class=\"n\">Unloading</span> <span class=\"k\">of</span> <span class=\"n\">the</span> <span class=\"n\">DB</span> <span class=\"p\">[</span><span class=\"n\">stg</span><span class=\"p\">]</span> <span class=\"k\">is</span> <span class=\"n\">success</span> <span class=\"o\">!!!</span>    <span class=\"k\">CALL</span></code></pre></figure><p><strong>Export all the tables in all the schemas:</strong></p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\">    <span class=\"n\">stg</span><span class=\"o\">=#</span> <span class=\"k\">call</span> <span class=\"n\">unload_pro</span><span class=\"p\">(</span><span class=\"s1\">'s3://datalake/test/'</span><span class=\"p\">,</span><span class=\"k\">NULL</span><span class=\"p\">,</span><span class=\"k\">NULL</span><span class=\"p\">);</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"p\">[</span><span class=\"mi\">2019</span><span class=\"o\">-</span><span class=\"mi\">11</span><span class=\"o\">-</span><span class=\"mi\">22</span> <span class=\"mi\">04</span><span class=\"p\">:</span><span class=\"mi\">05</span><span class=\"p\">:</span><span class=\"mi\">23</span><span class=\"p\">]</span> <span class=\"n\">Unloading</span><span class=\"p\">...</span> <span class=\"k\">schema</span> <span class=\"o\">=</span> <span class=\"n\">sc1</span> <span class=\"k\">and</span> <span class=\"k\">table</span> <span class=\"o\">=</span> <span class=\"n\">tbl2</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"n\">UNLOAD</span> <span class=\"n\">completed</span><span class=\"p\">,</span> <span class=\"mi\">3</span> <span class=\"n\">record</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">)</span> <span class=\"n\">unloaded</span> <span class=\"n\">successfully</span><span class=\"p\">.</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"p\">[</span><span class=\"mi\">2019</span><span class=\"o\">-</span><span class=\"mi\">11</span><span class=\"o\">-</span><span class=\"mi\">22</span> <span class=\"mi\">04</span><span class=\"p\">:</span><span class=\"mi\">05</span><span class=\"p\">:</span><span class=\"mi\">23</span><span class=\"p\">]</span> <span class=\"n\">Unloading</span><span class=\"p\">...</span> <span class=\"k\">schema</span> <span class=\"o\">=</span> <span class=\"n\">sc2</span> <span class=\"k\">and</span> <span class=\"k\">table</span> <span class=\"o\">=</span> <span class=\"n\">tbl4</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"n\">UNLOAD</span> <span class=\"n\">completed</span><span class=\"p\">,</span> <span class=\"mi\">3</span> <span class=\"n\">record</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">)</span> <span class=\"n\">unloaded</span> <span class=\"n\">successfully</span><span class=\"p\">.</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"p\">[</span><span class=\"mi\">2019</span><span class=\"o\">-</span><span class=\"mi\">11</span><span class=\"o\">-</span><span class=\"mi\">22</span> <span class=\"mi\">04</span><span class=\"p\">:</span><span class=\"mi\">05</span><span class=\"p\">:</span><span class=\"mi\">24</span><span class=\"p\">]</span> <span class=\"n\">Unloading</span><span class=\"p\">...</span> <span class=\"k\">schema</span> <span class=\"o\">=</span> <span class=\"n\">sc2</span> <span class=\"k\">and</span> <span class=\"k\">table</span> <span class=\"o\">=</span> <span class=\"n\">tbl6</span>    <span class=\"p\">...................</span>    <span class=\"p\">...................</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"p\">[</span><span class=\"mi\">2019</span><span class=\"o\">-</span><span class=\"mi\">11</span><span class=\"o\">-</span><span class=\"mi\">22</span> <span class=\"mi\">04</span><span class=\"p\">:</span><span class=\"mi\">05</span><span class=\"p\">:</span><span class=\"mi\">28</span><span class=\"p\">]</span> <span class=\"n\">Unloading</span><span class=\"p\">...</span> <span class=\"k\">schema</span> <span class=\"o\">=</span> <span class=\"k\">public</span> <span class=\"k\">and</span> <span class=\"k\">table</span> <span class=\"o\">=</span> <span class=\"n\">nsa</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"n\">UNLOAD</span> <span class=\"n\">completed</span><span class=\"p\">,</span> <span class=\"mi\">0</span> <span class=\"n\">record</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">)</span> <span class=\"n\">unloaded</span> <span class=\"n\">successfully</span><span class=\"p\">.</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>   <span class=\"n\">Unloading</span> <span class=\"k\">of</span> <span class=\"n\">the</span> <span class=\"n\">DB</span> <span class=\"p\">[</span><span class=\"n\">stg</span><span class=\"p\">]</span> <span class=\"k\">is</span> <span class=\"n\">success</span> <span class=\"o\">!!!</span>    <span class=\"k\">CALL</span></code></pre></figure><ul>  <li>From the <code class=\"language-html highlighter-rouge\">unload_history</code> table you can get the COPY query to load into any other RedShift cluster.</li>  <li>Caution: You need to install this procedure on all the databases to work seamlessly.</li></ul><h2 id=\"reference-links\">Reference Links:</h2><ol>  <li><a href=\"\">Why are we unload with partitions (yyyy/mm/dd) in S3</a></li>  <li><a href=\"https://thedataguy.in/redshift-unload-all-tables-to-s3/\">Unload all the tables to S3.</a></li>  <li><a href=\"https://thedataguy.in/redshift-stored-procedure-comma-separated-string-in-argument/\">Use comma separated string in RedShift stored procedure argument.</a></li></ol>",
            "url": "/2019/11/22/redshift-unload-multiple-tables-schema-to-s3",
            "image": "/assets/RedShift Unload Like A Pro - Multiple Tables And Schemas.jpg",
            
            
            "tags": ["aws","redshift","unload","s3","sql"],
            
            "date_published": "2019-11-22T04:35:00+00:00",
            "date_modified": "2019-11-22T04:35:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2019/11/07/redshift-stored-procedure-comma-separated-string-in-argument",
            "title": "Redshift Stored Procedure Comma separated string in Argument",
            "summary": "In this blog we experimented in redshift stored procedure to use multiple comma separated values as an augment/parameter and pass it into where clause. ",
            "content_text": "RedShift stored procedures are really useful feature and after a long time they introduced it. I tried this stored procedure for many use cases and recently I came up with a complexity in it. You can’t get a comma separated sting in the argument. Lets say I have an employee table and I want to pass the employee names in a variable/argument and you need to select * from table where employee_name in ('your comma separated values). It’ll not work directly.Why? many SQL peoples already know the reason. Because you have multiple values in one string and you need to parse it as multiple quotes string and pass it.Example:Input: 'emp1, emp2, emp3'In SQL you can’t use IN clause like this. You need to split each and every value with single quote. Then it should be'emp1', 'emp2', 'emp3'In plpgsql, you can do this parsing with some temp table snd SQL queries. I have asked the steps in DBA’s stackoverflow and a horse with no name (yeah, its a weird name) answered it.But in Redshift it did’t work as expected.What’s wrong in Redshift:Before checking why its not working, we’ll see how are we thinking to parsing it.  Pass the argument into a variable.  Use trim functions to remove the single quote.  Use the split function to split these values.  For each values put single quote in the beginning and the end.Or as mentioned the stackoverflow answer, we can’t use arrays for this. Thats why its not working and it’ll throw the following error.    ERROR: varchar[] is not a supported parameter type for functions or proceduresThen I searched many blogs and found the useful information.  Convert comma separated values as rows.  Convert arrays into rowsI almost fixed the issue. But now the other problem came when I do query the Information_schema tables. My select query will look like below.    SELECT table_schema,            table_name     FROM   information_schema.TABLES     WHERE  table_type = 'BASE TABLE'            AND table_schema NOT IN ( 'pg_catalog', 'information_schema' )            AND table_name IN (SELECT tname                               FROM   my_parsed_variables);While running this, I got the below error.    test=# call my_sp('bhuvi');    INFO:  Table \"my_tables\" does not exist and will be skipped    INFO:  Table \"my_tokenized_tables\" does not exist and will be skipped    INFO:  Function \"has_table_privilege(oid,text)\" not supported.    INFO:  Function \"has_table_privilege(oid,text)\" not supported.    INFO:  Function \"has_table_privilege(oid,text)\" not supported.    INFO:  Function \"has_table_privilege(oid,text)\" not supported.    INFO:  Function \"has_table_privilege(oid,text)\" not supported.    INFO:  Function \"has_table_privilege(oid,text)\" not supported.    INFO:  Function \"has_table_privilege(oid,text)\" not supported.    ERROR:  Specified types or functions (one per INFO message) not supported on Redshift tables.    CONTEXT:  SQL statement \" SELECT table_schema, table_name FROM information_schema.TABLES WHERE table_type='BASE TABLE' AND table_schema NOT IN ('pg_catalog', 'information_schema') AND table_name !='unload_history' AND TABLE_NAME in (select tname from my_tokenized_tables)\"Its saying we don’t have enough permissions. So I need to reached out AWS support team to help on this one piece. Then they got help from RedShift product team to write the correct SQL query.Stored Procedure with multiple values:With my stored procedure, I want to get the list of table names from the argument and print its name along with schema.    DROP PROCEDURE multi_comma_values(varchar);        CREATE OR REPLACE PROCEDURE multi_comma_values(tbl_list IN varchar(400))    LANGUAGE plpgsql    AS $$    DECLARE        rec RECORD;    BEGIN        DROP TABLE IF EXISTS my_tables;        DROP TABLE IF EXISTS my_tokenized_tables;        CREATE TEMP TABLE my_tables (comma_table_names varchar(400));        EXECUTE 'INSERT INTO my_tables VALUES (' || quote_literal(tbl_list) || ')';            with NS AS (          select 1 as n union all          select 2 union all          select 3 union all          select 4 union all          select 5 union all          select 6 union all          select 7 union all          select 8 union all          select 9 union all          select 10        )        SELECT     Trim(Split_part(b.comma_table_names, ',', ns.n)) AS tname         INTO       temp table my_tokenized_tables         FROM       ns         inner join my_tables b         ON         ns.n &lt;= regexp_count(b.comma_table_names, ',') + 1;            FOR rec IN         SELECT nspname :: text AS namespace,            relname :: text AS tablename         FROM   pg_class pc            join pg_namespace pn              ON pc.relnamespace = pn.oid         WHERE  Trim(relname :: text) IN (SELECT tname                                      FROM   my_tokenized_tables)            AND Trim(nspname :: text) NOT IN ( 'pg_catalog', 'information_schema' )                 LOOP            raise info 'schema = %, table = %', rec.namespace, rec.tablename;        END LOOP;        DROP TABLE IF EXISTS my_tables;        DROP TABLE IF EXISTS my_tokenized_tables;    END;    $$;Call the procedure:    test=# call multi_comma_values ('bhuvi,test');    INFO:  Table \"my_tables\" does not exist and will be skipped    INFO:  Table \"my_tokenized_tables\" does not exist and will be skipped    INFO:  schema = public, table = test    INFO:  schema = public, table = bhuvi    CALLAnother Scanrio:I have a test table called bhuvi and I have 3 rows in it.CREATE TABLE bhuvi   (      name VARCHAR(10)   ); INSERT INTO bhuvi VALUES      ('a'); INSERT INTO bhuvi VALUES      ('b'); INSERT INTO bhuvi VALUES      ('c'); Now I want to run a select query where name in b and c. So in the above stored procure, I just edited the below FOR LOOP part.FOR rec IN          SELECT name        FROM   bhuvi        WHERE  name IN (SELECT tname                                      FROM   my_tokenized_tables)                 LOOP            raise info 'value is = %',rec.name;        END LOOP;Call the Proceduretest=# call multi_comma_values ('b,c');INFO:  Table \"my_tables\" does not exist and will be skippedINFO:  Table \"my_tokenized_tables\" does not exist and will be skippedINFO:  value is = bINFO:  value is = cCALLMore Customization:  If you would to have a permanent table, then create it in your database and remove the With NS as  This will only extract upto 256 I guess, if you inputing more than 10 then refer this.",
            "content_html": "<p>RedShift stored procedures are really useful feature and after a long time they introduced it. I tried this stored procedure for many use cases and recently I came up with a complexity in it. You can’t get a comma separated sting in the argument. Lets say I have an employee table and I want to pass the employee names in a variable/argument and you need to <code class=\"language-html highlighter-rouge\">select * from table where employee_name in ('your comma separated values)</code>. It’ll not work directly.</p><p>Why? many SQL peoples already know the reason. Because you have multiple values in one string and you need to parse it as multiple quotes string and pass it.</p><p><strong>Example:</strong></p><p>Input: <code class=\"language-html highlighter-rouge\">'emp1, emp2, emp3'</code></p><p>In SQL you can’t use IN clause like this. You need to split each and every value with single quote. Then it should be</p><p><code class=\"language-html highlighter-rouge\">'emp1', 'emp2', 'emp3'</code></p><p>In <code class=\"language-html highlighter-rouge\">plpgsql</code>, you can do this parsing with some temp table snd SQL queries. I have asked the steps in <a href=\"https://dba.stackexchange.com/questions/250323/plpgsql-stored-procedure-comma-separated-values-in-parameters\">DBA’s stackoverflow</a> and <a href=\"https://dba.stackexchange.com/questions/250323/plpgsql-stored-procedure-comma-separated-values-in-parameters\"><strong>a horse with no name</strong></a> (yeah, its a weird name) answered it.</p><p>But in Redshift it did’t work as expected.</p><h2 id=\"whats-wrong-in-redshift\">What’s wrong in Redshift:</h2><p>Before checking why its not working, we’ll see how are we thinking to parsing it.</p><ol>  <li>Pass the argument into a variable.</li>  <li>Use trim functions to remove the single quote.</li>  <li>Use the split function to split these values.</li>  <li>For each values put single quote in the beginning and the end.</li></ol><p>Or as mentioned the stackoverflow answer, we can’t use arrays for this. Thats why its not working and it’ll throw the following error.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    ERROR: varchar[] is not a supported parameter <span class=\"nb\">type </span><span class=\"k\">for </span>functions or procedures</code></pre></figure><p>Then I searched many blogs and found the useful information.</p><ol>  <li><a href=\"https://stackoverflow.com/questions/25112389/redshift-convert-comma-delimited-values-into-rows\">Convert comma separated values as rows.</a></li>  <li><a href=\"https://www.holistics.io/blog/splitting-array-string-into-rows-in-amazon-redshift-or-mysql/\">Convert arrays into rows</a></li></ol><p>I almost fixed the issue. But now the other problem came when I do query the <code class=\"language-html highlighter-rouge\">Information_schema</code> tables. My select query will look like below.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\">    <span class=\"k\">SELECT</span> <span class=\"n\">table_schema</span><span class=\"p\">,</span>            <span class=\"k\">table_name</span>     <span class=\"k\">FROM</span>   <span class=\"n\">information_schema</span><span class=\"p\">.</span><span class=\"n\">TABLES</span>     <span class=\"k\">WHERE</span>  <span class=\"n\">table_type</span> <span class=\"o\">=</span> <span class=\"s1\">'BASE TABLE'</span>            <span class=\"k\">AND</span> <span class=\"n\">table_schema</span> <span class=\"k\">NOT</span> <span class=\"k\">IN</span> <span class=\"p\">(</span> <span class=\"s1\">'pg_catalog'</span><span class=\"p\">,</span> <span class=\"s1\">'information_schema'</span> <span class=\"p\">)</span>            <span class=\"k\">AND</span> <span class=\"k\">table_name</span> <span class=\"k\">IN</span> <span class=\"p\">(</span><span class=\"k\">SELECT</span> <span class=\"n\">tname</span>                               <span class=\"k\">FROM</span>   <span class=\"n\">my_parsed_variables</span><span class=\"p\">);</span></code></pre></figure><p>While running this, I got the below error.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\">    <span class=\"n\">test</span><span class=\"o\">=#</span> <span class=\"k\">call</span> <span class=\"n\">my_sp</span><span class=\"p\">(</span><span class=\"s1\">'bhuvi'</span><span class=\"p\">);</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"k\">Table</span> <span class=\"nv\">\"my_tables\"</span> <span class=\"n\">does</span> <span class=\"k\">not</span> <span class=\"n\">exist</span> <span class=\"k\">and</span> <span class=\"n\">will</span> <span class=\"n\">be</span> <span class=\"n\">skipped</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"k\">Table</span> <span class=\"nv\">\"my_tokenized_tables\"</span> <span class=\"n\">does</span> <span class=\"k\">not</span> <span class=\"n\">exist</span> <span class=\"k\">and</span> <span class=\"n\">will</span> <span class=\"n\">be</span> <span class=\"n\">skipped</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"k\">Function</span> <span class=\"nv\">\"has_table_privilege(oid,text)\"</span> <span class=\"k\">not</span> <span class=\"n\">supported</span><span class=\"p\">.</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"k\">Function</span> <span class=\"nv\">\"has_table_privilege(oid,text)\"</span> <span class=\"k\">not</span> <span class=\"n\">supported</span><span class=\"p\">.</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"k\">Function</span> <span class=\"nv\">\"has_table_privilege(oid,text)\"</span> <span class=\"k\">not</span> <span class=\"n\">supported</span><span class=\"p\">.</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"k\">Function</span> <span class=\"nv\">\"has_table_privilege(oid,text)\"</span> <span class=\"k\">not</span> <span class=\"n\">supported</span><span class=\"p\">.</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"k\">Function</span> <span class=\"nv\">\"has_table_privilege(oid,text)\"</span> <span class=\"k\">not</span> <span class=\"n\">supported</span><span class=\"p\">.</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"k\">Function</span> <span class=\"nv\">\"has_table_privilege(oid,text)\"</span> <span class=\"k\">not</span> <span class=\"n\">supported</span><span class=\"p\">.</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"k\">Function</span> <span class=\"nv\">\"has_table_privilege(oid,text)\"</span> <span class=\"k\">not</span> <span class=\"n\">supported</span><span class=\"p\">.</span>    <span class=\"n\">ERROR</span><span class=\"p\">:</span>  <span class=\"n\">Specified</span> <span class=\"n\">types</span> <span class=\"k\">or</span> <span class=\"n\">functions</span> <span class=\"p\">(</span><span class=\"n\">one</span> <span class=\"n\">per</span> <span class=\"n\">INFO</span> <span class=\"n\">message</span><span class=\"p\">)</span> <span class=\"k\">not</span> <span class=\"n\">supported</span> <span class=\"k\">on</span> <span class=\"n\">Redshift</span> <span class=\"n\">tables</span><span class=\"p\">.</span>    <span class=\"n\">CONTEXT</span><span class=\"p\">:</span>  <span class=\"k\">SQL</span> <span class=\"k\">statement</span> <span class=\"nv\">\" SELECT table_schema, table_name FROM information_schema.TABLES WHERE table_type='BASE TABLE' AND table_schema NOT IN ('pg_catalog', 'information_schema') AND table_name !='unload_history' AND TABLE_NAME in (select tname from my_tokenized_tables)\"</span></code></pre></figure><p>Its saying we don’t have enough permissions. So I need to reached out AWS support team to help on this one piece. Then they got help from RedShift product team to write the correct SQL query.</p><h2 id=\"stored-procedure-with-multiple-values\">Stored Procedure with multiple values:</h2><p>With my stored procedure, I want to get the list of table names from the argument and print its name along with schema.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\">    <span class=\"k\">DROP</span> <span class=\"k\">PROCEDURE</span> <span class=\"n\">multi_comma_values</span><span class=\"p\">(</span><span class=\"nb\">varchar</span><span class=\"p\">);</span>        <span class=\"k\">CREATE</span> <span class=\"k\">OR</span> <span class=\"k\">REPLACE</span> <span class=\"k\">PROCEDURE</span> <span class=\"n\">multi_comma_values</span><span class=\"p\">(</span><span class=\"n\">tbl_list</span> <span class=\"k\">IN</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">400</span><span class=\"p\">))</span>    <span class=\"k\">LANGUAGE</span> <span class=\"n\">plpgsql</span>    <span class=\"k\">AS</span> <span class=\"err\">$$</span>    <span class=\"k\">DECLARE</span>        <span class=\"n\">rec</span> <span class=\"n\">RECORD</span><span class=\"p\">;</span>    <span class=\"k\">BEGIN</span>        <span class=\"k\">DROP</span> <span class=\"k\">TABLE</span> <span class=\"n\">IF</span> <span class=\"k\">EXISTS</span> <span class=\"n\">my_tables</span><span class=\"p\">;</span>        <span class=\"k\">DROP</span> <span class=\"k\">TABLE</span> <span class=\"n\">IF</span> <span class=\"k\">EXISTS</span> <span class=\"n\">my_tokenized_tables</span><span class=\"p\">;</span>        <span class=\"k\">CREATE</span> <span class=\"k\">TEMP</span> <span class=\"k\">TABLE</span> <span class=\"n\">my_tables</span> <span class=\"p\">(</span><span class=\"n\">comma_table_names</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">400</span><span class=\"p\">));</span>        <span class=\"k\">EXECUTE</span> <span class=\"s1\">'INSERT INTO my_tables VALUES ('</span> <span class=\"o\">||</span> <span class=\"n\">quote_literal</span><span class=\"p\">(</span><span class=\"n\">tbl_list</span><span class=\"p\">)</span> <span class=\"o\">||</span> <span class=\"s1\">')'</span><span class=\"p\">;</span>            <span class=\"k\">with</span> <span class=\"n\">NS</span> <span class=\"k\">AS</span> <span class=\"p\">(</span>          <span class=\"k\">select</span> <span class=\"mi\">1</span> <span class=\"k\">as</span> <span class=\"n\">n</span> <span class=\"k\">union</span> <span class=\"k\">all</span>          <span class=\"k\">select</span> <span class=\"mi\">2</span> <span class=\"k\">union</span> <span class=\"k\">all</span>          <span class=\"k\">select</span> <span class=\"mi\">3</span> <span class=\"k\">union</span> <span class=\"k\">all</span>          <span class=\"k\">select</span> <span class=\"mi\">4</span> <span class=\"k\">union</span> <span class=\"k\">all</span>          <span class=\"k\">select</span> <span class=\"mi\">5</span> <span class=\"k\">union</span> <span class=\"k\">all</span>          <span class=\"k\">select</span> <span class=\"mi\">6</span> <span class=\"k\">union</span> <span class=\"k\">all</span>          <span class=\"k\">select</span> <span class=\"mi\">7</span> <span class=\"k\">union</span> <span class=\"k\">all</span>          <span class=\"k\">select</span> <span class=\"mi\">8</span> <span class=\"k\">union</span> <span class=\"k\">all</span>          <span class=\"k\">select</span> <span class=\"mi\">9</span> <span class=\"k\">union</span> <span class=\"k\">all</span>          <span class=\"k\">select</span> <span class=\"mi\">10</span>        <span class=\"p\">)</span>        <span class=\"k\">SELECT</span>     <span class=\"k\">Trim</span><span class=\"p\">(</span><span class=\"n\">Split_part</span><span class=\"p\">(</span><span class=\"n\">b</span><span class=\"p\">.</span><span class=\"n\">comma_table_names</span><span class=\"p\">,</span> <span class=\"s1\">','</span><span class=\"p\">,</span> <span class=\"n\">ns</span><span class=\"p\">.</span><span class=\"n\">n</span><span class=\"p\">))</span> <span class=\"k\">AS</span> <span class=\"n\">tname</span>         <span class=\"k\">INTO</span>       <span class=\"k\">temp</span> <span class=\"k\">table</span> <span class=\"n\">my_tokenized_tables</span>         <span class=\"k\">FROM</span>       <span class=\"n\">ns</span>         <span class=\"k\">inner</span> <span class=\"k\">join</span> <span class=\"n\">my_tables</span> <span class=\"n\">b</span>         <span class=\"k\">ON</span>         <span class=\"n\">ns</span><span class=\"p\">.</span><span class=\"n\">n</span> <span class=\"o\">&lt;=</span> <span class=\"n\">regexp_count</span><span class=\"p\">(</span><span class=\"n\">b</span><span class=\"p\">.</span><span class=\"n\">comma_table_names</span><span class=\"p\">,</span> <span class=\"s1\">','</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">;</span>            <span class=\"k\">FOR</span> <span class=\"n\">rec</span> <span class=\"k\">IN</span>         <span class=\"k\">SELECT</span> <span class=\"n\">nspname</span> <span class=\"p\">::</span> <span class=\"nb\">text</span> <span class=\"k\">AS</span> <span class=\"n\">namespace</span><span class=\"p\">,</span>            <span class=\"n\">relname</span> <span class=\"p\">::</span> <span class=\"nb\">text</span> <span class=\"k\">AS</span> <span class=\"n\">tablename</span>         <span class=\"k\">FROM</span>   <span class=\"n\">pg_class</span> <span class=\"n\">pc</span>            <span class=\"k\">join</span> <span class=\"n\">pg_namespace</span> <span class=\"n\">pn</span>              <span class=\"k\">ON</span> <span class=\"n\">pc</span><span class=\"p\">.</span><span class=\"n\">relnamespace</span> <span class=\"o\">=</span> <span class=\"n\">pn</span><span class=\"p\">.</span><span class=\"n\">oid</span>         <span class=\"k\">WHERE</span>  <span class=\"k\">Trim</span><span class=\"p\">(</span><span class=\"n\">relname</span> <span class=\"p\">::</span> <span class=\"nb\">text</span><span class=\"p\">)</span> <span class=\"k\">IN</span> <span class=\"p\">(</span><span class=\"k\">SELECT</span> <span class=\"n\">tname</span>                                      <span class=\"k\">FROM</span>   <span class=\"n\">my_tokenized_tables</span><span class=\"p\">)</span>            <span class=\"k\">AND</span> <span class=\"k\">Trim</span><span class=\"p\">(</span><span class=\"n\">nspname</span> <span class=\"p\">::</span> <span class=\"nb\">text</span><span class=\"p\">)</span> <span class=\"k\">NOT</span> <span class=\"k\">IN</span> <span class=\"p\">(</span> <span class=\"s1\">'pg_catalog'</span><span class=\"p\">,</span> <span class=\"s1\">'information_schema'</span> <span class=\"p\">)</span>                 <span class=\"n\">LOOP</span>            <span class=\"n\">raise</span> <span class=\"n\">info</span> <span class=\"s1\">'schema = %, table = %'</span><span class=\"p\">,</span> <span class=\"n\">rec</span><span class=\"p\">.</span><span class=\"n\">namespace</span><span class=\"p\">,</span> <span class=\"n\">rec</span><span class=\"p\">.</span><span class=\"n\">tablename</span><span class=\"p\">;</span>        <span class=\"k\">END</span> <span class=\"n\">LOOP</span><span class=\"p\">;</span>        <span class=\"k\">DROP</span> <span class=\"k\">TABLE</span> <span class=\"n\">IF</span> <span class=\"k\">EXISTS</span> <span class=\"n\">my_tables</span><span class=\"p\">;</span>        <span class=\"k\">DROP</span> <span class=\"k\">TABLE</span> <span class=\"n\">IF</span> <span class=\"k\">EXISTS</span> <span class=\"n\">my_tokenized_tables</span><span class=\"p\">;</span>    <span class=\"k\">END</span><span class=\"p\">;</span>    <span class=\"err\">$$</span><span class=\"p\">;</span></code></pre></figure><h2 id=\"call-the-procedure\">Call the procedure:</h2><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\">    <span class=\"n\">test</span><span class=\"o\">=#</span> <span class=\"k\">call</span> <span class=\"n\">multi_comma_values</span> <span class=\"p\">(</span><span class=\"s1\">'bhuvi,test'</span><span class=\"p\">);</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"k\">Table</span> <span class=\"nv\">\"my_tables\"</span> <span class=\"n\">does</span> <span class=\"k\">not</span> <span class=\"n\">exist</span> <span class=\"k\">and</span> <span class=\"n\">will</span> <span class=\"n\">be</span> <span class=\"n\">skipped</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"k\">Table</span> <span class=\"nv\">\"my_tokenized_tables\"</span> <span class=\"n\">does</span> <span class=\"k\">not</span> <span class=\"n\">exist</span> <span class=\"k\">and</span> <span class=\"n\">will</span> <span class=\"n\">be</span> <span class=\"n\">skipped</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"k\">schema</span> <span class=\"o\">=</span> <span class=\"k\">public</span><span class=\"p\">,</span> <span class=\"k\">table</span> <span class=\"o\">=</span> <span class=\"n\">test</span>    <span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"k\">schema</span> <span class=\"o\">=</span> <span class=\"k\">public</span><span class=\"p\">,</span> <span class=\"k\">table</span> <span class=\"o\">=</span> <span class=\"n\">bhuvi</span>    <span class=\"k\">CALL</span></code></pre></figure><h2 id=\"another-scanrio\">Another Scanrio:</h2><p>I have a test table called <code class=\"language-html highlighter-rouge\">bhuvi</code> and I have 3 rows in it.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">CREATE</span> <span class=\"k\">TABLE</span> <span class=\"n\">bhuvi</span>   <span class=\"p\">(</span>      <span class=\"n\">name</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">)</span>   <span class=\"p\">);</span> <span class=\"k\">INSERT</span> <span class=\"k\">INTO</span> <span class=\"n\">bhuvi</span> <span class=\"k\">VALUES</span>      <span class=\"p\">(</span><span class=\"s1\">'a'</span><span class=\"p\">);</span> <span class=\"k\">INSERT</span> <span class=\"k\">INTO</span> <span class=\"n\">bhuvi</span> <span class=\"k\">VALUES</span>      <span class=\"p\">(</span><span class=\"s1\">'b'</span><span class=\"p\">);</span> <span class=\"k\">INSERT</span> <span class=\"k\">INTO</span> <span class=\"n\">bhuvi</span> <span class=\"k\">VALUES</span>      <span class=\"p\">(</span><span class=\"s1\">'c'</span><span class=\"p\">);</span> </code></pre></figure><p>Now I want to run a select query where name in <code class=\"language-html highlighter-rouge\">b and c</code>. So in the above stored procure, I just edited the below FOR LOOP part.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">FOR</span> <span class=\"n\">rec</span> <span class=\"k\">IN</span>          <span class=\"k\">SELECT</span> <span class=\"n\">name</span>        <span class=\"k\">FROM</span>   <span class=\"n\">bhuvi</span>        <span class=\"k\">WHERE</span>  <span class=\"n\">name</span> <span class=\"k\">IN</span> <span class=\"p\">(</span><span class=\"k\">SELECT</span> <span class=\"n\">tname</span>                                      <span class=\"k\">FROM</span>   <span class=\"n\">my_tokenized_tables</span><span class=\"p\">)</span>                 <span class=\"n\">LOOP</span>            <span class=\"n\">raise</span> <span class=\"n\">info</span> <span class=\"s1\">'value is = %'</span><span class=\"p\">,</span><span class=\"n\">rec</span><span class=\"p\">.</span><span class=\"n\">name</span><span class=\"p\">;</span>        <span class=\"k\">END</span> <span class=\"n\">LOOP</span><span class=\"p\">;</span></code></pre></figure><p><strong>Call the Procedure</strong></p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"n\">test</span><span class=\"o\">=#</span> <span class=\"k\">call</span> <span class=\"n\">multi_comma_values</span> <span class=\"p\">(</span><span class=\"s1\">'b,c'</span><span class=\"p\">);</span><span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"k\">Table</span> <span class=\"nv\">\"my_tables\"</span> <span class=\"n\">does</span> <span class=\"k\">not</span> <span class=\"n\">exist</span> <span class=\"k\">and</span> <span class=\"n\">will</span> <span class=\"n\">be</span> <span class=\"n\">skipped</span><span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"k\">Table</span> <span class=\"nv\">\"my_tokenized_tables\"</span> <span class=\"n\">does</span> <span class=\"k\">not</span> <span class=\"n\">exist</span> <span class=\"k\">and</span> <span class=\"n\">will</span> <span class=\"n\">be</span> <span class=\"n\">skipped</span><span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"n\">value</span> <span class=\"k\">is</span> <span class=\"o\">=</span> <span class=\"n\">b</span><span class=\"n\">INFO</span><span class=\"p\">:</span>  <span class=\"n\">value</span> <span class=\"k\">is</span> <span class=\"o\">=</span> <span class=\"k\">c</span><span class=\"k\">CALL</span></code></pre></figure><h2 id=\"more-customization\">More Customization:</h2><ul>  <li>If you would to have a permanent table, then create it in your database and remove the <code class=\"language-html highlighter-rouge\">With NS as</code></li>  <li>This will only extract upto 256 I guess, if you inputing more than 10 then refer <a href=\"https://www.holistics.io/blog/splitting-array-string-into-rows-in-amazon-redshift-or-mysql/\">this</a>.</li></ul>",
            "url": "/2019/11/07/redshift-stored-procedure-comma-separated-string-in-argument",
            "image": "/assets/Redshift Stored Procedure Comma separated string in Argument.jpg",
            
            
            "tags": ["aws"," redshift","sql"],
            
            "date_published": "2019-11-07T09:00:00+00:00",
            "date_modified": "2019-11-07T09:00:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2019/10/29/where-gcp-internal-load-balancer-fails",
            "title": "Where GCP Internal TCP Load Balancer Fails",
            "summary": "GCP Internal TCP load balancer route the traffic to the same node the traffic source is the node which is under the load balancer",
            "content_text": "GCP’s Load balancers are globally scalable and its the unique identify for GCP while comparing its competitors. Generally GCP’s networking is very strong and mature than other Cloud providers.  Recently I was working with a SQL Server setup which integrates the GCP Internal TCP load balancer. During that PoC setup I found this strange behaviour and then I ignored this problem because I thought its something related to SQL server.The PoC Setup:I wanted to reproduce this scenario in simple web server stack. So I created 2 apache web servers and put them under the Internal TCP load balancer.  Web Server 1: the index page will show This is from Node1  Web Server 2: the index page will show This is from Node2  Load balancer IP: 10.128.0.46The health check is passed and Im able to see both nodes are healthy. Then I tried to CURL the load balancer’s IP from the other VM which is on the same subnet. The traffic is routed to node 1 and second time node2. Till now everything looks good.Curl from a VM in the same subnet:    # Curl 1st time    curl 10.128.0.46    &lt;html&gt;            &lt;body&gt;                    &lt;H1&gt; This is from Node1 &lt;/H2&gt;            &lt;/body&gt;    &lt;/html&gt;        # Curl 2nd time        curl 10.128.0.46    &lt;html&gt;            &lt;body&gt;                    &lt;H1&gt; This is from Node2 &lt;/H2&gt;            &lt;/body&gt;    &lt;/html&gt;The strange behaviour in my previous setup:In my SQL Server setup the issue I faced is, Its an alwayson availability group setup and the TCP Load balancer will route the traffic to the Primary Node. The complete setup of Configuring the external Load balancer for SQL server availability groups is here. There I have some packages where I need to talk to the Primary Node. The package will be executing from the both primary and secondary. It tried to talk to the Load balancer IP. But from Primary node and other nodes in the subnet always properly reached the Primary node via the TCP load balancer. But the secondary server is not able to reach, instead it was showing the role is Secondary. Then I connected to SQL server using load balancer IP and print the node name. It was showing the secondary server’s name.Then I noticed, the traffic is always routed to the Node 2 if Im trying to access the Load balancer IP from the node2.Issue Verified:From apache webserver’s PoC, I tried to curl the Load Balancer’s from Node 1 and then Node2.Node 1:    root@bhuvi-node1:/var/www/html# curl 10.128.0.46    &lt;html&gt;            &lt;body&gt;                    &lt;H1&gt; This is from Node1 &lt;/H2&gt;            &lt;/body&gt;    &lt;/html&gt;         root@bhuvi-node1:/var/www/html# curl 10.128.0.46    &lt;html&gt;            &lt;body&gt;                    &lt;H1&gt; This is from Node1 &lt;/H2&gt;            &lt;/body&gt;    &lt;/html&gt;          root@bhuvi-node1:/var/www/html# Node 2:    root@bhuvi-node1:/var/www/html# curl 10.128.0.46    &lt;html&gt;            &lt;body&gt;                    &lt;H1&gt; This is from Node2 &lt;/H2&gt;            &lt;/body&gt;    &lt;/html&gt;          root@bhuvi-node1:/var/www/html# curl 10.128.0.46    &lt;html&gt;            &lt;body&gt;                    &lt;H1&gt; This is from Node2 &lt;/H2&gt;            &lt;/body&gt;    &lt;/html&gt;        root@bhuvi-node1:/var/www/html#   Its proving that the internal TCP load balancer routes the traffic to the same node in the source packet is coming from the node which is part of the load balancer.The Cause for this routing:The exact reason for this behaviour is not a bug or an issue. Its the design of the TCP load balancer in GCP. What happened is, the moment when you add the instance group to the load balancer’s backend, then GCP will automatically add a route in your VM that the Load balancer’s IP is the Local host’s IP.Node 1:    sudo ip route list table local    local 10.128.0.40 dev ens4 proto kernel scope host src 10.128.0.40     local 10.128.0.46 dev ens4 proto 66 scope host     broadcast 127.0.0.0 dev lo proto kernel scope link src 127.0.0.1     local 127.0.0.0/8 dev lo proto kernel scope host src 127.0.0.1     local 127.0.0.1 dev lo proto kernel scope host src 127.0.0.1     broadcast 127.255.255.255 dev lo proto kernel scope link src 127.0.0.1  Node 2:    sudo ip route list table local    local 10.128.0.41 dev ens4 proto kernel scope host src 10.128.0.41     local 10.128.0.46 dev ens4 proto 66 scope host     broadcast 127.0.0.0 dev lo proto kernel scope link src 127.0.0.1     local 127.0.0.0/8 dev lo proto kernel scope host src 127.0.0.1     local 127.0.0.1 dev lo proto kernel scope host src 127.0.0.1     broadcast 127.255.255.255 dev lo proto kernel scope link src 127.0.0.1 Both nodes are showing that local 10.128.0.46 dev ens4 proto 66 scope host , so whenever we tried to access the load balancer’s IP, then the OS route table will route the traffic to the same node.  I have verified this with GCP support team.Is this a serious Issue:Not at all. No one will try to access the load balancer from the nodes which are under the load balancer(Except some cases like mine).What about HTTP/HTTPS Load balancer?:I tried to reproduce the same issue with HTTP/HTTPS load balancer. There I didn’t see this problem.",
            "content_html": "<p>GCP’s Load balancers are globally scalable and its the unique identify for GCP while comparing its competitors. Generally GCP’s networking is very strong and mature than other Cloud providers.  Recently I was working with a SQL Server setup which integrates the GCP Internal TCP load balancer. During that PoC setup I found this strange behaviour and then I ignored this problem because I thought its something related to SQL server.</p><h2 id=\"the-poc-setup\">The PoC Setup:</h2><p>I wanted to reproduce this scenario in simple web server stack. So I created 2 apache web servers and put them under the Internal TCP load balancer.</p><ul>  <li><strong>Web Server 1</strong>: the index page will show <code class=\"language-html highlighter-rouge\">This is from Node1</code></li>  <li><strong>Web Server 2</strong>: the index page will show <code class=\"language-html highlighter-rouge\">This is from Node2</code></li>  <li><strong>Load balancer IP:</strong> 10.128.0.46</li></ul><p>The health check is passed and Im able to see both nodes are healthy. Then I tried to CURL the load balancer’s IP from the other VM which is on the same subnet. The traffic is routed to node 1 and second time node2. Till now everything looks good.</p><h3 id=\"curl-from-a-vm-in-the-same-subnet\">Curl from a VM in the same subnet:</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    <span class=\"c\"># Curl 1st time</span>    curl 10.128.0.46    &lt;html&gt;            &lt;body&gt;                    &lt;H1&gt; This is from Node1 &lt;/H2&gt;            &lt;/body&gt;    &lt;/html&gt;        <span class=\"c\"># Curl 2nd time</span>        curl 10.128.0.46    &lt;html&gt;            &lt;body&gt;                    &lt;H1&gt; This is from Node2 &lt;/H2&gt;            &lt;/body&gt;    &lt;/html&gt;</code></pre></figure><h2 id=\"the-strange-behaviour-in-my-previous-setup\">The strange behaviour in my previous setup:</h2><p>In my SQL Server setup the issue I faced is, Its an alwayson availability group setup and the TCP Load balancer will route the traffic to the Primary Node. <a href=\"https://medium.com/searce/configure-external-listener-for-always-on-availability-groups-in-gcp-e1ae1c9632d1\">The complete setup of Configuring the external Load balancer for SQL server availability groups is here</a>. There I have some packages where I need to talk to the Primary Node. The package will be executing from the both primary and secondary. It tried to talk to the Load balancer IP. But from Primary node and other nodes in the subnet always properly reached the Primary node via the TCP load balancer. But the secondary server is not able to reach, instead it was showing the role is Secondary. Then I connected to SQL server using load balancer IP and print the node name. It was showing the secondary server’s name.</p><p>Then I noticed, the traffic is always routed to the Node 2 if Im trying to access the Load balancer IP from the node2.</p><h2 id=\"issue-verified\">Issue Verified:</h2><p>From apache webserver’s PoC, I tried to curl the Load Balancer’s from Node 1 and then Node2.</p><h3 id=\"node-1\">Node 1:</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    root@bhuvi-node1:/var/www/html# curl 10.128.0.46    &lt;html&gt;            &lt;body&gt;                    &lt;H1&gt; This is from Node1 &lt;/H2&gt;            &lt;/body&gt;    &lt;/html&gt;         root@bhuvi-node1:/var/www/html# curl 10.128.0.46    &lt;html&gt;            &lt;body&gt;                    &lt;H1&gt; This is from Node1 &lt;/H2&gt;            &lt;/body&gt;    &lt;/html&gt;          root@bhuvi-node1:/var/www/html# </code></pre></figure><h3 id=\"node-2\">Node 2:</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    root@bhuvi-node1:/var/www/html# curl 10.128.0.46    &lt;html&gt;            &lt;body&gt;                    &lt;H1&gt; This is from Node2 &lt;/H2&gt;            &lt;/body&gt;    &lt;/html&gt;          root@bhuvi-node1:/var/www/html# curl 10.128.0.46    &lt;html&gt;            &lt;body&gt;                    &lt;H1&gt; This is from Node2 &lt;/H2&gt;            &lt;/body&gt;    &lt;/html&gt;        root@bhuvi-node1:/var/www/html# </code></pre></figure><blockquote>  <p>Its proving that the internal TCP load balancer routes the traffic to the same node in the source packet is coming from the node which is part of the load balancer.</p></blockquote><h2 id=\"the-cause-for-this-routing\">The Cause for this routing:</h2><p>The exact reason for this behaviour is not a bug or an issue. Its the design of the TCP load balancer in GCP. What happened is, the moment when you add the instance group to the load balancer’s backend, then GCP will automatically add a route in your VM that the Load balancer’s IP is the Local host’s IP.</p><h3 id=\"node-1-1\">Node 1:</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    <span class=\"nb\">sudo </span>ip route list table <span class=\"nb\">local    local </span>10.128.0.40 dev ens4 proto kernel scope host src 10.128.0.40     <span class=\"nb\">local </span>10.128.0.46 dev ens4 proto 66 scope host     broadcast 127.0.0.0 dev lo proto kernel scope <span class=\"nb\">link </span>src 127.0.0.1     <span class=\"nb\">local </span>127.0.0.0/8 dev lo proto kernel scope host src 127.0.0.1     <span class=\"nb\">local </span>127.0.0.1 dev lo proto kernel scope host src 127.0.0.1     broadcast 127.255.255.255 dev lo proto kernel scope <span class=\"nb\">link </span>src 127.0.0.1  </code></pre></figure><h3 id=\"node-2-1\">Node 2:</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    <span class=\"nb\">sudo </span>ip route list table <span class=\"nb\">local    local </span>10.128.0.41 dev ens4 proto kernel scope host src 10.128.0.41     <span class=\"nb\">local </span>10.128.0.46 dev ens4 proto 66 scope host     broadcast 127.0.0.0 dev lo proto kernel scope <span class=\"nb\">link </span>src 127.0.0.1     <span class=\"nb\">local </span>127.0.0.0/8 dev lo proto kernel scope host src 127.0.0.1     <span class=\"nb\">local </span>127.0.0.1 dev lo proto kernel scope host src 127.0.0.1     broadcast 127.255.255.255 dev lo proto kernel scope <span class=\"nb\">link </span>src 127.0.0.1 </code></pre></figure><p>Both nodes are showing that <code class=\"language-html highlighter-rouge\">local 10.128.0.46 dev ens4 proto 66 scope host</code> , so whenever we tried to access the load balancer’s IP, then the OS route table will route the traffic to the same node.</p><blockquote>  <p>I have verified this with GCP support team.</p></blockquote><h2 id=\"is-this-a-serious-issue\">Is this a serious Issue:</h2><p><strong>Not at all.</strong> No one will try to access the load balancer from the nodes which are under the load balancer(Except some cases like mine).</p><h2 id=\"what-about-httphttps-load-balancer\">What about HTTP/HTTPS Load balancer?:</h2><p>I tried to reproduce the same issue with HTTP/HTTPS load balancer. There I didn’t see this problem.</p>",
            "url": "/2019/10/29/where-gcp-internal-load-balancer-fails",
            "image": "/assets/Where GCP Internal TCP Load Balancer Fails.png",
            
            
            "tags": ["gcp","networking","load balancer"],
            
            "date_published": "2019-10-29T08:00:00+00:00",
            "date_modified": "2019-10-29T08:00:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2019/10/21/mysql-calculate-how-much-disk-space-you-wasted",
            "title": "MySQL Calculate How Much Disk Space You Wasted",
            "summary": "MySQL calculate the fragmentation size for all tables. Get the wasted space in GB.",
            "content_text": "Its not the new term for DBAs. MySQL has an awesome parameter innodb-file-per-tables allows MySQL to create separate files for each tables. This helped a lot to manage the disk space in more efficient way. But when we perform a large batch job for delete or update the data in MySQL tables, you may face this fragmentation issue. Comparing with SQL server, MySQL’s fragmentation is not high. I had a similar situation where my Disk space was consuming 80% and when I check the huge files in OS, one table’s idb file consumed 300GB+. I know it has some wasted blocks(but not actually wasted, MySQL will use this space, it’ll not return this to OS) Then I checked the information schema to find out the data size and its index size. It was 27GB only. Then I realize, we did a batch operation to delete many billions of records in that table.Thanks to Rolando - MySQL DBA:When I searched the similar issue on dba stackexchange, I found this great script by Rolando. He had given this script to calculate the wasted size for a single table. I just add some South Indian Masala on top of it.(just for fun). You can use the below script to identify the wasted space/fragmented space in GB for all tables in a database.Parameters:  DB - Your Database Name  MYSQL_DATA_DIR - Your Data directory for MySQL  MYSQL_USER - MySQL user to query the information schema.  MYSQL_PASS - Password for the MySQL user.    DB='mydb'    MYSQL_DATA_DIR='/mysqldata'    MYSQL_USER=sqladmin    MYSQL_PASS='mypass!'    MYSQL_CONN=\"-u${MYSQL_USER} -p${MYSQL_PASS}\"        Tables=`ls -l $MYSQL_DATA_DIR/$DB/ | grep ibd | awk -F' ' '{print $9}' | sed -e 's/\\.ibd//g'`    for x in `echo $Tables`    do    TB=$x    SQL=\"SELECT data_length+index_length FROM information_schema.tables\"    SQL=\"${SQL} WHERE table_schema='${DB}' AND table_name='${TB}'\"    TBLSIZE_OPER=`ls -l $MYSQL_DATA_DIR/${DB}/${TB}.ibd | awk -F' ' '{print $5}'`    TBLSIZE_INFO=`mysql ${MYSQL_CONN} -ANe\"${SQL}\"`    TBLSIZE_FRAG=$(($TBLSIZE_OPER - $TBLSIZE_INFO))    TBLSIZE_FRAG_GB=$(($TBLSIZE_FRAG / 1073741824))    echo ${TB} ${TBLSIZE_FRAG_GB}    doneExecutionIts better to create the script as a shell file and print the output in a file../script.sh &gt; output.txt",
            "content_html": "<p>Its not the new term for DBAs. MySQL has an awesome parameter <code class=\"language-html highlighter-rouge\">innodb-file-per-tables</code> allows MySQL to create separate files for each tables. This helped a lot to manage the disk space in more efficient way. But when we perform a large batch job for delete or update the data in MySQL tables, you may face this fragmentation issue. Comparing with SQL server, MySQL’s fragmentation is not high. I had a similar situation where my Disk space was consuming 80% and when I check the huge files in OS, one table’s <code class=\"language-html highlighter-rouge\">idb</code> file consumed 300GB+. I know it has some wasted blocks(but not actually wasted, MySQL will use this space, it’ll not return this to OS) Then I checked the <code class=\"language-html highlighter-rouge\">information schema</code> to find out the data size and its index size. It was 27GB only. Then I realize, we did a batch operation to delete many billions of records in that table.</p><h2 id=\"thanks-to-rolando---mysql-dba\">Thanks to Rolando - MySQL DBA:</h2><p>When I searched the similar issue on <a href=\"https://dba.stackexchange.com/questions/142653/mysql-ibd-file-is-too-big/176812\">dba stackexchange</a>, I found this great script by <a href=\"https://dba.stackexchange.com/users/877/rolandomysqldba\">Rolando</a>. He had given this script to calculate the wasted size for a single table. I just add some <a href=\"https://food.ndtv.com/lists/10-best-south-indian-recipes-736459\">South Indian Masala</a> on top of it.(just for fun). You can use the below script to identify the wasted space/fragmented space in GB for all tables in a database.</p><h2 id=\"parameters\">Parameters:</h2><ul>  <li><strong>DB</strong> - Your Database Name</li>  <li><strong>MYSQL_DATA_DIR</strong> - Your Data directory for MySQL</li>  <li><strong>MYSQL_USER</strong> - MySQL user to query the <code class=\"language-html highlighter-rouge\">information schema</code>.</li>  <li><strong>MYSQL_PASS</strong> - Password for the MySQL user.</li></ul><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    <span class=\"nv\">DB</span><span class=\"o\">=</span><span class=\"s1\">'mydb'</span>    <span class=\"nv\">MYSQL_DATA_DIR</span><span class=\"o\">=</span><span class=\"s1\">'/mysqldata'</span>    <span class=\"nv\">MYSQL_USER</span><span class=\"o\">=</span>sqladmin    <span class=\"nv\">MYSQL_PASS</span><span class=\"o\">=</span><span class=\"s1\">'mypass!'</span>    <span class=\"nv\">MYSQL_CONN</span><span class=\"o\">=</span><span class=\"s2\">\"-u</span><span class=\"k\">${</span><span class=\"nv\">MYSQL_USER</span><span class=\"k\">}</span><span class=\"s2\"> -p</span><span class=\"k\">${</span><span class=\"nv\">MYSQL_PASS</span><span class=\"k\">}</span><span class=\"s2\">\"</span>        <span class=\"nv\">Tables</span><span class=\"o\">=</span><span class=\"sb\">`</span><span class=\"nb\">ls</span> <span class=\"nt\">-l</span> <span class=\"nv\">$MYSQL_DATA_DIR</span>/<span class=\"nv\">$DB</span>/ | <span class=\"nb\">grep </span>ibd | <span class=\"nb\">awk</span> <span class=\"nt\">-F</span><span class=\"s1\">' '</span> <span class=\"s1\">'{print $9}'</span> | <span class=\"nb\">sed</span> <span class=\"nt\">-e</span> <span class=\"s1\">'s/\\.ibd//g'</span><span class=\"sb\">`</span>    <span class=\"k\">for </span>x <span class=\"k\">in</span> <span class=\"sb\">`</span><span class=\"nb\">echo</span> <span class=\"nv\">$Tables</span><span class=\"sb\">`</span>    <span class=\"k\">do    </span><span class=\"nv\">TB</span><span class=\"o\">=</span><span class=\"nv\">$x</span>    <span class=\"nv\">SQL</span><span class=\"o\">=</span><span class=\"s2\">\"SELECT data_length+index_length FROM information_schema.tables\"</span>    <span class=\"nv\">SQL</span><span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"k\">${</span><span class=\"nv\">SQL</span><span class=\"k\">}</span><span class=\"s2\"> WHERE table_schema='</span><span class=\"k\">${</span><span class=\"nv\">DB</span><span class=\"k\">}</span><span class=\"s2\">' AND table_name='</span><span class=\"k\">${</span><span class=\"nv\">TB</span><span class=\"k\">}</span><span class=\"s2\">'\"</span>    <span class=\"nv\">TBLSIZE_OPER</span><span class=\"o\">=</span><span class=\"sb\">`</span><span class=\"nb\">ls</span> <span class=\"nt\">-l</span> <span class=\"nv\">$MYSQL_DATA_DIR</span>/<span class=\"k\">${</span><span class=\"nv\">DB</span><span class=\"k\">}</span>/<span class=\"k\">${</span><span class=\"nv\">TB</span><span class=\"k\">}</span>.ibd | <span class=\"nb\">awk</span> <span class=\"nt\">-F</span><span class=\"s1\">' '</span> <span class=\"s1\">'{print $5}'</span><span class=\"sb\">`</span>    <span class=\"nv\">TBLSIZE_INFO</span><span class=\"o\">=</span><span class=\"sb\">`</span>mysql <span class=\"k\">${</span><span class=\"nv\">MYSQL_CONN</span><span class=\"k\">}</span> <span class=\"nt\">-ANe</span><span class=\"s2\">\"</span><span class=\"k\">${</span><span class=\"nv\">SQL</span><span class=\"k\">}</span><span class=\"s2\">\"</span><span class=\"sb\">`</span>    <span class=\"nv\">TBLSIZE_FRAG</span><span class=\"o\">=</span><span class=\"k\">$((</span><span class=\"nv\">$TBLSIZE_OPER</span> <span class=\"o\">-</span> <span class=\"nv\">$TBLSIZE_INFO</span><span class=\"k\">))</span>    <span class=\"nv\">TBLSIZE_FRAG_GB</span><span class=\"o\">=</span><span class=\"k\">$((</span><span class=\"nv\">$TBLSIZE_FRAG</span> <span class=\"o\">/</span> <span class=\"m\">1073741824</span><span class=\"k\">))</span>    <span class=\"nb\">echo</span> <span class=\"k\">${</span><span class=\"nv\">TB</span><span class=\"k\">}</span> <span class=\"k\">${</span><span class=\"nv\">TBLSIZE_FRAG_GB</span><span class=\"k\">}</span>    <span class=\"k\">done</span></code></pre></figure><h2 id=\"execution\">Execution</h2><p>Its better to create the script as a shell file and print the output in a file.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">./script.sh <span class=\"o\">&gt;</span> output.txt</code></pre></figure>",
            "url": "/2019/10/21/mysql-calculate-how-much-disk-space-you-wasted",
            "image": "/assets/MySQL Calculate How Much Disk Space You Wasted.jpg",
            
            
            "tags": ["mysql","shell","script","linux"],
            
            "date_published": "2019-10-21T05:13:00+00:00",
            "date_modified": "2019-10-21T05:13:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2019/10/17/backfill-failed-delivery-from-kinesis-to-redshift-with-lambda",
            "title": "BackFill Failed Delivery From Kinesis To RedShift With Lambda",
            "summary": "Automatically backfill failed delivery from kinesis firehose to Redshift using AWS lambda with boto3 and psycopg2",
            "content_text": "If you are dealing with the realtime data stream from Kinesis to RedShift, then you may face this situation where Redshift was down due to some maintenance activity and kinesis firehose was not able to ingest the data. But it has awesome features to retry after the next 60 Minutes. I had a situation that, there was a password change on the RedShift cluster on staging infra and we didn’t notice that the data load was failing. Now we need to backfill all those data into redshift using manifest files. One simple way, list all the files in errors manifest folders and generate the copy command and run those commands in loop or import as a .sql file. But I wanted to automate this process with lambda.Why Lambda?I know it has 15mins of total runtime, but if you have fewer files then its good to use lambda. It’s serverless this is the only reason I choose lambda. Alternatively, you can use shell scripts with aws cli or any other language to process this.Solution Overview:Lambda will put the failed deliveries manifest files in a directory called errors. The files are proper in a partitioned way error/manifest/yyyy/mm/dd/hh/  Lambda should read all the manifest files for importing.(no manual process for mention the location every time)  list-objects-v2 will not return more than 1000 files, so we used paginate to loop this and get all the objects.  Redshift’s password is encrypted with KMS.  Lambda needs psychopg2 to access Redshift, but the official version will not support redshift. We used a custom compiled version of psychopg2.  Once you imported the data with a manifest file, the next execution should not load the same file again and again. So we are moving the file once it’s imported.  I’m a great fan of track everything into a metadata table. So every import process will be tracked along with what COPY command it used.Lambda Setup:  If you are thinking to launch lambda outside the VPC, then please don’t consider this blog.  lambda needs to access KMS to decrypt the password. Its mandatory that the subnets which you are going to use launch the Lambda should have the NAT Gateway.  Create a KMS key for the region where you are going to create this lambda function.  Add the following variables in Lambda’s environment variables.            Variable Name      Value                  REDSHIFT_DATABASE      Your database name              REDSHIFT_TABLE      Your table name to import the data              REDSHIFT_USER      Redshift User Name              REDSHIFT_PASSWD      Redshift user’s password              REDSHIFT_PORT      Redshift port              REDSHIFT_ENDPOINT      Redshift Endpoint              REDSHIFT_IAMROLE      IAM role to access S3 inside Redshfit              SOURCE_BUCKET      Bucket name where you have the manifest file              SOURCE_PREFIX      Location of the error manifest files              TARGET_PREFIX      Location where to move the loaded manifest files        For this blog, I just encrypt the password only. Under encryption, configuration checks the Enable helpers for encryption in transit and Use a customer master key. Choose the KMS key which you created for this.  Then you can see a button called Encrypt on all the environment variables. Just click encrypt on the password variable.  Lambda’s IAM role should have the predefined policy AWSLambdaVPCAccessExecutionRole , AWSLambdaBasicExecutionRole and one custom policy to access the KMS for decrypting it.        {            \"Version\": \"2012-10-17\",            \"Statement\": [                {                    \"Sid\": \"VisualEditor0\",                    \"Effect\": \"Allow\",                    \"Action\": \"kms:Decrypt\",                    \"Resource\": \"&lt;your-kms-arn&gt;\"                }            ]        }  From the network choose the VPC and subnets(must be attached with NAT/NAT Gateway) and a security group where all traffic allowed to its own ID.  Make sure the redshift cluster’s security group should accept the connections from the lambda subnet IP range.  128MB Memory fine for me, but this memory and the timeout can be configured as per your workload.Code for the Function:import osimport boto3import psycopg2import sysfrom base64 import b64decodefrom datetime import datetimes3  = boto3.client('s3')kms = boto3.client('kms')# Get values from EnvREDSHIFT_DATABASE = os.environ['REDSHIFT_DATABASE']REDSHIFT_TABLE    = os.environ['REDSHIFT_TABLE']REDSHIFT_USER     = os.environ['REDSHIFT_USER']REDSHIFT_PASSWD   = os.environ['REDSHIFT_PASSWD']REDSHIFT_PORT     = os.environ['REDSHIFT_PORT']REDSHIFT_ENDPOINT = os.environ['REDSHIFT_ENDPOINT']REDSHIFT_CLUSTER  = os.environ['REDSHIFT_CLUSTER']REDSHIFT_IAMROLE  = os.environ['REDSHIFT_IAMROLE']DE_PASS = kms.decrypt(CiphertextBlob=b64decode(REDSHIFT_PASSWD))['Plaintext']# Declare other parametersTRIGGER_TIME  = str(datetime.now())SOURCE_BUCKET = os.environ['SOURCE_BUCKET']SOURCE_PREFIX = os.environ['SOURCE_PREFIX']TARGET_PREFIX = os.environ['TARGET_PREFIX']# Define the Functions\"\"\"Function 1: Get all manifest filesThis fuction is written by alexwlchan(https://alexwlchan.net/2019/07/listing-s3-keys/)list_objects_v2  won't support more than 1000 files,so it'll paginate to next 1000 and so on.\"\"\"def get_matching_s3_objects(bucket, prefix=\"\", suffix=\"\"):    paginator = s3.get_paginator(\"list_objects_v2\")    kwargs = {'Bucket': bucket}    if isinstance(prefix, str):        prefixes = (prefix, )    else:        prefixes = prefix    for key_prefix in prefixes:        kwargs[\"Prefix\"] = key_prefix        for page in paginator.paginate(**kwargs):            try:                contents = page[\"Contents\"]            except KeyError:                return            for obj in contents:                key = obj[\"Key\"]                if key.endswith(suffix):                    yield objdef get_matching_s3_keys(bucket, prefix=\"\", suffix=\"\"):    for obj in get_matching_s3_objects(bucket, prefix, suffix):        yield obj[\"Key\"]\"\"\"Function 2: Connection string for RedShiftIts using a custom complied psycopg2https://github.com/jkehler/awslambda-psycopg2\"\"\"         def get_pg_con(    user=REDSHIFT_USER,    password=DE_PASS.decode(\"utf-8\"),    host=REDSHIFT_ENDPOINT,    dbname=REDSHIFT_DATABASE,    port=REDSHIFT_PORT,    ):    return psycopg2.connect(dbname=dbname,       host=host,       user=user,      password=password,      port=port)\"\"\"Function 3: Main function\"\"\"def run_handler(handler, context):    all_files = get_matching_s3_keys(SOURCE_BUCKET, SOURCE_PREFIX)    for file in all_files:      source_file = {'Bucket': SOURCE_BUCKET,'Key': file}      target_file = TARGET_PREFIX + str(file) + \".done\"      print (SOURCE_BUCKET)      print (SOURCE_PREFIX)      print (file)      #Process starting here      start_time = str(datetime.now())      copy_command=\"COPY \" + REDSHIFT_TABLE + \" FROM 's3://\" + SOURCE_BUCKET + \"/\" + file + \"' iam_role '\" + REDSHIFT_IAMROLE + \"' MANIFEST json 'auto' GZIP;\"      conn = get_pg_con()      cur = conn.cursor()      #print (copy_command) - For debug       cur.execute(copy_command)            #Insert to History Table      end_time = str(datetime.now())      history_insert=\"insert into error_copy_history (TRIGGER_TIME,start_time,end_time,db_name,table_name,file) values ( '\" + TRIGGER_TIME + \"','\"  + start_time + \"','\"  + REDSHIFT_DATABASE + \"','\"  + db_name + \"','\"  + table_name + \"','s3://floweraura-rawdata/\"  + file +\"');\"      cur.execute(history_insert)            #Commit and Close      conn.commit()      cur.close()      conn.close()            #Move the files from Errors directory to processed directory      s3.copy(source_file, SOURCE_BUCKET, target_file)      print (\"copied\", file)      s3.delete_object(Bucket=SOURCE_BUCKET,Key=file)      print (\"deleted\", file)How to Deploy it?As I mentioned above, you need to use the custom complied psychopg2 which you can download from the below link.https://github.com/jkehler/awslambda-psycopg2I’m using Python 3.6 on Lambda. So download this repo and rename the psycopg2-3.6 to psycopg2. And then create a file with name pgcode.py and paste the above python code.Now create a zip file with the psycopg2 and pgcode.py upload this file to lambda. In the lambda Handler use pgcode.run_handlerThat’s it, your lambda function is ready, not really ready to execute.Create the History Table:To maintain this import process in a table, we need to create a table in RedShift.    CREATE TABLE error_copy_history       (          pid          INT IDENTITY(1, 1),          trigger_time DATETIME,          start_time   DATETIME,          end_time     DATETIME,          db_name      VARCHAR(100),          table_name   VARCHAR(100),          FILE         VARCHAR(65000)       ); Run the Function:Im giving my s3 path and lambda environment variables here for your reference.  S3 bucket - bhuvi-datalake Here Im storing all the kinesis data.  S3 prefix - kinesis/errors/ Failed manifest files will go to this path (eg: kinesis/errors/2019/10/21/13/  Target prefix - kinesis/processed/ Once the data imported to Redshift the loaded manifest file will go to this location.Once the execution was done, you can see the load history from the History table.    bhuvi=# select * from error_copy_history limit 1;    pid          | 260    trigger_time | 2019-10-17 08:14:23.495213    start_time   | 2019-10-17 08:14:24.59309    end_time     | 2019-10-17 08:14:24.917248    db_name      | bhuvi    table_name   | s3cp    file         | s3://bhuvi-datalake/errors/manifests/2019/09/26/13/collect-redshift-2019-09-26-13-21-49-56371982-2375-4b28-8d79-45f01952667eFurther Customization:  I ran this on Ad-Hoc basis, but if you want to run this automatically, then use Cloudwatch triggers to trigger this on daily or any N internal.  I used KMS for encrypting the password, you can use IAM temporary credentials also.  My S3 data is compressed and JSON format. If you have different file format and compression then modify your COPY command in the python code.Related interesting Reading:  Get the email notification when kinesis failed to import the data into RedShift.  List keys in S3 more than 1000 with list-objects-v2 include Prefix and Suffix.  Psycopg2 - custom compiler for Python 2.7, 3.6, 3.7",
            "content_html": "<p>If you are dealing with the realtime data stream from Kinesis to RedShift, then you may face this situation where Redshift was down due to some maintenance activity and kinesis firehose was not able to ingest the data. But it has awesome features to retry after the next 60 Minutes. I had a situation that, there was a password change on the RedShift cluster on staging infra and we didn’t notice that the data load was failing. Now we need to backfill all those data into redshift using manifest files. One simple way, list all the files in errors manifest folders and generate the copy command and run those commands in loop or import as a <code class=\"language-html highlighter-rouge\">.sql</code> file. But I wanted to automate this process with lambda.</p><h2 id=\"why-lambda\">Why Lambda?</h2><p>I know it has 15mins of total runtime, but if you have fewer files then its good to use lambda. It’s serverless this is the only reason I choose lambda. Alternatively, you can use shell scripts with <code class=\"language-html highlighter-rouge\">aws cli</code> or any other language to process this.</p><h2 id=\"solution-overview\">Solution Overview:</h2><p>Lambda will put the failed deliveries manifest files in a directory called <code class=\"language-html highlighter-rouge\">errors</code>. The files are proper in a partitioned way <code class=\"language-html highlighter-rouge\">error/manifest/yyyy/mm/dd/hh/</code></p><ol>  <li>Lambda should read all the manifest files for importing.(no manual process for mention the location every time)</li>  <li>list-objects-v2 will not return more than 1000 files, so we used <code class=\"language-html highlighter-rouge\">paginate</code> to loop this and get all the objects.</li>  <li>Redshift’s password is encrypted with KMS.</li>  <li>Lambda needs <code class=\"language-html highlighter-rouge\">psychopg2</code> to access Redshift, but the official version will not support redshift. We used a custom compiled version of psychopg2.</li>  <li>Once you imported the data with a manifest file, the next execution should not load the same file again and again. So we are moving the file once it’s imported.</li>  <li>I’m a great fan of track everything into a metadata table. So every import process will be tracked along with what <code class=\"language-html highlighter-rouge\">COPY</code> command it used.</li></ol><h2 id=\"lambda-setup\">Lambda Setup:</h2><ul>  <li>If you are thinking to launch lambda outside the VPC, then please don’t consider this blog.</li>  <li>lambda needs to access KMS to decrypt the password. Its mandatory that the subnets which you are going to use launch the Lambda should have the NAT Gateway.</li>  <li>Create a KMS key for the region where you are going to create this lambda function.</li>  <li>Add the following variables in Lambda’s environment variables.</li></ul><table>  <thead>    <tr>      <th>Variable Name</th>      <th>Value</th>    </tr>  </thead>  <tbody>    <tr>      <td>REDSHIFT_DATABASE</td>      <td>Your database name</td>    </tr>    <tr>      <td>REDSHIFT_TABLE</td>      <td>Your table name to import the data</td>    </tr>    <tr>      <td>REDSHIFT_USER</td>      <td>Redshift User Name</td>    </tr>    <tr>      <td>REDSHIFT_PASSWD</td>      <td>Redshift user’s password</td>    </tr>    <tr>      <td>REDSHIFT_PORT</td>      <td>Redshift port</td>    </tr>    <tr>      <td>REDSHIFT_ENDPOINT</td>      <td>Redshift Endpoint</td>    </tr>    <tr>      <td>REDSHIFT_IAMROLE</td>      <td>IAM role to access S3 inside Redshfit</td>    </tr>    <tr>      <td>SOURCE_BUCKET</td>      <td>Bucket name where you have the manifest file</td>    </tr>    <tr>      <td>SOURCE_PREFIX</td>      <td>Location of the error manifest files</td>    </tr>    <tr>      <td>TARGET_PREFIX</td>      <td>Location where to move the loaded manifest files</td>    </tr>  </tbody></table><ul>  <li>For this blog, I just encrypt the password only. Under encryption, configuration checks the <code class=\"language-html highlighter-rouge\">Enable helpers for encryption in transit</code> and <code class=\"language-html highlighter-rouge\">Use a customer master key</code>. Choose the KMS key which you created for this.</li>  <li>Then you can see a button called <code class=\"language-html highlighter-rouge\">Encrypt</code> on all the environment variables. Just click encrypt on the password variable.</li>  <li>Lambda’s IAM role should have the predefined policy <code class=\"language-html highlighter-rouge\">AWSLambdaVPCAccessExecutionRole</code> , <code class=\"language-html highlighter-rouge\">AWSLambdaBasicExecutionRole</code> and one custom policy to access the KMS for decrypting it.</li></ul><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">        <span class=\"o\">{</span>            <span class=\"s2\">\"Version\"</span>: <span class=\"s2\">\"2012-10-17\"</span>,            <span class=\"s2\">\"Statement\"</span>: <span class=\"o\">[</span>                <span class=\"o\">{</span>                    <span class=\"s2\">\"Sid\"</span>: <span class=\"s2\">\"VisualEditor0\"</span>,                    <span class=\"s2\">\"Effect\"</span>: <span class=\"s2\">\"Allow\"</span>,                    <span class=\"s2\">\"Action\"</span>: <span class=\"s2\">\"kms:Decrypt\"</span>,                    <span class=\"s2\">\"Resource\"</span>: <span class=\"s2\">\"&lt;your-kms-arn&gt;\"</span>                <span class=\"o\">}</span>            <span class=\"o\">]</span>        <span class=\"o\">}</span></code></pre></figure><ul>  <li>From the network choose the VPC and subnets(must be attached with NAT/NAT Gateway) and a security group where all traffic allowed to its own ID.</li>  <li>Make sure the redshift cluster’s security group should accept the connections from the lambda subnet IP range.</li>  <li><code class=\"language-html highlighter-rouge\">128MB</code> Memory fine for me, but this memory and the timeout can be configured as per your workload.</li></ul><h2 id=\"code-for-the-function\">Code for the Function:</h2><figure class=\"highlight\"><pre><code class=\"language-python\" data-lang=\"python\"><span class=\"kn\">import</span> <span class=\"nn\">os</span><span class=\"kn\">import</span> <span class=\"nn\">boto3</span><span class=\"kn\">import</span> <span class=\"nn\">psycopg2</span><span class=\"kn\">import</span> <span class=\"nn\">sys</span><span class=\"kn\">from</span> <span class=\"nn\">base64</span> <span class=\"kn\">import</span> <span class=\"n\">b64decode</span><span class=\"kn\">from</span> <span class=\"nn\">datetime</span> <span class=\"kn\">import</span> <span class=\"n\">datetime</span><span class=\"n\">s3</span>  <span class=\"o\">=</span> <span class=\"n\">boto3</span><span class=\"p\">.</span><span class=\"n\">client</span><span class=\"p\">(</span><span class=\"s\">'s3'</span><span class=\"p\">)</span><span class=\"n\">kms</span> <span class=\"o\">=</span> <span class=\"n\">boto3</span><span class=\"p\">.</span><span class=\"n\">client</span><span class=\"p\">(</span><span class=\"s\">'kms'</span><span class=\"p\">)</span><span class=\"c1\"># Get values from Env</span><span class=\"n\">REDSHIFT_DATABASE</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s\">'REDSHIFT_DATABASE'</span><span class=\"p\">]</span><span class=\"n\">REDSHIFT_TABLE</span>    <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s\">'REDSHIFT_TABLE'</span><span class=\"p\">]</span><span class=\"n\">REDSHIFT_USER</span>     <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s\">'REDSHIFT_USER'</span><span class=\"p\">]</span><span class=\"n\">REDSHIFT_PASSWD</span>   <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s\">'REDSHIFT_PASSWD'</span><span class=\"p\">]</span><span class=\"n\">REDSHIFT_PORT</span>     <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s\">'REDSHIFT_PORT'</span><span class=\"p\">]</span><span class=\"n\">REDSHIFT_ENDPOINT</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s\">'REDSHIFT_ENDPOINT'</span><span class=\"p\">]</span><span class=\"n\">REDSHIFT_CLUSTER</span>  <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s\">'REDSHIFT_CLUSTER'</span><span class=\"p\">]</span><span class=\"n\">REDSHIFT_IAMROLE</span>  <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s\">'REDSHIFT_IAMROLE'</span><span class=\"p\">]</span><span class=\"n\">DE_PASS</span> <span class=\"o\">=</span> <span class=\"n\">kms</span><span class=\"p\">.</span><span class=\"n\">decrypt</span><span class=\"p\">(</span><span class=\"n\">CiphertextBlob</span><span class=\"o\">=</span><span class=\"n\">b64decode</span><span class=\"p\">(</span><span class=\"n\">REDSHIFT_PASSWD</span><span class=\"p\">))[</span><span class=\"s\">'Plaintext'</span><span class=\"p\">]</span><span class=\"c1\"># Declare other parameters</span><span class=\"n\">TRIGGER_TIME</span>  <span class=\"o\">=</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">datetime</span><span class=\"p\">.</span><span class=\"n\">now</span><span class=\"p\">())</span><span class=\"n\">SOURCE_BUCKET</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s\">'SOURCE_BUCKET'</span><span class=\"p\">]</span><span class=\"n\">SOURCE_PREFIX</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s\">'SOURCE_PREFIX'</span><span class=\"p\">]</span><span class=\"n\">TARGET_PREFIX</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s\">'TARGET_PREFIX'</span><span class=\"p\">]</span><span class=\"c1\"># Define the Functions</span><span class=\"s\">\"\"\"Function 1: Get all manifest filesThis fuction is written by alexwlchan(https://alexwlchan.net/2019/07/listing-s3-keys/)list_objects_v2  won't support more than 1000 files,so it'll paginate to next 1000 and so on.\"\"\"</span><span class=\"k\">def</span> <span class=\"nf\">get_matching_s3_objects</span><span class=\"p\">(</span><span class=\"n\">bucket</span><span class=\"p\">,</span> <span class=\"n\">prefix</span><span class=\"o\">=</span><span class=\"s\">\"\"</span><span class=\"p\">,</span> <span class=\"n\">suffix</span><span class=\"o\">=</span><span class=\"s\">\"\"</span><span class=\"p\">):</span>    <span class=\"n\">paginator</span> <span class=\"o\">=</span> <span class=\"n\">s3</span><span class=\"p\">.</span><span class=\"n\">get_paginator</span><span class=\"p\">(</span><span class=\"s\">\"list_objects_v2\"</span><span class=\"p\">)</span>    <span class=\"n\">kwargs</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s\">'Bucket'</span><span class=\"p\">:</span> <span class=\"n\">bucket</span><span class=\"p\">}</span>    <span class=\"k\">if</span> <span class=\"nb\">isinstance</span><span class=\"p\">(</span><span class=\"n\">prefix</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">):</span>        <span class=\"n\">prefixes</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">prefix</span><span class=\"p\">,</span> <span class=\"p\">)</span>    <span class=\"k\">else</span><span class=\"p\">:</span>        <span class=\"n\">prefixes</span> <span class=\"o\">=</span> <span class=\"n\">prefix</span>    <span class=\"k\">for</span> <span class=\"n\">key_prefix</span> <span class=\"ow\">in</span> <span class=\"n\">prefixes</span><span class=\"p\">:</span>        <span class=\"n\">kwargs</span><span class=\"p\">[</span><span class=\"s\">\"Prefix\"</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">key_prefix</span>        <span class=\"k\">for</span> <span class=\"n\">page</span> <span class=\"ow\">in</span> <span class=\"n\">paginator</span><span class=\"p\">.</span><span class=\"n\">paginate</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">):</span>            <span class=\"k\">try</span><span class=\"p\">:</span>                <span class=\"n\">contents</span> <span class=\"o\">=</span> <span class=\"n\">page</span><span class=\"p\">[</span><span class=\"s\">\"Contents\"</span><span class=\"p\">]</span>            <span class=\"k\">except</span> <span class=\"nb\">KeyError</span><span class=\"p\">:</span>                <span class=\"k\">return</span>            <span class=\"k\">for</span> <span class=\"n\">obj</span> <span class=\"ow\">in</span> <span class=\"n\">contents</span><span class=\"p\">:</span>                <span class=\"n\">key</span> <span class=\"o\">=</span> <span class=\"n\">obj</span><span class=\"p\">[</span><span class=\"s\">\"Key\"</span><span class=\"p\">]</span>                <span class=\"k\">if</span> <span class=\"n\">key</span><span class=\"p\">.</span><span class=\"n\">endswith</span><span class=\"p\">(</span><span class=\"n\">suffix</span><span class=\"p\">):</span>                    <span class=\"k\">yield</span> <span class=\"n\">obj</span><span class=\"k\">def</span> <span class=\"nf\">get_matching_s3_keys</span><span class=\"p\">(</span><span class=\"n\">bucket</span><span class=\"p\">,</span> <span class=\"n\">prefix</span><span class=\"o\">=</span><span class=\"s\">\"\"</span><span class=\"p\">,</span> <span class=\"n\">suffix</span><span class=\"o\">=</span><span class=\"s\">\"\"</span><span class=\"p\">):</span>    <span class=\"k\">for</span> <span class=\"n\">obj</span> <span class=\"ow\">in</span> <span class=\"n\">get_matching_s3_objects</span><span class=\"p\">(</span><span class=\"n\">bucket</span><span class=\"p\">,</span> <span class=\"n\">prefix</span><span class=\"p\">,</span> <span class=\"n\">suffix</span><span class=\"p\">):</span>        <span class=\"k\">yield</span> <span class=\"n\">obj</span><span class=\"p\">[</span><span class=\"s\">\"Key\"</span><span class=\"p\">]</span><span class=\"s\">\"\"\"Function 2: Connection string for RedShiftIts using a custom complied psycopg2https://github.com/jkehler/awslambda-psycopg2\"\"\"</span>         <span class=\"k\">def</span> <span class=\"nf\">get_pg_con</span><span class=\"p\">(</span>    <span class=\"n\">user</span><span class=\"o\">=</span><span class=\"n\">REDSHIFT_USER</span><span class=\"p\">,</span>    <span class=\"n\">password</span><span class=\"o\">=</span><span class=\"n\">DE_PASS</span><span class=\"p\">.</span><span class=\"n\">decode</span><span class=\"p\">(</span><span class=\"s\">\"utf-8\"</span><span class=\"p\">),</span>    <span class=\"n\">host</span><span class=\"o\">=</span><span class=\"n\">REDSHIFT_ENDPOINT</span><span class=\"p\">,</span>    <span class=\"n\">dbname</span><span class=\"o\">=</span><span class=\"n\">REDSHIFT_DATABASE</span><span class=\"p\">,</span>    <span class=\"n\">port</span><span class=\"o\">=</span><span class=\"n\">REDSHIFT_PORT</span><span class=\"p\">,</span>    <span class=\"p\">):</span>    <span class=\"k\">return</span> <span class=\"n\">psycopg2</span><span class=\"p\">.</span><span class=\"n\">connect</span><span class=\"p\">(</span><span class=\"n\">dbname</span><span class=\"o\">=</span><span class=\"n\">dbname</span><span class=\"p\">,</span>       <span class=\"n\">host</span><span class=\"o\">=</span><span class=\"n\">host</span><span class=\"p\">,</span>       <span class=\"n\">user</span><span class=\"o\">=</span><span class=\"n\">user</span><span class=\"p\">,</span>      <span class=\"n\">password</span><span class=\"o\">=</span><span class=\"n\">password</span><span class=\"p\">,</span>      <span class=\"n\">port</span><span class=\"o\">=</span><span class=\"n\">port</span><span class=\"p\">)</span><span class=\"s\">\"\"\"Function 3: Main function\"\"\"</span><span class=\"k\">def</span> <span class=\"nf\">run_handler</span><span class=\"p\">(</span><span class=\"n\">handler</span><span class=\"p\">,</span> <span class=\"n\">context</span><span class=\"p\">):</span>    <span class=\"n\">all_files</span> <span class=\"o\">=</span> <span class=\"n\">get_matching_s3_keys</span><span class=\"p\">(</span><span class=\"n\">SOURCE_BUCKET</span><span class=\"p\">,</span> <span class=\"n\">SOURCE_PREFIX</span><span class=\"p\">)</span>    <span class=\"k\">for</span> <span class=\"nb\">file</span> <span class=\"ow\">in</span> <span class=\"n\">all_files</span><span class=\"p\">:</span>      <span class=\"n\">source_file</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s\">'Bucket'</span><span class=\"p\">:</span> <span class=\"n\">SOURCE_BUCKET</span><span class=\"p\">,</span><span class=\"s\">'Key'</span><span class=\"p\">:</span> <span class=\"nb\">file</span><span class=\"p\">}</span>      <span class=\"n\">target_file</span> <span class=\"o\">=</span> <span class=\"n\">TARGET_PREFIX</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"nb\">file</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"s\">\".done\"</span>      <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"n\">SOURCE_BUCKET</span><span class=\"p\">)</span>      <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"n\">SOURCE_PREFIX</span><span class=\"p\">)</span>      <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"nb\">file</span><span class=\"p\">)</span>      <span class=\"c1\">#Process starting here</span>      <span class=\"n\">start_time</span> <span class=\"o\">=</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">datetime</span><span class=\"p\">.</span><span class=\"n\">now</span><span class=\"p\">())</span>      <span class=\"n\">copy_command</span><span class=\"o\">=</span><span class=\"s\">\"COPY \"</span> <span class=\"o\">+</span> <span class=\"n\">REDSHIFT_TABLE</span> <span class=\"o\">+</span> <span class=\"s\">\" FROM 's3://\"</span> <span class=\"o\">+</span> <span class=\"n\">SOURCE_BUCKET</span> <span class=\"o\">+</span> <span class=\"s\">\"/\"</span> <span class=\"o\">+</span> <span class=\"nb\">file</span> <span class=\"o\">+</span> <span class=\"s\">\"' iam_role '\"</span> <span class=\"o\">+</span> <span class=\"n\">REDSHIFT_IAMROLE</span> <span class=\"o\">+</span> <span class=\"s\">\"' MANIFEST json 'auto' GZIP;\"</span>      <span class=\"n\">conn</span> <span class=\"o\">=</span> <span class=\"n\">get_pg_con</span><span class=\"p\">()</span>      <span class=\"n\">cur</span> <span class=\"o\">=</span> <span class=\"n\">conn</span><span class=\"p\">.</span><span class=\"n\">cursor</span><span class=\"p\">()</span>      <span class=\"c1\">#print (copy_command) - For debug </span>      <span class=\"n\">cur</span><span class=\"p\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"n\">copy_command</span><span class=\"p\">)</span>            <span class=\"c1\">#Insert to History Table</span>      <span class=\"n\">end_time</span> <span class=\"o\">=</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">datetime</span><span class=\"p\">.</span><span class=\"n\">now</span><span class=\"p\">())</span>      <span class=\"n\">history_insert</span><span class=\"o\">=</span><span class=\"s\">\"insert into error_copy_history (TRIGGER_TIME,start_time,end_time,db_name,table_name,file) values ( '\"</span> <span class=\"o\">+</span> <span class=\"n\">TRIGGER_TIME</span> <span class=\"o\">+</span> <span class=\"s\">\"','\"</span>  <span class=\"o\">+</span> <span class=\"n\">start_time</span> <span class=\"o\">+</span> <span class=\"s\">\"','\"</span>  <span class=\"o\">+</span> <span class=\"n\">REDSHIFT_DATABASE</span> <span class=\"o\">+</span> <span class=\"s\">\"','\"</span>  <span class=\"o\">+</span> <span class=\"n\">db_name</span> <span class=\"o\">+</span> <span class=\"s\">\"','\"</span>  <span class=\"o\">+</span> <span class=\"n\">table_name</span> <span class=\"o\">+</span> <span class=\"s\">\"','s3://floweraura-rawdata/\"</span>  <span class=\"o\">+</span> <span class=\"nb\">file</span> <span class=\"o\">+</span><span class=\"s\">\"');\"</span>      <span class=\"n\">cur</span><span class=\"p\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"n\">history_insert</span><span class=\"p\">)</span>            <span class=\"c1\">#Commit and Close</span>      <span class=\"n\">conn</span><span class=\"p\">.</span><span class=\"n\">commit</span><span class=\"p\">()</span>      <span class=\"n\">cur</span><span class=\"p\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>      <span class=\"n\">conn</span><span class=\"p\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>            <span class=\"c1\">#Move the files from Errors directory to processed directory</span>      <span class=\"n\">s3</span><span class=\"p\">.</span><span class=\"n\">copy</span><span class=\"p\">(</span><span class=\"n\">source_file</span><span class=\"p\">,</span> <span class=\"n\">SOURCE_BUCKET</span><span class=\"p\">,</span> <span class=\"n\">target_file</span><span class=\"p\">)</span>      <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s\">\"copied\"</span><span class=\"p\">,</span> <span class=\"nb\">file</span><span class=\"p\">)</span>      <span class=\"n\">s3</span><span class=\"p\">.</span><span class=\"n\">delete_object</span><span class=\"p\">(</span><span class=\"n\">Bucket</span><span class=\"o\">=</span><span class=\"n\">SOURCE_BUCKET</span><span class=\"p\">,</span><span class=\"n\">Key</span><span class=\"o\">=</span><span class=\"nb\">file</span><span class=\"p\">)</span>      <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s\">\"deleted\"</span><span class=\"p\">,</span> <span class=\"nb\">file</span><span class=\"p\">)</span></code></pre></figure><h2 id=\"how-to-deploy-it\">How to Deploy it?</h2><p>As I mentioned above, you need to use the custom complied psychopg2 which you can download from the below link.</p><p><a href=\"https://github.com/jkehler/awslambda-psycopg2\" title=\"https://github.com/jkehler/awslambda-psycopg2\">https://github.com/jkehler/awslambda-psycopg2</a></p><p>I’m using <code class=\"language-html highlighter-rouge\">Python 3.6</code> on Lambda. So download this repo and rename the <code class=\"language-html highlighter-rouge\">psycopg2-3.6</code> to <code class=\"language-html highlighter-rouge\">psycopg2</code>. And then create a file with name <code class=\"language-html highlighter-rouge\">pgcode.py</code> and paste the above python code.</p><p>Now create a zip file with the <code class=\"language-html highlighter-rouge\">psycopg2</code> and <code class=\"language-html highlighter-rouge\">pgcode.py</code> upload this file to lambda. In the <code class=\"language-html highlighter-rouge\">lambda Handler</code> use <code class=\"language-html highlighter-rouge\">pgcode.run_handler</code></p><p><img src=\"/assets/BackFill Failed Delivery From Kinesis To RedShift With Lambda2.jpg\" alt=\"\" /></p><p>That’s it, your lambda function is ready, not really ready to execute.</p><h2 id=\"create-the-history-table\">Create the History Table:</h2><p>To maintain this import process in a table, we need to create a table in RedShift.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\">    <span class=\"k\">CREATE</span> <span class=\"k\">TABLE</span> <span class=\"n\">error_copy_history</span>       <span class=\"p\">(</span>          <span class=\"n\">pid</span>          <span class=\"nb\">INT</span> <span class=\"k\">IDENTITY</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span>          <span class=\"n\">trigger_time</span> <span class=\"nb\">DATETIME</span><span class=\"p\">,</span>          <span class=\"n\">start_time</span>   <span class=\"nb\">DATETIME</span><span class=\"p\">,</span>          <span class=\"n\">end_time</span>     <span class=\"nb\">DATETIME</span><span class=\"p\">,</span>          <span class=\"n\">db_name</span>      <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">),</span>          <span class=\"k\">table_name</span>   <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">),</span>          <span class=\"n\">FILE</span>         <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">65000</span><span class=\"p\">)</span>       <span class=\"p\">);</span> </code></pre></figure><h2 id=\"run-the-function\">Run the Function:</h2><p>Im giving my s3 path and lambda environment variables here for your reference.</p><ul>  <li>S3 bucket - <code class=\"language-html highlighter-rouge\">bhuvi-datalake</code> Here Im storing all the kinesis data.</li>  <li>S3 prefix - <code class=\"language-html highlighter-rouge\">kinesis/errors/</code> Failed manifest files will go to this path (eg: <code class=\"language-html highlighter-rouge\">kinesis/errors/2019/10/21/13/</code></li>  <li>Target prefix - <code class=\"language-html highlighter-rouge\">kinesis/processed/</code> Once the data imported to Redshift the loaded manifest file will go to this location.</li></ul><p><img src=\"/assets/BackFill Failed Delivery From Kinesis To RedShift With Lambda1.jpg\" alt=\"\" /></p><p>Once the execution was done, you can see the load history from the History table.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\">    <span class=\"n\">bhuvi</span><span class=\"o\">=#</span> <span class=\"k\">select</span> <span class=\"o\">*</span> <span class=\"k\">from</span> <span class=\"n\">error_copy_history</span> <span class=\"k\">limit</span> <span class=\"mi\">1</span><span class=\"p\">;</span>    <span class=\"n\">pid</span>          <span class=\"o\">|</span> <span class=\"mi\">260</span>    <span class=\"n\">trigger_time</span> <span class=\"o\">|</span> <span class=\"mi\">2019</span><span class=\"o\">-</span><span class=\"mi\">10</span><span class=\"o\">-</span><span class=\"mi\">17</span> <span class=\"mi\">08</span><span class=\"p\">:</span><span class=\"mi\">14</span><span class=\"p\">:</span><span class=\"mi\">23</span><span class=\"p\">.</span><span class=\"mi\">495213</span>    <span class=\"n\">start_time</span>   <span class=\"o\">|</span> <span class=\"mi\">2019</span><span class=\"o\">-</span><span class=\"mi\">10</span><span class=\"o\">-</span><span class=\"mi\">17</span> <span class=\"mi\">08</span><span class=\"p\">:</span><span class=\"mi\">14</span><span class=\"p\">:</span><span class=\"mi\">24</span><span class=\"p\">.</span><span class=\"mi\">59309</span>    <span class=\"n\">end_time</span>     <span class=\"o\">|</span> <span class=\"mi\">2019</span><span class=\"o\">-</span><span class=\"mi\">10</span><span class=\"o\">-</span><span class=\"mi\">17</span> <span class=\"mi\">08</span><span class=\"p\">:</span><span class=\"mi\">14</span><span class=\"p\">:</span><span class=\"mi\">24</span><span class=\"p\">.</span><span class=\"mi\">917248</span>    <span class=\"n\">db_name</span>      <span class=\"o\">|</span> <span class=\"n\">bhuvi</span>    <span class=\"k\">table_name</span>   <span class=\"o\">|</span> <span class=\"n\">s3cp</span>    <span class=\"n\">file</span>         <span class=\"o\">|</span> <span class=\"n\">s3</span><span class=\"p\">:</span><span class=\"o\">//</span><span class=\"n\">bhuvi</span><span class=\"o\">-</span><span class=\"n\">datalake</span><span class=\"o\">/</span><span class=\"n\">errors</span><span class=\"o\">/</span><span class=\"n\">manifests</span><span class=\"o\">/</span><span class=\"mi\">2019</span><span class=\"o\">/</span><span class=\"mi\">09</span><span class=\"o\">/</span><span class=\"mi\">26</span><span class=\"o\">/</span><span class=\"mi\">13</span><span class=\"o\">/</span><span class=\"n\">collect</span><span class=\"o\">-</span><span class=\"n\">redshift</span><span class=\"o\">-</span><span class=\"mi\">2019</span><span class=\"o\">-</span><span class=\"mi\">09</span><span class=\"o\">-</span><span class=\"mi\">26</span><span class=\"o\">-</span><span class=\"mi\">13</span><span class=\"o\">-</span><span class=\"mi\">21</span><span class=\"o\">-</span><span class=\"mi\">49</span><span class=\"o\">-</span><span class=\"mi\">56371982</span><span class=\"o\">-</span><span class=\"mi\">2375</span><span class=\"o\">-</span><span class=\"mi\">4</span><span class=\"n\">b28</span><span class=\"o\">-</span><span class=\"mi\">8</span><span class=\"n\">d79</span><span class=\"o\">-</span><span class=\"mi\">45</span><span class=\"n\">f01952667e</span></code></pre></figure><h2 id=\"further-customization\">Further Customization:</h2><ol>  <li>I ran this on Ad-Hoc basis, but if you want to run this automatically, then use Cloudwatch triggers to trigger this on daily or any N internal.</li>  <li>I used KMS for encrypting the password, you can use IAM temporary credentials also.</li>  <li>My S3 data is compressed and JSON format. If you have different file format and compression then modify your COPY command in the python code.</li></ol><h2 id=\"related-interesting-reading\">Related interesting Reading:</h2><ol>  <li><a href=\"\">Get the email notification when kinesis failed to import the data into RedShift.</a></li>  <li><a href=\"https://alexwlchan.net/2019/07/listing-s3-keys/\">List keys in S3 more than 1000 with list-objects-v2 include Prefix and Suffix.</a></li>  <li><a href=\"https://github.com/jkehler/awslambda-psycopg2\">Psycopg2 - custom compiler for Python 2.7, 3.6, 3.7</a></li></ol>",
            "url": "/2019/10/17/backfill-failed-delivery-from-kinesis-to-redshift-with-lambda",
            "image": "/assets/BackFill Failed Delivery From Kinesis To RedShift With Lambda.jpg",
            
            
            "tags": ["aws","kinesis","firehose","redshift","lambda","python"],
            
            "date_published": "2019-10-17T16:26:00+00:00",
            "date_modified": "2019-10-17T16:26:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2019/10/07/aws-glue-custom-output-file-size-and-fixed-number-of-files",
            "title": "AWS Glue Custom Output File Size And Fixed Number Of Files",
            "summary": "AWS Glue parquet out files in a custom size and set the number of output files. We can use groupFiles and repartition in Glue to achieve this. ",
            "content_text": "AWS Glue is the serverless version of EMR clusters. Many organizations now adopted to use Glue for their day to day BigData workloads. I have written a blog in Searce’s Medium publication for Converting the CSV/JSON files to parquet using AWS Glue. Till now its many people are reading that and implementing on their infra. But many people are commenting about the Glue is producing a huge number for output files(converted Parquet files) in S3, even for converting 100MB of CSV file will produce 500+ Parquet files. we need to customize this output file size and number of files.Why Glue is producing more small files?If you are processing small chunks of files in Glue, it will read then and convert them into DynamicFrames. Glue is running on top of the Spark. So the dynamic frames will be moved to Partitions in the EMR cluster. And the Glue partition the data evenly among all of the nodes for better performance. Once its processed, all the partitions will be  pushing to your target. Each partition will and one file. That’s why we are getting more files.Customize the output files:We can customize it in two ways.  While reading the data from the source.  While writing the data to the target.If you have so many small numbers of files in your source, them Glue process them in many partitions. So we can force the Glue to read multiple file in one shot. Like we are grouping multiple file and the Glue virtually consider this as a single file.Else, once you processed the data, you can repartition the data. So you can mention how many partitions you want. Let’s say if you repartition the data with 5, then it’ll write 5 files in your target.Testing Infra setup:  I have 1GB of test data set.  Format CSV  Split into 20 files.  Each file is 52MB.  Created a Glue crawler on top of this data and its created the table in Glue catalog.  Im using glue to convert this CSV to Parquet. Follow the instructions here: https://medium.com/searce/convert-csv-json-files-to-apache-parquet-using-aws-glue-a760d177b45fOption 1: groupFilesFrom AWS Doc,  You can set properties of your tables to enable an AWS Glue ETL job to group files when they are read from an Amazon S3 data store. These properties enable each ETL task to read a group of input files into a single in-memory partition, this is especially useful when there is a large number of small files in your Amazon S3 data store.groupFiles:Set groupFiles to inPartition to enable the grouping of files within an Amazon S3 data partition. AWS Glue automatically enables grouping if there are more than 50,000 input files.groupSize:Set groupSize to the target size of groups in bytes. The groupSize property is optional, if not provided, AWS Glue calculates a size to use all the CPU cores in the cluster while still reducing the overall number of ETL tasks and in-memory partitions.Go to Glue –&gt; Tables –&gt; select your table –&gt; Edit Table.Unde the table properties, add the following parameters.  groupFiles - inPartition  groupSize - 209715200This will read 200MB data in one partition. Lets run the job and see the output.  Total Number of files: 5  Each file size: 393kbOption 2: groupFiles while reading from S3It’s the same as the previous one, but if you take a look at the datasource, its creating the dynamic frame from the catalog table.datasource0 = glueContext.create_dynamic_frame.from_catalog(database = \"bhuvi\"But if you are directly reading it from S3, you can change the source like below.datasource0 = glueContext.create_dynamic_frame_from_options(\"s3\", {'paths': [\"s3://s3path/\"], 'recurse':True, 'groupFiles': 'inPartition', 'groupSize': '104857600'}, format=\"csv\")Option 3: RepartitionOnce the ETL process is completed, before writing it to S3, we need to repartition it. The partition size is equal to the number of files you want in s3.My current code as below.    datasource0 = glueContext.create_dynamic_frame.from_catalog(database = \"bhuvi\", table_name = \"glue_csv\", transformation_ctx = \"datasource0\")    applymapping1 = ApplyMapping.apply(frame = datasource0, mappings = [(\"ln\", \"string\", \"ln\", \"string\"), (\"gender\", \"string\", \"gender\", \"string\"), (\"ip\", \"string\", \"ip\", \"string\"), (\"fn\", \"string\", \"fn\", \"string\"), (\"id\", \"long\", \"id\", \"long\"), (\"email\", \"string\", \"email\", \"string\")], transformation_ctx = \"applymapping1\")    resolvechoice2 = ResolveChoice.apply(frame = applymapping1, choice = \"make_struct\", transformation_ctx = \"resolvechoice2\")    dropnullfields3 = DropNullFields.apply(frame = resolvechoice2, transformation_ctx = \"dropnullfields3\")    datasink4 = glueContext.write_dynamic_frame.from_options(frame = dropnullfields3, connection_type = \"s3\", connection_options = {\"path\": \"s3://bhuvi-datalake/parquet-new\"}, format = \"parquet\", transformation_ctx = \"datasink4\")    job.commit()Just add the repartition command above the write data frame line.    datasource0 = glueContext.create_dynamic_frame.from_catalog(database = \"bhuvi\", table_name = \"glue_csv\", transformation_ctx = \"datasource0\")    applymapping1 = ApplyMapping.apply(frame = datasource0, mappings = [(\"ln\", \"string\", \"ln\", \"string\"), (\"gender\", \"string\", \"gender\", \"string\"), (\"ip\", \"string\", \"ip\", \"string\"), (\"fn\", \"string\", \"fn\", \"string\"), (\"id\", \"long\", \"id\", \"long\"), (\"email\", \"string\", \"email\", \"string\")], transformation_ctx = \"applymapping1\")    resolvechoice2 = ResolveChoice.apply(frame = applymapping1, choice = \"make_struct\", transformation_ctx = \"resolvechoice2\")    dropnullfields3 = DropNullFields.apply(frame = resolvechoice2, transformation_ctx = \"dropnullfields3\")    datasource_df = dropnullfields3.repartition(2)    datasink4 = glueContext.write_dynamic_frame.from_options(frame = datasource_df, connection_type = \"s3\", connection_options = {\"path\": \"s3://bhuvi-datalake/parquet-new\"}, format = \"parquet\", transformation_ctx = \"datasink4\")    job.commit()repartition(2) - This will create 2 files in S3.  Total files: 2  Each file size: 1.7MBFinal words:From the above experiments, we can control the number of files for output and reducing the small files as output. But we can set the exact file size for output. Because Spark’s coalesce and repartition features are not yet implemented in Glue’s Python API, but its supports in Scala. I personally recommend using option 1.Further Reading:I have found some useful information while doing this experiment. Im giving those links below.  Reading Input Files in Larger Groups  Connection Types and Options for ETL in AWS Glue  How to use AWS Glue / Spark to convert CSVs partitioned and split in S3 to partitioned and split Parquet  Combine multiple raw files into single parquet file  AWS Glue FAQ, or How to Get Things Done  DynamicFrame vs DataFrame  DynamicFrame ClassSome interesting blogs for you:  Automate your AWS Redshift Vacuum and Analyze with Scripts  Export RedShift system tables to S3",
            "content_html": "<p>AWS Glue is the serverless version of EMR clusters. Many organizations now adopted to use Glue for their day to day BigData workloads. I have written a blog in Searce’s Medium publication for Converting the CSV/JSON files to parquet using AWS Glue. Till now its many people are reading that and implementing on their infra. But many people are commenting about the Glue is producing a huge number for output files(converted Parquet files) in S3, even for converting 100MB of CSV file will produce 500+ Parquet files. we need to customize this output file size and number of files.</p><h2 id=\"why-glue-is-producing-more-small-files\">Why Glue is producing more small files?</h2><p>If you are processing small chunks of files in Glue, it will read then and convert them into DynamicFrames. Glue is running on top of the Spark. So the dynamic frames will be moved to Partitions in the EMR cluster. And the Glue partition the data evenly among all of the nodes for better performance. Once its processed, all the partitions will be  pushing to your target. Each partition will and one file. That’s why we are getting more files.</p><h2 id=\"customize-the-output-files\">Customize the output files:</h2><p>We can customize it in two ways.</p><ol>  <li>While reading the data from the source.</li>  <li>While writing the data to the target.</li></ol><p>If you have so many small numbers of files in your source, them Glue process them in many partitions. So we can force the Glue to read multiple file in one shot. Like we are grouping multiple file and the Glue virtually consider this as a single file.</p><p>Else, once you processed the data, you can repartition the data. So you can mention how many partitions you want. Let’s say if you repartition the data with 5, then it’ll write 5 files in your target.</p><h2 id=\"testing-infra-setup\">Testing Infra setup:</h2><ul>  <li>I have 1GB of test data set.</li>  <li>Format CSV</li>  <li>Split into 20 files.</li>  <li>Each file is 52MB.</li>  <li>Created a Glue crawler on top of this data and its created the table in Glue catalog.</li>  <li>Im using glue to convert this CSV to Parquet. Follow the instructions here: <a href=\"https://medium.com/searce/convert-csv-json-files-to-apache-parquet-using-aws-glue-a760d177b45f\" title=\"https://medium.com/searce/convert-csv-json-files-to-apache-parquet-using-aws-glue-a760d177b45f\">https://medium.com/searce/convert-csv-json-files-to-apache-parquet-using-aws-glue-a760d177b45f</a></li></ul><p><img src=\"/assets/AWS Glue Custom Output File Size And Fixed Number Of Files2 .jpg\" alt=\"\" /></p><h2 id=\"option-1-groupfiles\">Option 1: groupFiles</h2><p><strong>From AWS Doc,</strong></p><blockquote>  <p>You can set properties of your tables to enable an AWS Glue ETL job to group files when they are read from an Amazon S3 data store. These properties enable each ETL task to read a group of input files into a single in-memory partition, this is especially useful when there is a large number of small files in your Amazon S3 data store.</p></blockquote><p><strong>groupFiles:</strong></p><p>Set <strong>groupFiles</strong> to <code class=\"language-html highlighter-rouge\">inPartition</code> to enable the grouping of files within an Amazon S3 data partition. AWS Glue automatically enables grouping if there are more than 50,000 input files.</p><p><strong>groupSize:</strong></p><p>Set <strong>groupSize</strong> to the target size of groups in bytes. The <strong>groupSize</strong> property is optional, if not provided, AWS Glue calculates a size to use all the CPU cores in the cluster while still reducing the overall number of ETL tasks and in-memory partitions.</p><p>Go to Glue –&gt; Tables –&gt; select your table –&gt; Edit Table.</p><p>Unde the table properties, add the following parameters.</p><ul>  <li><code class=\"language-html highlighter-rouge\">groupFiles</code> - <code class=\"language-html highlighter-rouge\">inPartition</code></li>  <li><code class=\"language-html highlighter-rouge\">groupSize</code> - <code class=\"language-html highlighter-rouge\">209715200</code></li></ul><p>This will read 200MB data in one partition. Lets run the job and see the output.</p><ul>  <li>Total Number of files: 5</li>  <li>Each file size: 393kb</li></ul><p><img src=\"/assets/AWS Glue Custom Output File Size And Fixed Number Of Files3.jpg\" alt=\"\" /></p><h2 id=\"option-2-groupfiles-while-reading-from-s3\">Option 2: groupFiles while reading from S3</h2><p>It’s the same as the previous one, but if you take a look at the datasource, its creating the dynamic frame from the catalog table.</p><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>datasource0 = glueContext.create_dynamic_frame.from_catalog(database = \"bhuvi\"</code></pre></div></div><p>But if you are directly reading it from S3, you can change the source like below.</p><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>datasource0 = glueContext.create_dynamic_frame_from_options(\"s3\", {'paths': [\"s3://s3path/\"], 'recurse':True, 'groupFiles': 'inPartition', 'groupSize': '104857600'}, format=\"csv\")</code></pre></div></div><h2 id=\"option-3-repartition\">Option 3: Repartition</h2><p>Once the ETL process is completed, before writing it to S3, we need to repartition it. The partition size is equal to the number of files you want in s3.</p><p>My current code as below.</p><figure class=\"highlight\"><pre><code class=\"language-python\" data-lang=\"python\">    <span class=\"n\">datasource0</span> <span class=\"o\">=</span> <span class=\"n\">glueContext</span><span class=\"p\">.</span><span class=\"n\">create_dynamic_frame</span><span class=\"p\">.</span><span class=\"n\">from_catalog</span><span class=\"p\">(</span><span class=\"n\">database</span> <span class=\"o\">=</span> <span class=\"s\">\"bhuvi\"</span><span class=\"p\">,</span> <span class=\"n\">table_name</span> <span class=\"o\">=</span> <span class=\"s\">\"glue_csv\"</span><span class=\"p\">,</span> <span class=\"n\">transformation_ctx</span> <span class=\"o\">=</span> <span class=\"s\">\"datasource0\"</span><span class=\"p\">)</span>    <span class=\"n\">applymapping1</span> <span class=\"o\">=</span> <span class=\"n\">ApplyMapping</span><span class=\"p\">.</span><span class=\"nb\">apply</span><span class=\"p\">(</span><span class=\"n\">frame</span> <span class=\"o\">=</span> <span class=\"n\">datasource0</span><span class=\"p\">,</span> <span class=\"n\">mappings</span> <span class=\"o\">=</span> <span class=\"p\">[(</span><span class=\"s\">\"ln\"</span><span class=\"p\">,</span> <span class=\"s\">\"string\"</span><span class=\"p\">,</span> <span class=\"s\">\"ln\"</span><span class=\"p\">,</span> <span class=\"s\">\"string\"</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s\">\"gender\"</span><span class=\"p\">,</span> <span class=\"s\">\"string\"</span><span class=\"p\">,</span> <span class=\"s\">\"gender\"</span><span class=\"p\">,</span> <span class=\"s\">\"string\"</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s\">\"ip\"</span><span class=\"p\">,</span> <span class=\"s\">\"string\"</span><span class=\"p\">,</span> <span class=\"s\">\"ip\"</span><span class=\"p\">,</span> <span class=\"s\">\"string\"</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s\">\"fn\"</span><span class=\"p\">,</span> <span class=\"s\">\"string\"</span><span class=\"p\">,</span> <span class=\"s\">\"fn\"</span><span class=\"p\">,</span> <span class=\"s\">\"string\"</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s\">\"id\"</span><span class=\"p\">,</span> <span class=\"s\">\"long\"</span><span class=\"p\">,</span> <span class=\"s\">\"id\"</span><span class=\"p\">,</span> <span class=\"s\">\"long\"</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s\">\"email\"</span><span class=\"p\">,</span> <span class=\"s\">\"string\"</span><span class=\"p\">,</span> <span class=\"s\">\"email\"</span><span class=\"p\">,</span> <span class=\"s\">\"string\"</span><span class=\"p\">)],</span> <span class=\"n\">transformation_ctx</span> <span class=\"o\">=</span> <span class=\"s\">\"applymapping1\"</span><span class=\"p\">)</span>    <span class=\"n\">resolvechoice2</span> <span class=\"o\">=</span> <span class=\"n\">ResolveChoice</span><span class=\"p\">.</span><span class=\"nb\">apply</span><span class=\"p\">(</span><span class=\"n\">frame</span> <span class=\"o\">=</span> <span class=\"n\">applymapping1</span><span class=\"p\">,</span> <span class=\"n\">choice</span> <span class=\"o\">=</span> <span class=\"s\">\"make_struct\"</span><span class=\"p\">,</span> <span class=\"n\">transformation_ctx</span> <span class=\"o\">=</span> <span class=\"s\">\"resolvechoice2\"</span><span class=\"p\">)</span>    <span class=\"n\">dropnullfields3</span> <span class=\"o\">=</span> <span class=\"n\">DropNullFields</span><span class=\"p\">.</span><span class=\"nb\">apply</span><span class=\"p\">(</span><span class=\"n\">frame</span> <span class=\"o\">=</span> <span class=\"n\">resolvechoice2</span><span class=\"p\">,</span> <span class=\"n\">transformation_ctx</span> <span class=\"o\">=</span> <span class=\"s\">\"dropnullfields3\"</span><span class=\"p\">)</span>    <span class=\"n\">datasink4</span> <span class=\"o\">=</span> <span class=\"n\">glueContext</span><span class=\"p\">.</span><span class=\"n\">write_dynamic_frame</span><span class=\"p\">.</span><span class=\"n\">from_options</span><span class=\"p\">(</span><span class=\"n\">frame</span> <span class=\"o\">=</span> <span class=\"n\">dropnullfields3</span><span class=\"p\">,</span> <span class=\"n\">connection_type</span> <span class=\"o\">=</span> <span class=\"s\">\"s3\"</span><span class=\"p\">,</span> <span class=\"n\">connection_options</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s\">\"path\"</span><span class=\"p\">:</span> <span class=\"s\">\"s3://bhuvi-datalake/parquet-new\"</span><span class=\"p\">},</span> <span class=\"nb\">format</span> <span class=\"o\">=</span> <span class=\"s\">\"parquet\"</span><span class=\"p\">,</span> <span class=\"n\">transformation_ctx</span> <span class=\"o\">=</span> <span class=\"s\">\"datasink4\"</span><span class=\"p\">)</span>    <span class=\"n\">job</span><span class=\"p\">.</span><span class=\"n\">commit</span><span class=\"p\">()</span></code></pre></figure><p>Just add the repartition command above the write data frame line.</p><figure class=\"highlight\"><pre><code class=\"language-python\" data-lang=\"python\">    <span class=\"n\">datasource0</span> <span class=\"o\">=</span> <span class=\"n\">glueContext</span><span class=\"p\">.</span><span class=\"n\">create_dynamic_frame</span><span class=\"p\">.</span><span class=\"n\">from_catalog</span><span class=\"p\">(</span><span class=\"n\">database</span> <span class=\"o\">=</span> <span class=\"s\">\"bhuvi\"</span><span class=\"p\">,</span> <span class=\"n\">table_name</span> <span class=\"o\">=</span> <span class=\"s\">\"glue_csv\"</span><span class=\"p\">,</span> <span class=\"n\">transformation_ctx</span> <span class=\"o\">=</span> <span class=\"s\">\"datasource0\"</span><span class=\"p\">)</span>    <span class=\"n\">applymapping1</span> <span class=\"o\">=</span> <span class=\"n\">ApplyMapping</span><span class=\"p\">.</span><span class=\"nb\">apply</span><span class=\"p\">(</span><span class=\"n\">frame</span> <span class=\"o\">=</span> <span class=\"n\">datasource0</span><span class=\"p\">,</span> <span class=\"n\">mappings</span> <span class=\"o\">=</span> <span class=\"p\">[(</span><span class=\"s\">\"ln\"</span><span class=\"p\">,</span> <span class=\"s\">\"string\"</span><span class=\"p\">,</span> <span class=\"s\">\"ln\"</span><span class=\"p\">,</span> <span class=\"s\">\"string\"</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s\">\"gender\"</span><span class=\"p\">,</span> <span class=\"s\">\"string\"</span><span class=\"p\">,</span> <span class=\"s\">\"gender\"</span><span class=\"p\">,</span> <span class=\"s\">\"string\"</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s\">\"ip\"</span><span class=\"p\">,</span> <span class=\"s\">\"string\"</span><span class=\"p\">,</span> <span class=\"s\">\"ip\"</span><span class=\"p\">,</span> <span class=\"s\">\"string\"</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s\">\"fn\"</span><span class=\"p\">,</span> <span class=\"s\">\"string\"</span><span class=\"p\">,</span> <span class=\"s\">\"fn\"</span><span class=\"p\">,</span> <span class=\"s\">\"string\"</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s\">\"id\"</span><span class=\"p\">,</span> <span class=\"s\">\"long\"</span><span class=\"p\">,</span> <span class=\"s\">\"id\"</span><span class=\"p\">,</span> <span class=\"s\">\"long\"</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s\">\"email\"</span><span class=\"p\">,</span> <span class=\"s\">\"string\"</span><span class=\"p\">,</span> <span class=\"s\">\"email\"</span><span class=\"p\">,</span> <span class=\"s\">\"string\"</span><span class=\"p\">)],</span> <span class=\"n\">transformation_ctx</span> <span class=\"o\">=</span> <span class=\"s\">\"applymapping1\"</span><span class=\"p\">)</span>    <span class=\"n\">resolvechoice2</span> <span class=\"o\">=</span> <span class=\"n\">ResolveChoice</span><span class=\"p\">.</span><span class=\"nb\">apply</span><span class=\"p\">(</span><span class=\"n\">frame</span> <span class=\"o\">=</span> <span class=\"n\">applymapping1</span><span class=\"p\">,</span> <span class=\"n\">choice</span> <span class=\"o\">=</span> <span class=\"s\">\"make_struct\"</span><span class=\"p\">,</span> <span class=\"n\">transformation_ctx</span> <span class=\"o\">=</span> <span class=\"s\">\"resolvechoice2\"</span><span class=\"p\">)</span>    <span class=\"n\">dropnullfields3</span> <span class=\"o\">=</span> <span class=\"n\">DropNullFields</span><span class=\"p\">.</span><span class=\"nb\">apply</span><span class=\"p\">(</span><span class=\"n\">frame</span> <span class=\"o\">=</span> <span class=\"n\">resolvechoice2</span><span class=\"p\">,</span> <span class=\"n\">transformation_ctx</span> <span class=\"o\">=</span> <span class=\"s\">\"dropnullfields3\"</span><span class=\"p\">)</span>    <span class=\"n\">datasource_df</span> <span class=\"o\">=</span> <span class=\"n\">dropnullfields3</span><span class=\"p\">.</span><span class=\"n\">repartition</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">)</span>    <span class=\"n\">datasink4</span> <span class=\"o\">=</span> <span class=\"n\">glueContext</span><span class=\"p\">.</span><span class=\"n\">write_dynamic_frame</span><span class=\"p\">.</span><span class=\"n\">from_options</span><span class=\"p\">(</span><span class=\"n\">frame</span> <span class=\"o\">=</span> <span class=\"n\">datasource_df</span><span class=\"p\">,</span> <span class=\"n\">connection_type</span> <span class=\"o\">=</span> <span class=\"s\">\"s3\"</span><span class=\"p\">,</span> <span class=\"n\">connection_options</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s\">\"path\"</span><span class=\"p\">:</span> <span class=\"s\">\"s3://bhuvi-datalake/parquet-new\"</span><span class=\"p\">},</span> <span class=\"nb\">format</span> <span class=\"o\">=</span> <span class=\"s\">\"parquet\"</span><span class=\"p\">,</span> <span class=\"n\">transformation_ctx</span> <span class=\"o\">=</span> <span class=\"s\">\"datasink4\"</span><span class=\"p\">)</span>    <span class=\"n\">job</span><span class=\"p\">.</span><span class=\"n\">commit</span><span class=\"p\">()</span></code></pre></figure><p><code class=\"language-html highlighter-rouge\">repartition(2)</code> - This will create 2 files in S3.</p><ul>  <li>Total files: 2</li>  <li>Each file size: 1.7MB</li></ul><p><img src=\"/assets/AWS Glue Custom Output File Size And Fixed Number Of Files1.jpg\" alt=\"\" /></p><h2 id=\"final-words\">Final words:</h2><p>From the above experiments, we can control the number of files for output and reducing the small files as output. But we can set the exact file size for output. Because Spark’s <a href=\"https://hackernoon.com/managing-spark-partitions-with-coalesce-and-repartition-4050c57ad5c4\">coalesce and repartition</a> features are not yet implemented in Glue’s Python API, but its supports in Scala. I personally recommend using option 1.</p><h2 id=\"further-reading\">Further Reading:</h2><p>I have found some useful information while doing this experiment. Im giving those links below.</p><ol>  <li><a href=\"https://docs.aws.amazon.com/glue/latest/dg/grouping-input-files.html\">Reading Input Files in Larger Groups</a></li>  <li><a href=\"https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-connect.html#aws-glue-programming-etl-connect-s3\">Connection Types and Options for ETL in AWS Glue</a></li>  <li><a href=\"https://stackoverflow.com/questions/48693943/how-to-use-aws-glue-spark-to-convert-csvs-partitioned-and-split-in-s3-to-parti\">How to use AWS Glue / Spark to convert CSVs partitioned and split in S3 to partitioned and split Parquet</a></li>  <li><a href=\"https://stackoverflow.com/questions/47147159/combine-multiple-raw-files-into-single-parquet-file\">Combine multiple raw files into single parquet file</a></li>  <li><a href=\"https://github.com/aws-samples/aws-glue-samples/blob/master/FAQ_and_How_to.md#aws-glue-faq-or-how-to-get-things-done\">AWS Glue FAQ, or How to Get Things Done</a></li>  <li><a href=\"https://stackoverflow.com/questions/52822526/dynamicframe-vs-dataframe\">DynamicFrame vs DataFrame</a></li>  <li><a href=\"https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-dynamic-frame.html\">DynamicFrame Class</a></li></ol><h2 id=\"some-interesting-blogs-for-you\">Some interesting blogs for you:</h2><ol>  <li><a href=\"https://thedataguy.in/automate-redshift-vacuum-analyze-using-shell-script-utility/\">Automate your AWS Redshift Vacuum and Analyze with Scripts</a></li>  <li><a href=\"https://thedataguy.in/export-redshift-system-tables-views-to-s3/\">Export RedShift system tables to S3</a></li></ol>",
            "url": "/2019/10/07/aws-glue-custom-output-file-size-and-fixed-number-of-files",
            "image": "/assets/AWS Glue Custom Output File Size And Fixed Number Of Files.jpg",
            
            
            "tags": ["aws","glue","parquet","s3"],
            
            "date_published": "2019-10-07T20:40:00+00:00",
            "date_modified": "2019-10-07T20:40:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2019/10/06/redshift-unload-all-tables-to-s3",
            "title": "RedShift Unload All Tables To S3",
            "summary": "Easy way to export or unload all the tables to  S3 from Redshift using stored procedure. Also it'll export with partitions in S3",
            "content_text": "RedShift unload function will help us to export/unload the data from the tables to S3 directly. It actually runs a select query to get the results and them store them into S3. But unfortunately, it supports only one table at a time. You need to create a script to get the all the tables then store it in a variable, and loop the unload query with the list of tables. Here I have done with PL/SQL way to handle this. You can Export/Unload all the tables to S3 with partitions.If you didn’t take a look at how to export a table with Partition and why? Please hit here and read about the importance of it.Why this procedure actually doing?  It will get the list of schema and table in your database from the information_schema.  Store this information in a variable.  IAM role, Partitions are hardcoded, you can customize it or pass them in a variable.  A FOR LOOP will run the unload query for all the tables.Variables:  list - list of schema and table names in the database.  db - Current connected database name.  tablename - table name (used for history table only).  tableschema - table schema (used for history table only).  starttime - When the unload the process stated.  endtime - When the unload process end.  SQL - its a select * from, but if you want to change like where timestamp &gt;= something you can customize this variable.  s3_path - Location of S3, you need to pass this variable while executing the procedure.  iamrole - IAM role to write into the S3 bucket.  delimiter - Delimiter for the file.  max_filesize - Redshift will split your files in S3 in random sizes, you can mention a size for the files.  un_year, un_month, un_day - Current Year, month, day  unload_query - Dynamically generate the unload query.  unload_id - This is for maintaining the history purpose, In one shot you can export all the tables, from this ID, you can get the list of tables uploaded from a particular export operation.  unload_time - Timestamp of when you started executing the procedure.  NOTE: This stored procedure and the history table needs to installed on all the databases. Because from information schema it’ll only return the list of tables in the current schema. Its Redshift’s limitation.Update 2019-10-08I have made a small change here, the stored procedure will generate the COPY command as well. You can query the unload_history table to get the COPY command for a particular table. So you can easily import the data into any RedShift clusters.Update 2019-11-22I have published a new blog. You can now export based on your requirements like export only few tables, all tables in a schema, all tables in multiple schema and etc. Click on the below link.https://thedataguy.in/redshift-unload-multiple-tables-schema-to-s3/Table for maintaining the History of Unload:CREATE TABLE unload_history(pid          INT IDENTITY(1, 1),u_id         INT,u_timestamp  DATETIME,start_time   DATETIME,end_time     DATETIME,db_name      VARCHAR(100),table_schema VARCHAR(100),table_name   VARCHAR(100),export_query VARCHAR(65000),import_query VARCHAR(65000));Stored Procedure:CREATE OR replace PROCEDURE unload_all(s3_location varchar(10000)) LANGUAGE plpgsqlAS$$DECLARElist RECORD;db        VARCHAR(100);tablename VARCHAR(100);tableschema VARCHAR(100);starttime datetime;endtime   datetime;SQL text;s3_path      VARCHAR(1000);iamrole      VARCHAR(100);delimiter    VARCHAR(10);max_filesize VARCHAR(100);un_year      INT;un_month     INT;un_day       INT;unload_query varchar(65000);copy_query   varchar(65000);unload_id INT;unload_time timestamp;      BEGIN               -- Pass values for the variables         SELECT extract(year FROM getdate())         INTO   un_year;                  SELECT extract(month FROM getdate())         INTO   un_month;                  SELECT extract(day FROM getdate())         INTO   un_day;                  SELECT DISTINCT(table_catalog)         FROM            information_schema.TABLES         INTO            db;                  SELECT coalesce(max(u_id), 0)+1         FROM   unload_history         INTO   unload_id;                  SELECT getdate()         INTO   unload_time;                  s3_path:=s3_location;                 -- IAM ROLE and the Delimiter is hardcoded here        iamrole:='arn:aws:iam::123123123:role/myredshiftrole';         delimiter:='|';                 -- Get the list of tables except the unload history table        FOR list IN         SELECT table_schema,                table_name         FROM   information_schema.TABLES         WHERE  table_type='BASE TABLE'         AND    table_schema NOT IN ('pg_catalog',                                     'information_schema')         AND    table_name !='unload_history' LOOP                 SELECT getdate()         INTO   starttime;                  sql:='select * from '||list.table_schema||'.'||list.table_name||'' ;            RAISE info '[%] Unloading... schema = % and table = %',starttime, list.table_schema, list.table_name;                -- Start unloading the data         unload_query := 'unload ('''||sql||''') to '''||s3_path||un_year||'/'||un_month||'/'||un_day||'/'||db||'/'||list.table_schema||'/'||list.table_name||'/'||list.table_schema||'-'||list.table_name||'_'' iam_role '''||iamrole||''' delimiter '''||delimiter||''' MAXFILESIZE 300 MB PARALLEL ADDQUOTES HEADER GZIP';        EXECUTE unload_query;                 copy_query := 'copy '||list.table_schema||'.'||list.table_name||' from '''||s3_path||un_year||'/'||un_month||'/'||un_day||'/'||db||'/'||list.table_schema||'/'||list.table_name||'/'' iam_role '''||iamrole||''' delimiter '''||delimiter||''' IGNOREHEADER 1 REMOVEQUOTES gzip';                SELECT getdate()         INTO   endtime;             SELECT list.table_schema         INTO tableschema;            SELECT list.table_name         INTO tablename;                 -- Insert into the history table        INSERT INTO unload_history                     (                                 u_id,                                 u_timestamp,                                 start_time,                                 end_time,                                 db_name,                                 table_schema,                                table_name,                                export_query,                                import_query                     )                     VALUES                     (                                 unload_id,                                 unload_time,                                 starttime,                                 endtime,                                 db,                                 tableschema,                                tablename,                                unload_query,                                copy_query                    );              END LOOP;       RAISE info ' Unloading of the DB [%] is success !!!' ,db;    END;     $$;Hardcoded Items:In the stored procedure, I have hardcoded the follow parameters.  IAM ROLE - arn:aws:iam::123123123123:role/myredshiftrole  Delimiter - |Also, the following Items are hardcoded in the Unload query. You can get these things as variable or hardcoded as per your convenient.  MAXFILESIZE  - 100 MB  PARALLEL  ADDQUOTES  HEADER  GZIPExecute the procedure:call unload_all('s3://bhuvi-datalake/test/');You can see the status in the terminalINFO:  [2019-10-06 19:20:04] Unloading... schema = etl and table = tbl1INFO:  UNLOAD completed, 2 record(s) unloaded successfully.INFO:  [2019-10-06 19:20:10] Unloading... schema = etl and table = tbl2INFO:  UNLOAD completed, 2 record(s) unloaded successfully.INFO:  [2019-10-06 19:20:12] Unloading... schema = stage and table = tbl3INFO:  UNLOAD completed, 2 record(s) unloaded successfully.INFO:  [2019-10-06 19:20:12] Unloading... schema = stage and table = tbl4INFO:  UNLOAD completed, 2 record(s) unloaded successfully.INFO:  [2019-10-06 19:20:14] Unloading... schema = stage and table = tbl5INFO:  UNLOAD completed, 2 record(s) unloaded successfully.INFO:  [2019-10-06 19:20:15] Unloading... schema = prod and table = tbl6INFO:  UNLOAD completed, 2 record(s) unloaded successfully.INFO:  [2019-10-06 19:20:15] Unloading... schema = prod and table = tbl7INFO:  UNLOAD completed, 2 record(s) unloaded successfully.INFO:  [2019-10-06 19:20:15] Unloading... schema = public and table = debugINFO:  UNLOAD completed, 0 record(s) unloaded successfully.INFO:  [2019-10-06 19:20:16] Unloading... schema = public and table = testINFO:  UNLOAD completed, 1 record(s) unloaded successfully.INFO:   Unloading of the DB [preprod] is success !!!CALLFiles on S3:Retrieve the History:preprod=# select * from unload_history limit 1;    pid          | 28u_id         | 1u_timestamp  | 2019-10-08 10:33:23start_time   | 2019-10-08 10:33:23end_time     | 2019-10-08 10:33:23db_name      | preprodtable_schema | etltable_name   | tbl2export_query | unload ('select * from etl.tbl2') to 's3://bhuvi-datalake/test/2019/10/8/preprod/etl/tbl2/etl-tbl2_' iam_role 'arn:aws:iam::123123123:role/myredshiftrole' delimiter '|' MAXFILESIZE 300 MB PARALLEL ADDQUOTES HEADER GZIPimport_query | copy etl.tbl2 from 's3://bhuvi-datalake/test/2019/10/8/preprod/etl/tbl2/' iam_role 'arn:aws:iam::123123123:role/myredshiftrole' delimiter '|' IGNOREHEADER 1 REMOVEQUOTES gzip",
            "content_html": "<p>RedShift <code class=\"language-html highlighter-rouge\">unload</code> function will help us to export/unload the data from the tables to S3 directly. It actually runs a select query to get the results and them store them into S3. But unfortunately, it supports only one table at a time. You need to create a script to get the all the tables then store it in a variable, and loop the unload query with the list of tables. Here I have done with PL/SQL way to handle this. You can Export/Unload all the tables to S3 with partitions.</p><p>If you didn’t take a look at how to export a table with Partition and why? Please <a href=\"https://thedataguy.in/redshift-unload-to-s3-with-partitions-stored-procedure-way/\">hit here</a> and read about the importance of it.</p><h2 id=\"why-this-procedure-actually-doing\">Why this procedure actually doing?</h2><ol>  <li>It will get the list of schema and table in your database from the <code class=\"language-html highlighter-rouge\">information_schema</code>.</li>  <li>Store this information in a variable.</li>  <li>IAM role, Partitions are hardcoded, you can customize it or pass them in a variable.</li>  <li>A <code class=\"language-html highlighter-rouge\">FOR LOOP</code> will run the unload query for all the tables.</li></ol><h2 id=\"variables\">Variables:</h2><ul>  <li>list - list of schema and table names in the database.</li>  <li>db - Current connected database name.</li>  <li>tablename - table name (used for history table only).</li>  <li>tableschema - table schema (used for history table only).</li>  <li>starttime - When the unload the process stated.</li>  <li>endtime - When the unload process end.</li>  <li>SQL - its a <code class=\"language-html highlighter-rouge\">select * from</code>, but if you want to change like <code class=\"language-html highlighter-rouge\">where timestamp &gt;= something</code> you can customize this variable.</li>  <li>s3_path - Location of S3, you need to pass this variable while executing the procedure.</li>  <li>iamrole - IAM role to write into the S3 bucket.</li>  <li>delimiter - Delimiter for the file.</li>  <li>max_filesize - Redshift will split your files in S3 in random sizes, you can mention a size for the files.</li>  <li>un_year, un_month, un_day - Current Year, month, day</li>  <li>unload_query - Dynamically generate the unload query.</li>  <li>unload_id - This is for maintaining the history purpose, In one shot you can export all the tables, from this ID, you can get the list of tables uploaded from a particular export operation.</li>  <li>unload_time - Timestamp of when you started executing the procedure.</li></ul><blockquote>  <p><strong>NOTE</strong>: This stored procedure and the history table needs to installed on all the databases. Because from information schema it’ll only return the list of tables in the current schema. Its Redshift’s limitation.</p></blockquote><h3 id=\"update-2019-10-08\">Update 2019-10-08</h3><p>I have made a small change here, the stored procedure will generate the COPY command as well. You can query the <code class=\"language-html highlighter-rouge\">unload_history</code> table to get the COPY command for a particular table. So you can easily import the data into any RedShift clusters.</p><h3 id=\"update-2019-11-22\">Update 2019-11-22</h3><p>I have published a new blog. You can now export based on your requirements like export only few tables, all tables in a schema, all tables in multiple schema and etc. Click on the below link.<a href=\"https://thedataguy.in/redshift-unload-multiple-tables-schema-to-s3/\">https://thedataguy.in/redshift-unload-multiple-tables-schema-to-s3/</a></p><h2 id=\"table-for-maintaining-the-history-of-unload\">Table for maintaining the History of Unload:</h2><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">CREATE</span> <span class=\"k\">TABLE</span> <span class=\"n\">unload_history</span><span class=\"p\">(</span><span class=\"n\">pid</span>          <span class=\"nb\">INT</span> <span class=\"k\">IDENTITY</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span><span class=\"n\">u_id</span>         <span class=\"nb\">INT</span><span class=\"p\">,</span><span class=\"n\">u_timestamp</span>  <span class=\"nb\">DATETIME</span><span class=\"p\">,</span><span class=\"n\">start_time</span>   <span class=\"nb\">DATETIME</span><span class=\"p\">,</span><span class=\"n\">end_time</span>     <span class=\"nb\">DATETIME</span><span class=\"p\">,</span><span class=\"n\">db_name</span>      <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">),</span><span class=\"n\">table_schema</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">),</span><span class=\"k\">table_name</span>   <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">),</span><span class=\"n\">export_query</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">65000</span><span class=\"p\">),</span><span class=\"n\">import_query</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">65000</span><span class=\"p\">)</span><span class=\"p\">);</span></code></pre></figure><h2 id=\"stored-procedure\">Stored Procedure:</h2><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"k\">CREATE</span> <span class=\"k\">OR</span> <span class=\"k\">replace</span> <span class=\"k\">PROCEDURE</span> <span class=\"n\">unload_all</span><span class=\"p\">(</span><span class=\"n\">s3_location</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">10000</span><span class=\"p\">))</span> <span class=\"k\">LANGUAGE</span> <span class=\"n\">plpgsql</span><span class=\"k\">AS</span><span class=\"err\">$$</span><span class=\"k\">DECLARE</span><span class=\"n\">list</span> <span class=\"n\">RECORD</span><span class=\"p\">;</span><span class=\"n\">db</span>        <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">);</span><span class=\"n\">tablename</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">);</span><span class=\"n\">tableschema</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">);</span><span class=\"n\">starttime</span> <span class=\"nb\">datetime</span><span class=\"p\">;</span><span class=\"n\">endtime</span>   <span class=\"nb\">datetime</span><span class=\"p\">;</span><span class=\"k\">SQL</span> <span class=\"nb\">text</span><span class=\"p\">;</span><span class=\"n\">s3_path</span>      <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">);</span><span class=\"n\">iamrole</span>      <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">);</span><span class=\"k\">delimiter</span>    <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">);</span><span class=\"n\">max_filesize</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">);</span><span class=\"n\">un_year</span>      <span class=\"nb\">INT</span><span class=\"p\">;</span><span class=\"n\">un_month</span>     <span class=\"nb\">INT</span><span class=\"p\">;</span><span class=\"n\">un_day</span>       <span class=\"nb\">INT</span><span class=\"p\">;</span><span class=\"n\">unload_query</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">65000</span><span class=\"p\">);</span><span class=\"n\">copy_query</span>   <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">65000</span><span class=\"p\">);</span><span class=\"n\">unload_id</span> <span class=\"nb\">INT</span><span class=\"p\">;</span><span class=\"n\">unload_time</span> <span class=\"nb\">timestamp</span><span class=\"p\">;</span>      <span class=\"k\">BEGIN</span>               <span class=\"c1\">-- Pass values for the variables </span>        <span class=\"k\">SELECT</span> <span class=\"k\">extract</span><span class=\"p\">(</span><span class=\"nb\">year</span> <span class=\"k\">FROM</span> <span class=\"n\">getdate</span><span class=\"p\">())</span>         <span class=\"k\">INTO</span>   <span class=\"n\">un_year</span><span class=\"p\">;</span>                  <span class=\"k\">SELECT</span> <span class=\"k\">extract</span><span class=\"p\">(</span><span class=\"k\">month</span> <span class=\"k\">FROM</span> <span class=\"n\">getdate</span><span class=\"p\">())</span>         <span class=\"k\">INTO</span>   <span class=\"n\">un_month</span><span class=\"p\">;</span>                  <span class=\"k\">SELECT</span> <span class=\"k\">extract</span><span class=\"p\">(</span><span class=\"k\">day</span> <span class=\"k\">FROM</span> <span class=\"n\">getdate</span><span class=\"p\">())</span>         <span class=\"k\">INTO</span>   <span class=\"n\">un_day</span><span class=\"p\">;</span>                  <span class=\"k\">SELECT</span> <span class=\"k\">DISTINCT</span><span class=\"p\">(</span><span class=\"n\">table_catalog</span><span class=\"p\">)</span>         <span class=\"k\">FROM</span>            <span class=\"n\">information_schema</span><span class=\"p\">.</span><span class=\"n\">TABLES</span>         <span class=\"k\">INTO</span>            <span class=\"n\">db</span><span class=\"p\">;</span>                  <span class=\"k\">SELECT</span> <span class=\"n\">coalesce</span><span class=\"p\">(</span><span class=\"k\">max</span><span class=\"p\">(</span><span class=\"n\">u_id</span><span class=\"p\">),</span> <span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"o\">+</span><span class=\"mi\">1</span>         <span class=\"k\">FROM</span>   <span class=\"n\">unload_history</span>         <span class=\"k\">INTO</span>   <span class=\"n\">unload_id</span><span class=\"p\">;</span>                  <span class=\"k\">SELECT</span> <span class=\"n\">getdate</span><span class=\"p\">()</span>         <span class=\"k\">INTO</span>   <span class=\"n\">unload_time</span><span class=\"p\">;</span>                  <span class=\"n\">s3_path</span><span class=\"p\">:</span><span class=\"o\">=</span><span class=\"n\">s3_location</span><span class=\"p\">;</span>                 <span class=\"c1\">-- IAM ROLE and the Delimiter is hardcoded here</span>        <span class=\"n\">iamrole</span><span class=\"p\">:</span><span class=\"o\">=</span><span class=\"s1\">'arn:aws:iam::123123123:role/myredshiftrole'</span><span class=\"p\">;</span>         <span class=\"k\">delimiter</span><span class=\"p\">:</span><span class=\"o\">=</span><span class=\"s1\">'|'</span><span class=\"p\">;</span>                 <span class=\"c1\">-- Get the list of tables except the unload history table</span>        <span class=\"k\">FOR</span> <span class=\"n\">list</span> <span class=\"k\">IN</span>         <span class=\"k\">SELECT</span> <span class=\"n\">table_schema</span><span class=\"p\">,</span>                <span class=\"k\">table_name</span>         <span class=\"k\">FROM</span>   <span class=\"n\">information_schema</span><span class=\"p\">.</span><span class=\"n\">TABLES</span>         <span class=\"k\">WHERE</span>  <span class=\"n\">table_type</span><span class=\"o\">=</span><span class=\"s1\">'BASE TABLE'</span>         <span class=\"k\">AND</span>    <span class=\"n\">table_schema</span> <span class=\"k\">NOT</span> <span class=\"k\">IN</span> <span class=\"p\">(</span><span class=\"s1\">'pg_catalog'</span><span class=\"p\">,</span>                                     <span class=\"s1\">'information_schema'</span><span class=\"p\">)</span>         <span class=\"k\">AND</span>    <span class=\"k\">table_name</span> <span class=\"o\">!=</span><span class=\"s1\">'unload_history'</span> <span class=\"n\">LOOP</span>                 <span class=\"k\">SELECT</span> <span class=\"n\">getdate</span><span class=\"p\">()</span>         <span class=\"k\">INTO</span>   <span class=\"n\">starttime</span><span class=\"p\">;</span>                  <span class=\"k\">sql</span><span class=\"p\">:</span><span class=\"o\">=</span><span class=\"s1\">'select * from '</span><span class=\"o\">||</span><span class=\"n\">list</span><span class=\"p\">.</span><span class=\"n\">table_schema</span><span class=\"o\">||</span><span class=\"s1\">'.'</span><span class=\"o\">||</span><span class=\"n\">list</span><span class=\"p\">.</span><span class=\"k\">table_name</span><span class=\"o\">||</span><span class=\"s1\">''</span> <span class=\"p\">;</span>            <span class=\"n\">RAISE</span> <span class=\"n\">info</span> <span class=\"s1\">'[%] Unloading... schema = % and table = %'</span><span class=\"p\">,</span><span class=\"n\">starttime</span><span class=\"p\">,</span> <span class=\"n\">list</span><span class=\"p\">.</span><span class=\"n\">table_schema</span><span class=\"p\">,</span> <span class=\"n\">list</span><span class=\"p\">.</span><span class=\"k\">table_name</span><span class=\"p\">;</span>                <span class=\"c1\">-- Start unloading the data </span>        <span class=\"n\">unload_query</span> <span class=\"p\">:</span><span class=\"o\">=</span> <span class=\"s1\">'unload (</span><span class=\"se\">''</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"k\">sql</span><span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">''</span><span class=\"s1\">) to </span><span class=\"se\">''</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"n\">s3_path</span><span class=\"o\">||</span><span class=\"n\">un_year</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">un_month</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">un_day</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">db</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">list</span><span class=\"p\">.</span><span class=\"n\">table_schema</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">list</span><span class=\"p\">.</span><span class=\"k\">table_name</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">list</span><span class=\"p\">.</span><span class=\"n\">table_schema</span><span class=\"o\">||</span><span class=\"s1\">'-'</span><span class=\"o\">||</span><span class=\"n\">list</span><span class=\"p\">.</span><span class=\"k\">table_name</span><span class=\"o\">||</span><span class=\"s1\">'_</span><span class=\"se\">''</span><span class=\"s1\"> iam_role </span><span class=\"se\">''</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"n\">iamrole</span><span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">''</span><span class=\"s1\"> delimiter </span><span class=\"se\">''</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"k\">delimiter</span><span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">''</span><span class=\"s1\"> MAXFILESIZE 300 MB PARALLEL ADDQUOTES HEADER GZIP'</span><span class=\"p\">;</span>        <span class=\"k\">EXECUTE</span> <span class=\"n\">unload_query</span><span class=\"p\">;</span>                 <span class=\"n\">copy_query</span> <span class=\"p\">:</span><span class=\"o\">=</span> <span class=\"s1\">'copy '</span><span class=\"o\">||</span><span class=\"n\">list</span><span class=\"p\">.</span><span class=\"n\">table_schema</span><span class=\"o\">||</span><span class=\"s1\">'.'</span><span class=\"o\">||</span><span class=\"n\">list</span><span class=\"p\">.</span><span class=\"k\">table_name</span><span class=\"o\">||</span><span class=\"s1\">' from </span><span class=\"se\">''</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"n\">s3_path</span><span class=\"o\">||</span><span class=\"n\">un_year</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">un_month</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">un_day</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">db</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">list</span><span class=\"p\">.</span><span class=\"n\">table_schema</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">list</span><span class=\"p\">.</span><span class=\"k\">table_name</span><span class=\"o\">||</span><span class=\"s1\">'/</span><span class=\"se\">''</span><span class=\"s1\"> iam_role </span><span class=\"se\">''</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"n\">iamrole</span><span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">''</span><span class=\"s1\"> delimiter </span><span class=\"se\">''</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"k\">delimiter</span><span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">''</span><span class=\"s1\"> IGNOREHEADER 1 REMOVEQUOTES gzip'</span><span class=\"p\">;</span>                <span class=\"k\">SELECT</span> <span class=\"n\">getdate</span><span class=\"p\">()</span>         <span class=\"k\">INTO</span>   <span class=\"n\">endtime</span><span class=\"p\">;</span>             <span class=\"k\">SELECT</span> <span class=\"n\">list</span><span class=\"p\">.</span><span class=\"n\">table_schema</span>         <span class=\"k\">INTO</span> <span class=\"n\">tableschema</span><span class=\"p\">;</span>            <span class=\"k\">SELECT</span> <span class=\"n\">list</span><span class=\"p\">.</span><span class=\"k\">table_name</span>         <span class=\"k\">INTO</span> <span class=\"n\">tablename</span><span class=\"p\">;</span>                 <span class=\"c1\">-- Insert into the history table</span>        <span class=\"k\">INSERT</span> <span class=\"k\">INTO</span> <span class=\"n\">unload_history</span>                     <span class=\"p\">(</span>                                 <span class=\"n\">u_id</span><span class=\"p\">,</span>                                 <span class=\"n\">u_timestamp</span><span class=\"p\">,</span>                                 <span class=\"n\">start_time</span><span class=\"p\">,</span>                                 <span class=\"n\">end_time</span><span class=\"p\">,</span>                                 <span class=\"n\">db_name</span><span class=\"p\">,</span>                                 <span class=\"n\">table_schema</span><span class=\"p\">,</span>                                <span class=\"k\">table_name</span><span class=\"p\">,</span>                                <span class=\"n\">export_query</span><span class=\"p\">,</span>                                <span class=\"n\">import_query</span>                     <span class=\"p\">)</span>                     <span class=\"k\">VALUES</span>                     <span class=\"p\">(</span>                                 <span class=\"n\">unload_id</span><span class=\"p\">,</span>                                 <span class=\"n\">unload_time</span><span class=\"p\">,</span>                                 <span class=\"n\">starttime</span><span class=\"p\">,</span>                                 <span class=\"n\">endtime</span><span class=\"p\">,</span>                                 <span class=\"n\">db</span><span class=\"p\">,</span>                                 <span class=\"n\">tableschema</span><span class=\"p\">,</span>                                <span class=\"n\">tablename</span><span class=\"p\">,</span>                                <span class=\"n\">unload_query</span><span class=\"p\">,</span>                                <span class=\"n\">copy_query</span>                    <span class=\"p\">);</span>              <span class=\"k\">END</span> <span class=\"n\">LOOP</span><span class=\"p\">;</span>       <span class=\"n\">RAISE</span> <span class=\"n\">info</span> <span class=\"s1\">' Unloading of the DB [%] is success !!!'</span> <span class=\"p\">,</span><span class=\"n\">db</span><span class=\"p\">;</span>    <span class=\"k\">END</span><span class=\"p\">;</span>     <span class=\"err\">$$</span><span class=\"p\">;</span></code></pre></figure><h2 id=\"hardcoded-items\">Hardcoded Items:</h2><p>In the stored procedure, I have hardcoded the follow parameters.</p><ul>  <li>IAM ROLE - <code class=\"language-html highlighter-rouge\">arn:aws:iam::123123123123:role/myredshiftrole</code></li>  <li>Delimiter - <code class=\"language-html highlighter-rouge\">|</code></li></ul><p>Also, the following Items are hardcoded in the Unload query. You can get these things as variable or hardcoded as per your convenient.</p><ul>  <li>MAXFILESIZE  - 100 MB</li>  <li>PARALLEL</li>  <li>ADDQUOTES</li>  <li>HEADER</li>  <li>GZIP</li></ul><h2 id=\"execute-the-procedure\">Execute the procedure:</h2><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>call unload_all('s3://bhuvi-datalake/test/');</code></pre></div></div><p>You can see the status in the terminal</p><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>INFO:  [2019-10-06 19:20:04] Unloading... schema = etl and table = tbl1INFO:  UNLOAD completed, 2 record(s) unloaded successfully.INFO:  [2019-10-06 19:20:10] Unloading... schema = etl and table = tbl2INFO:  UNLOAD completed, 2 record(s) unloaded successfully.INFO:  [2019-10-06 19:20:12] Unloading... schema = stage and table = tbl3INFO:  UNLOAD completed, 2 record(s) unloaded successfully.INFO:  [2019-10-06 19:20:12] Unloading... schema = stage and table = tbl4INFO:  UNLOAD completed, 2 record(s) unloaded successfully.INFO:  [2019-10-06 19:20:14] Unloading... schema = stage and table = tbl5INFO:  UNLOAD completed, 2 record(s) unloaded successfully.INFO:  [2019-10-06 19:20:15] Unloading... schema = prod and table = tbl6INFO:  UNLOAD completed, 2 record(s) unloaded successfully.INFO:  [2019-10-06 19:20:15] Unloading... schema = prod and table = tbl7INFO:  UNLOAD completed, 2 record(s) unloaded successfully.INFO:  [2019-10-06 19:20:15] Unloading... schema = public and table = debugINFO:  UNLOAD completed, 0 record(s) unloaded successfully.INFO:  [2019-10-06 19:20:16] Unloading... schema = public and table = testINFO:  UNLOAD completed, 1 record(s) unloaded successfully.INFO:   Unloading of the DB [preprod] is success !!!CALL</code></pre></div></div><h2 id=\"files-on-s3\">Files on S3:</h2><p><img src=\"/assets/RedShift Unload All Tables To S3_1.jpg\" alt=\"\" /></p><p><img src=\"/assets/RedShift Unload All Tables To S3_2.jpg\" alt=\"\" /></p><h2 id=\"retrieve-the-history\">Retrieve the History:</h2><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"n\">preprod</span><span class=\"o\">=#</span> <span class=\"k\">select</span> <span class=\"o\">*</span> <span class=\"k\">from</span> <span class=\"n\">unload_history</span> <span class=\"k\">limit</span> <span class=\"mi\">1</span><span class=\"p\">;</span>    <span class=\"n\">pid</span>          <span class=\"o\">|</span> <span class=\"mi\">28</span><span class=\"n\">u_id</span>         <span class=\"o\">|</span> <span class=\"mi\">1</span><span class=\"n\">u_timestamp</span>  <span class=\"o\">|</span> <span class=\"mi\">2019</span><span class=\"o\">-</span><span class=\"mi\">10</span><span class=\"o\">-</span><span class=\"mi\">08</span> <span class=\"mi\">10</span><span class=\"p\">:</span><span class=\"mi\">33</span><span class=\"p\">:</span><span class=\"mi\">23</span><span class=\"n\">start_time</span>   <span class=\"o\">|</span> <span class=\"mi\">2019</span><span class=\"o\">-</span><span class=\"mi\">10</span><span class=\"o\">-</span><span class=\"mi\">08</span> <span class=\"mi\">10</span><span class=\"p\">:</span><span class=\"mi\">33</span><span class=\"p\">:</span><span class=\"mi\">23</span><span class=\"n\">end_time</span>     <span class=\"o\">|</span> <span class=\"mi\">2019</span><span class=\"o\">-</span><span class=\"mi\">10</span><span class=\"o\">-</span><span class=\"mi\">08</span> <span class=\"mi\">10</span><span class=\"p\">:</span><span class=\"mi\">33</span><span class=\"p\">:</span><span class=\"mi\">23</span><span class=\"n\">db_name</span>      <span class=\"o\">|</span> <span class=\"n\">preprod</span><span class=\"n\">table_schema</span> <span class=\"o\">|</span> <span class=\"n\">etl</span><span class=\"k\">table_name</span>   <span class=\"o\">|</span> <span class=\"n\">tbl2</span><span class=\"n\">export_query</span> <span class=\"o\">|</span> <span class=\"n\">unload</span> <span class=\"p\">(</span><span class=\"s1\">'select * from etl.tbl2'</span><span class=\"p\">)</span> <span class=\"k\">to</span> <span class=\"s1\">'s3://bhuvi-datalake/test/2019/10/8/preprod/etl/tbl2/etl-tbl2_'</span> <span class=\"n\">iam_role</span> <span class=\"s1\">'arn:aws:iam::123123123:role/myredshiftrole'</span> <span class=\"k\">delimiter</span> <span class=\"s1\">'|'</span> <span class=\"n\">MAXFILESIZE</span> <span class=\"mi\">300</span> <span class=\"n\">MB</span> <span class=\"n\">PARALLEL</span> <span class=\"n\">ADDQUOTES</span> <span class=\"n\">HEADER</span> <span class=\"n\">GZIP</span><span class=\"n\">import_query</span> <span class=\"o\">|</span> <span class=\"k\">copy</span> <span class=\"n\">etl</span><span class=\"p\">.</span><span class=\"n\">tbl2</span> <span class=\"k\">from</span> <span class=\"s1\">'s3://bhuvi-datalake/test/2019/10/8/preprod/etl/tbl2/'</span> <span class=\"n\">iam_role</span> <span class=\"s1\">'arn:aws:iam::123123123:role/myredshiftrole'</span> <span class=\"k\">delimiter</span> <span class=\"s1\">'|'</span> <span class=\"n\">IGNOREHEADER</span> <span class=\"mi\">1</span> <span class=\"n\">REMOVEQUOTES</span> <span class=\"n\">gzip</span></code></pre></figure>",
            "url": "/2019/10/06/redshift-unload-all-tables-to-s3",
            "image": "/assets/RedShift Unload All Tables To S3.jpg",
            
            
            "tags": ["aws","redshift","s3","sql"],
            
            "date_published": "2019-10-06T19:40:00+00:00",
            "date_modified": "2019-10-06T19:40:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2019/10/01/how-gcp-browser-bases-ssh-works",
            "title": "How GCP Browser Based SSH Works",
            "summary": "A blog port that explains about how GCP browser based SSH works internally. You can access the VM without allowing your Public IP address. ",
            "content_text": "If you are using GCP platform and lazy to setup VPN or bastion host, then you may familiar with using SSH connection via a browser. Yeah, its just one click to login to the Linux VM. Here are some questions for you.  How many of you think how this actually works in the backend?  Can we access the VM (via browser SSH) without whitelist our Public IP address in the firewall rule?  Why the VM is not accessible via Browser SSH even though we whitelisted our Public IP?I had a situation to dig deeper into understanding this concept.My Problematic situation:I was launching a new VM in GCP and tagged with allow-office-ssh. Then created a Firewall rule which allows Port 22 from the IP address X.X.X.X(my office IP address) to the target tag allow-office-ssh. Once the VM came to an active state, I tried to telnet on port 22. It was working fine.But I don’t any username and password to login, So I thought to access the VM via browser then create the user. But unfortunately Im not able to access the VM via Browser.  This made me go deep into this functionality.How Browser SSH Works:Here is an illustration that I have in my mind how this actually works.  There is no need to allow your IP Address in the firewall rule.  The communication is happening between the VM and Google’s SSH Proxy(i don’t know the right term, so Im using the term proxy).  Your browser is just talking to the Proxy layer.  The Proxy layer is actually talking to the VM.  Its more or less a port forwarding or SSH tunneling.How GCP’s proxy can access the VM?As per GCP’s security policy, no one can access the VM without allowing their IP address. Then how this proxy is accessing it?There is no way to proxy to access the VM. But Google has some static IP ranges which are assigned to its proxy layer. By whitelisting that IP range, they can talk to your VM.From the GCP doc:  Source IP addresses for browser-based SSH sessions are dynamically allocated by GCP Console and can vary from session to session. For the feature to work, you must allow connections either from any IP address or from Google’s IP address range which you can retrieve using public SPF records.If you run the below commands, you can get the IP address range for proxy layer.nslookup -q=TXT _netblocks.google.com 8.8.8.8nslookup -q=TXT _netblocks2.google.com 8.8.8.8nslookup -q=TXT _netblocks3.google.com 8.8.8.8For eg:nslookup -q=TXT _netblocks.google.com 8.8.8.8Server:\t\t8.8.8.8Address:\t8.8.8.8#53Non-authoritative answer:_netblocks.google.com\ttext = \"v=spf1 ip4:35.190.247.0/24 ip4:64.233.160.0/19 ip4:66.102.0.0/20 ip4:66.249.80.0/20 ip4:72.14.192.0/18 ip4:74.125.0.0/16 ip4:108.177.8.0/21 ip4:173.194.0.0/16 ip4:209.85.128.0/17 ip4:216.58.192.0/19 ip4:216.239.32.0/19 ~all\"Authoritative answers can be found from:You can see a bunch of Public IP address. Right now we have the following IP ranges for accessing the VM via the browser.35.190.247.0/2464.233.160.0/1966.102.0.0/2066.249.80.0/2072.14.192.0/1874.125.0.0/16108.177.8.0/21173.194.0.0/16209.85.128.0/17216.58.192.0/19216.239.32.0/19172.217.0.0/19172.217.32.0/20172.217.128.0/19172.217.160.0/20172.217.192.0/19108.177.96.0/1935.191.0.0/16130.211.0.0/22Once you create a firewall rule which allows Port 22 from the above IP addresses, then we can access the VM via browser. Hope you learned something new today.My Personal Suggestion:But I always recommend that do not use this for production. Because I had some nightmares because of this. The VM goes offline due to the live host migration. After that, the services are not started. I tried to access it via browser. Unfortunately, that time GCP’s some services also down which establish the connection between the browser and the VM. So use VPN or bastion host for this.Happy SSH !!!",
            "content_html": "<p>If you are using GCP platform and lazy to setup VPN or bastion host, then you may familiar with using SSH connection via a browser. Yeah, its just one click to login to the Linux VM. Here are some questions for you.</p><ol>  <li>How many of you think how this actually works in the backend?</li>  <li>Can we access the VM (via browser SSH) without whitelist our Public IP address in the firewall rule?</li>  <li>Why the VM is not accessible via Browser SSH even though we whitelisted our Public IP?</li></ol><p>I had a situation to dig deeper into understanding this concept.</p><h2 id=\"my-problematic-situation\">My Problematic situation:</h2><p>I was launching a new VM in GCP and tagged with <code class=\"language-html highlighter-rouge\">allow-office-ssh</code>. Then created a Firewall rule which allows <code class=\"language-html highlighter-rouge\">Port 22</code> from the IP address <code class=\"language-html highlighter-rouge\">X.X.X.X</code>(my office IP address) to the target tag <code class=\"language-html highlighter-rouge\">allow-office-ssh</code>. Once the VM came to an active state, I tried to telnet on port 22. It was working fine.</p><p>But I don’t any username and password to login, So I thought to access the VM via browser then create the user. But unfortunately Im not able to access the VM via Browser.  This made me go deep into this functionality.</p><h2 id=\"how-browser-ssh-works\">How Browser SSH Works:</h2><p>Here is an illustration that I have in my mind how this actually works.</p><p><img src=\"/assets/How GCP Browser Based SSH Works.jpg\" alt=\"\" /></p><ol>  <li>There is no need to allow your IP Address in the firewall rule.</li>  <li>The communication is happening between the VM and Google’s SSH Proxy(i don’t know the right term, so Im using the term proxy).</li>  <li>Your browser is just talking to the Proxy layer.</li>  <li>The Proxy layer is actually talking to the VM.</li>  <li>Its more or less a port forwarding or SSH tunneling.</li></ol><h2 id=\"how-gcps-proxy-can-access-the-vm\">How GCP’s proxy can access the VM?</h2><p>As per GCP’s security policy, no one can access the VM without allowing their IP address. Then how this proxy is accessing it?</p><p>There is no way to proxy to access the VM. But Google has some static IP ranges which are assigned to its proxy layer. By whitelisting that IP range, they can talk to your VM.</p><p><strong>From the GCP doc:</strong></p><blockquote>  <p>Source IP addresses for browser-based SSH sessions are dynamically allocated by GCP Console and can vary from session to session. For the feature to work, you must allow connections either from any IP address or from Google’s IP address range which you can retrieve using <a href=\"https://support.google.com/a/answer/60764\">public SPF records</a>.</p></blockquote><p>If you run the below commands, you can get the IP address range for proxy layer.</p><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>nslookup -q=TXT _netblocks.google.com 8.8.8.8nslookup -q=TXT _netblocks2.google.com 8.8.8.8nslookup -q=TXT _netblocks3.google.com 8.8.8.8</code></pre></div></div><p><strong>For eg:</strong></p><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>nslookup -q=TXT _netblocks.google.com 8.8.8.8Server:\t\t8.8.8.8Address:\t8.8.8.8#53Non-authoritative answer:_netblocks.google.com\ttext = \"v=spf1 ip4:35.190.247.0/24 ip4:64.233.160.0/19 ip4:66.102.0.0/20 ip4:66.249.80.0/20 ip4:72.14.192.0/18 ip4:74.125.0.0/16 ip4:108.177.8.0/21 ip4:173.194.0.0/16 ip4:209.85.128.0/17 ip4:216.58.192.0/19 ip4:216.239.32.0/19 ~all\"Authoritative answers can be found from:</code></pre></div></div><p>You can see a bunch of Public IP address. Right now we have the following IP ranges for accessing the VM via the browser.</p><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>35.190.247.0/2464.233.160.0/1966.102.0.0/2066.249.80.0/2072.14.192.0/1874.125.0.0/16108.177.8.0/21173.194.0.0/16209.85.128.0/17216.58.192.0/19216.239.32.0/19172.217.0.0/19172.217.32.0/20172.217.128.0/19172.217.160.0/20172.217.192.0/19108.177.96.0/1935.191.0.0/16130.211.0.0/22</code></pre></div></div><p>Once you create a firewall rule which allows Port 22 from the above IP addresses, then we can access the VM via browser. Hope you learned something new today.</p><h2 id=\"my-personal-suggestion\">My Personal Suggestion:</h2><p>But I always recommend that do not use this for production. Because I had some nightmares because of this. The VM goes offline due to the <code class=\"language-html highlighter-rouge\">live host migration</code>. After that, the services are not started. I tried to access it via browser. Unfortunately, that time GCP’s some services also down which establish the connection between the browser and the VM. So use VPN or bastion host for this.</p><p>Happy SSH !!!</p>",
            "url": "/2019/10/01/how-gcp-browser-bases-ssh-works",
            "image": "/assets/How GCP Browser Based SSH Works-cover.jpg",
            
            
            "tags": ["gcp","ssh","security"],
            
            "date_published": "2019-10-01T18:32:00+00:00",
            "date_modified": "2019-10-01T18:32:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2019/10/01/cloudwatch-custom-log-filter-alarm-for-kinesis-load-failed-event",
            "title": "CloudWatch Custom Log Filter Alarm For Kinesis Load Failed Event",
            "summary": "Setup the cloudwatch custom log filter for sending email alert for kinesis firehose load failed event for RedShift",
            "content_text": "Kinesis Firehose is pushing the realtime data to S3, Redshift, ElasticSearch, and Splunk for realtime/Near real-time analytics. Sometime the target may not available due to maintenance or any reason. So the Kinesis will automatically push the data to S3 and create the manifest file in the errors directory. Then later we can reload the data into our targets. But unfortunately, there is no one-step action to set notification if the load is failed. In this blog, im writing how can we setup Cloudwatch custom log filter alarm for kinesis load failed events.Kinesis Firehose Setup:Im sending my clickstream data to Kinesis Firehose and then there is an intermediate S3 bucket to store the data in JSON format with GZIP compression. And then the data will go to RedShift for further analytics purpose.RedShift Failures in Kinesis:Kinesis will not send the data to Redshift in many cases. Here are some most common errors.  Redshift.AuthenticationFailed  Redshift.ConnectionFailed  Redshift.ReadOnlyCluster  Redshift.DiskFullThere are many other errors you can refer to the below Reference section to read more about the types of errors.Customer Log Filter in CloudWatch:To setup the email notification, we need to filter the Cloudwatch logs with the keyword errorCode and RedShift.  Go to Cloudwatch –&gt;  Logs  Click the radio button on /aws/kinesisfirehose/your-stream-name  Create Metric Filter.  Filter Pattern: [w1!=errorCode&amp;&amp;w1!=Redshift] ( This means the string errorCode and RedShift will be on the same line).  Select Log Data to Test: RedShiftDelivery  Test pattern. (it’ll give you some X number of matching lines)Now assign the metric.  Filter Name: kinesis-redshift-error  Filter Pattern: it’ll automatically select the pattern which we used in the previous step.  Metic Name: redshift-kinesis-error  Metic Value: 1  Click on the save filter.Create the Alarm:Once the clicked the Save Filter option you can see the window. Or you go to Cloudwatch –&gt; logs –&gt; /aws/kinesisfirehose/your-stream-name on  Metric Filters column you can see 1 filter.  Click on the Create Alarm link.  Under the metric option:  select the period as 10 seconds.  Conditions: Threshold type –&gt; Static  Define the alarm condition –&gt; Greater/Equal  Define the threshold value –&gt; 1  Under the Additional Configuration: Datapoints to alarm 1 out of 1  Missing data treatment: Treat Missing Data as Good.  Rest of the things are easy, you can select an SNS topic for sending an email alert.Why Treat Missing Data as Good?In the Cloudwatch, we’ll not get any logs unless kinesis gets some errors. So it’ll not get any values for the metric. Then your alarm will go to insufficient state. We are interested in only getting email alerts. So if my CloudWatch didn’t get any errors then this Alarm will go to OK state.Test this Alarm:For testing purpose, I changed my redshift password in Kinesis Firehose. Then I got this error from the Cloudwatch.CloudWatch Log:{    \"deliveryStreamARN\": \"arn:aws:firehose:ap-south-1:XXXXXXXX/Kinesis-test-stream\",    \"destination\": \"jdbc:redshift://XXXXXXXXXX.redshift.amazonaws.com:5439/test\",    \"deliveryStreamVersionId\": 11,    \"message\": \"The provided user name and password failed authentication. Provide valid user name and password.\",    \"errorCode\": \"Redshift.AuthenticationFailed\"}CloudWatch Alarm:You can see a blue line or a blue dot which indicates that the pattern matched.Email:Yes, and I got the email from SNS topic.Further References:  Learn more about all error events in Kinesis Firehose  Using AWS CloudWatch Alarms  CloudWatch Filter and Patten matching syntax  Other Cloudwatch metrics for Kinesis Firehose",
            "content_html": "<p>Kinesis Firehose is pushing the realtime data to S3, Redshift, ElasticSearch, and Splunk for realtime/Near real-time analytics. Sometime the target may not available due to maintenance or any reason. So the Kinesis will automatically push the data to S3 and create the manifest file in the <code class=\"language-html highlighter-rouge\">errors</code> directory. Then later we can reload the data into our targets. But unfortunately, there is no one-step action to set notification if the load is failed. In this blog, im writing how can we setup Cloudwatch custom log filter alarm for kinesis load failed events.</p><h2 id=\"kinesis-firehose-setup\">Kinesis Firehose Setup:</h2><p>Im sending my clickstream data to Kinesis Firehose and then there is an intermediate S3 bucket to store the data in <code class=\"language-html highlighter-rouge\">JSON</code> format with <code class=\"language-html highlighter-rouge\">GZIP</code> compression. And then the data will go to RedShift for further analytics purpose.</p><h2 id=\"redshift-failures-in-kinesis\">RedShift Failures in Kinesis:</h2><p>Kinesis will not send the data to Redshift in many cases. Here are some most common errors.</p><ol>  <li>Redshift.AuthenticationFailed</li>  <li>Redshift.ConnectionFailed</li>  <li>Redshift.ReadOnlyCluster</li>  <li>Redshift.DiskFull</li></ol><p>There are many other errors you can refer to the below Reference section to read more about the types of errors.</p><h2 id=\"customer-log-filter-in-cloudwatch\">Customer Log Filter in CloudWatch:</h2><p>To setup the email notification, we need to filter the Cloudwatch logs with the keyword <code class=\"language-html highlighter-rouge\">errorCode</code> and <code class=\"language-html highlighter-rouge\">RedShift</code>.</p><ul>  <li>Go to Cloudwatch –&gt;  Logs</li>  <li>Click the radio button on <code class=\"language-html highlighter-rouge\">/aws/kinesisfirehose/your-stream-name</code></li>  <li>Create Metric Filter.</li>  <li>Filter Pattern: <code class=\"language-html highlighter-rouge\">[w1!=errorCode<span class=\"err\">&amp;&amp;</span>w1!=Redshift] </code>( <em>This means the string errorCode and RedShift will be on the same line</em>).</li>  <li>Select Log Data to Test: RedShiftDelivery</li>  <li>Test pattern. (it’ll give you some X number of matching lines)</li></ul><p><img src=\"/assets/CloudWatch Custom Log Filter Alarm For Kinesis Load Failed Event1.jpg\" alt=\"\" /></p><p>Now assign the metric.</p><ul>  <li>Filter Name: kinesis-redshift-error</li>  <li>Filter Pattern: it’ll automatically select the pattern which we used in the previous step.</li>  <li>Metic Name: redshift-kinesis-error</li>  <li>Metic Value: 1</li>  <li>Click on the save filter.</li></ul><p><img src=\"/assets/CloudWatch Custom Log Filter Alarm For Kinesis Load Failed Event2.jpg\" alt=\"\" /></p><h2 id=\"create-the-alarm\">Create the Alarm:</h2><p>Once the clicked the Save Filter option you can see the window. Or you go to Cloudwatch –&gt; logs –&gt; /aws/kinesisfirehose/your-stream-name on  Metric Filters column you can see 1 filter.</p><p><img src=\"/assets/CloudWatch Custom Log Filter Alarm For Kinesis Load Failed Event4.jpg\" alt=\"\" /></p><p><img src=\"/assets/CloudWatch Custom Log Filter Alarm For Kinesis Load Failed Event3.jpg\" alt=\"\" /></p><ol>  <li>Click on the Create Alarm link.</li>  <li>Under the metric option:  select the period as 10 seconds.</li>  <li>Conditions: Threshold type –&gt; Static</li>  <li>Define the alarm condition –&gt; Greater/Equal</li>  <li>Define the threshold value –&gt; 1</li>  <li>Under the Additional Configuration: Datapoints to alarm <code class=\"language-html highlighter-rouge\">1 out of 1</code></li>  <li>Missing data treatment: Treat Missing Data as Good.</li>  <li>Rest of the things are easy, you can select an SNS topic for sending an email alert.</li></ol><p><img src=\"/assets/CloudWatch Custom Log Filter Alarm For Kinesis Load Failed Event5.jpg\" alt=\"\" /></p><p><img src=\"/assets/CloudWatch Custom Log Filter Alarm For Kinesis Load Failed Event8.jpg\" alt=\"\" /></p><h3 id=\"why-treat-missing-data-as-good\">Why Treat Missing Data as Good?</h3><p>In the Cloudwatch, we’ll not get any logs unless kinesis gets some errors. So it’ll not get any values for the metric. Then your alarm will go to insufficient state. We are interested in only getting email alerts. So if my CloudWatch didn’t get any errors then this Alarm will go to OK state.</p><h2 id=\"test-this-alarm\">Test this Alarm:</h2><p>For testing purpose, I changed my redshift password in Kinesis Firehose. Then I got this error from the Cloudwatch.</p><p><strong>CloudWatch Log:</strong></p><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>{    \"deliveryStreamARN\": \"arn:aws:firehose:ap-south-1:XXXXXXXX/Kinesis-test-stream\",    \"destination\": \"jdbc:redshift://XXXXXXXXXX.redshift.amazonaws.com:5439/test\",    \"deliveryStreamVersionId\": 11,    \"message\": \"The provided user name and password failed authentication. Provide valid user name and password.\",    \"errorCode\": \"Redshift.AuthenticationFailed\"}</code></pre></div></div><p><strong>CloudWatch Alarm:</strong></p><p>You can see a blue line or a blue dot which indicates that the pattern matched.</p><p><img src=\"/assets/CloudWatch Custom Log Filter Alarm For Kinesis Load Failed Event6.jpg\" alt=\"\" /></p><p><strong>Email:</strong></p><p>Yes, and I got the email from SNS topic.</p><p><img src=\"/assets/CloudWatch Custom Log Filter Alarm For Kinesis Load Failed Event7.jpg\" alt=\"\" /></p><h2 id=\"further-references\">Further References:</h2><ol>  <li><a href=\"https://docs.aws.amazon.com/firehose/latest/dev/monitoring-with-cloudwatch-logs.html\">Learn more about all error events in Kinesis Firehose</a></li>  <li><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html\">Using AWS CloudWatch Alarms</a></li>  <li><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html\">CloudWatch Filter and Patten matching syntax</a></li>  <li><a href=\"https://docs.aws.amazon.com/firehose/latest/dev/monitoring-with-cloudwatch-metrics.html\">Other Cloudwatch metrics for Kinesis Firehose</a></li></ol>",
            "url": "/2019/10/01/cloudwatch-custom-log-filter-alarm-for-kinesis-load-failed-event",
            "image": "/assets/CloudWatch Custom Log Filter Alarm For Kinesis Load Failed Event.jpeg",
            
            
            "tags": ["aws","kinesis","cloudwatch","monitoring","redshift"],
            
            "date_published": "2019-10-01T05:30:00+00:00",
            "date_modified": "2019-10-01T05:30:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2019/09/22/relationalize-unstructured-data-in-aws-athena-with-grokserde",
            "title": "Relationalize Unstructured Data In AWS Athena with GrokSerDe",
            "summary": "We can relationalize unstructured data like syslog, SQL Server error log or cloudwatch export in AWS Athena with GrokSerDe.",
            "content_text": "Managing the logs in a centralized repository is one of the most common best practices in the DevOps world. Application logs, system logs, error logs, and any databases logs also will be pushed into your centralized repository. You can use ELK stack or Splunk to visualize the logs to get better insights about it. But as a SQL guy, I wanted to solve this problem with Bigdata ecosystem(use SQL). As a part of that process, we can relationalize unstructured data in AWS Athena with the help of GrokSerDe.Here S3 is my centralized repository. I know it will not scale like ElasticSearch, but why should I miss this Fun. For this use case, Im going to rationalize the SQL Server Error log in AWS Athena. Let’s take a look at the SQL server’s error log pattern.2019-09-21 12:53:17.57 Server      UTC adjustment: 0:002019-09-21 12:53:17.57 Server      (c) Microsoft Corporation.2019-09-21 12:53:17.57 Server      All rights reserved.2019-09-21 12:53:17.57 Server      Server process ID is 4152.Its looks likeyyyy-mm-dd space hh:mm:ss:ms space User space messageBut sometimes, it has many lines like below.2019-09-21 12:53:17.57 Server      Microsoft SQL Server 2017 (RTM) - 14.0.1000.169 (X64)   Aug 22 2017 17:04:49   Copyright (C) 2017 Microsoft Corporation  Enterprise Edition: Core-based Licensing (64-bit) on Windows Server 2016 Datacenter 10.0 &lt;X64&gt; (Build 14393: ) (Hypervisor)2019-09-21 12:53:17.57 Server      UTC adjustment: 0:002019-09-21 12:53:17.57 Server      (c) Microsoft Corporation.If you see the 2nd, 3rd line we have the only message. And we know these all are just for information purpose, we’ll not get any useful information with that. Also as a part of Data cleansing, we should clean up some unwanted lines to make this relationalize.I can consider the below format for my relationalize structure.  Year - Integer  Month - Integer  Day - Integer  Hour - Integer  Minute - Integer  Second - Integer  User - String  Message - StringWe can convert this into a Grok pattern for this.%{YEAR:year}-%{MONTHNUM:month}-%{MONTHDAY:day}\\\\s*%{TIME:time} %{LOG_LEVEL:user}\\\\s*( )*%{GREEDYDATA:message}Create the table in Athena:CREATE EXTERNAL TABLE `sql_errorlog`(  `year` string ,   `month` string ,   `day` string ,   `time` string ,   `user` string ,   `message` string )  ROW FORMAT SERDE   'com.amazonaws.glue.serde.GrokSerDe' WITH SERDEPROPERTIES (   'input.format'='%{YEAR:year}-%{MONTHNUM:month}-%{MONTHDAY:day}\\\\s*%{TIME:time} %{LOG_LEVEL:user}\\\\s*( )*%{GREEDYDATA:message}', 'input.grokCustomPatterns'='LOG_LEVEL \\[a-zA-Z0-9\\]*') STORED AS INPUTFORMAT   'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT   'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'LOCATION  's3://bhuvi-datalake/sql-error-log/'The table got created. I used a custom pattern for pulling the user column.Query the data:SELECT *FROM \"default\".\"sql_errorlog\" limit 10;SELECT *FROM \"default\".\"sql_errorlog\"WHERE message LIKE '%shutdown%';SELECT *FROM \"default\".\"sql_errorlog\"WHERE message LIKE '%Login failed%'SELECT concat ('Server started at: ',year,'-',month,'-',day,' ',time) AS StartupTimeFROM \"default\".\"sql_errorlog\"WHERE message LIKE '%Server process ID is%';This is just a beginner guide, you can play around with windows logs, linux syslog, if you are a DBA then you may like to use this for MySQL, PostgreSQL, MongoDB logs.BONUS: Regex SerdeIf you are a developer, then regex might be easy for you. You can create a table with Regex Serde. Thanks to LeftJoin Who helped to write this RegexCREATE EXTERNAL TABLE `bhuvi`(  `date` string ,   `time` string ,   `user` string ,   `message` string )ROW FORMAT SERDE   'org.apache.hadoop.hive.serde2.RegexSerDe' WITH SERDEPROPERTIES (   'input.regex'='(.*\\\\-.*\\\\-.*)\\\\s+(\\\\d+:\\\\d+:\\\\d+.\\\\d+)\\\\s+(\\\\S+)\\\\s+(.*?)$') STORED AS INPUTFORMAT   'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT   'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'LOCATION  's3://bhuvi-datalake/bhuvi-1/'References:  Manage the SQL server workload with Customer Cloudwatch metrics.  AWS Athena Grok Serde.",
            "content_html": "<p>Managing the logs in a centralized repository is one of the most common best practices in the DevOps world. Application logs, system logs, error logs, and any databases logs also will be pushed into your centralized repository. You can use ELK stack or Splunk to visualize the logs to get better insights about it. But as a SQL guy, I wanted to solve this problem with Bigdata ecosystem(use SQL). As a part of that process, we can relationalize unstructured data in AWS Athena with the help of GrokSerDe.</p><p>Here S3 is my centralized repository. I know it will not scale like ElasticSearch, but why should I miss this Fun. For this use case, Im going to rationalize the SQL Server Error log in AWS Athena. Let’s take a look at the SQL server’s error log pattern.</p><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>2019-09-21 12:53:17.57 Server      UTC adjustment: 0:002019-09-21 12:53:17.57 Server      (c) Microsoft Corporation.2019-09-21 12:53:17.57 Server      All rights reserved.2019-09-21 12:53:17.57 Server      Server process ID is 4152.</code></pre></div></div><p>Its looks like</p><p><code class=\"language-html highlighter-rouge\">yyyy-mm-dd</code> space <code class=\"language-html highlighter-rouge\">hh:mm:ss:ms</code> space <code class=\"language-html highlighter-rouge\">User</code> space <code class=\"language-html highlighter-rouge\">message</code></p><p>But sometimes, it has many lines like below.</p><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>2019-09-21 12:53:17.57 Server      Microsoft SQL Server 2017 (RTM) - 14.0.1000.169 (X64)   Aug 22 2017 17:04:49   Copyright (C) 2017 Microsoft Corporation  Enterprise Edition: Core-based Licensing (64-bit) on Windows Server 2016 Datacenter 10.0 <span class=\"nt\">&lt;X64&gt;</span> (Build 14393: ) (Hypervisor)2019-09-21 12:53:17.57 Server      UTC adjustment: 0:002019-09-21 12:53:17.57 Server      (c) Microsoft Corporation.</code></pre></div></div><p>If you see the 2nd, 3rd line we have the only message. And we know these all are just for information purpose, we’ll not get any useful information with that. Also as a part of Data cleansing, we should clean up some unwanted lines to make this relationalize.</p><p>I can consider the below format for my relationalize structure.</p><ul>  <li>Year - Integer</li>  <li>Month - Integer</li>  <li>Day - Integer</li>  <li>Hour - Integer</li>  <li>Minute - Integer</li>  <li>Second - Integer</li>  <li>User - String</li>  <li>Message - String</li></ul><p><strong>We can convert this into a Grok pattern for this.</strong></p><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>%{YEAR:year}-%{MONTHNUM:month}-%{MONTHDAY:day}\\\\s*%{TIME:time} %{LOG_LEVEL:user}\\\\s*( )*%{GREEDYDATA:message}</code></pre></div></div><h2 id=\"create-the-table-in-athena\">Create the table in Athena:</h2><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>CREATE EXTERNAL TABLE `sql_errorlog`(  `year` string ,   `month` string ,   `day` string ,   `time` string ,   `user` string ,   `message` string )  ROW FORMAT SERDE   'com.amazonaws.glue.serde.GrokSerDe' WITH SERDEPROPERTIES (   'input.format'='%{YEAR:year}-%{MONTHNUM:month}-%{MONTHDAY:day}\\\\s*%{TIME:time} %{LOG_LEVEL:user}\\\\s*( )*%{GREEDYDATA:message}', 'input.grokCustomPatterns'='LOG_LEVEL \\[a-zA-Z0-9\\]*') STORED AS INPUTFORMAT   'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT   'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'LOCATION  's3://bhuvi-datalake/sql-error-log/'</code></pre></div></div><p>The table got created. I used a custom pattern for pulling the user column.</p><h2 id=\"query-the-data\">Query the data:</h2><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>SELECT *FROM \"default\".\"sql_errorlog\" limit 10;</code></pre></div></div><p><img src=\"/assets/Relationalize Unstructured Data In AWS Athena with GrokSerDe-1.jpg\" alt=\"\" /></p><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>SELECT *FROM \"default\".\"sql_errorlog\"WHERE message LIKE '%shutdown%';</code></pre></div></div><p><img src=\"/assets/Relationalize Unstructured Data In AWS Athena with GrokSerDe-2.jpg\" alt=\"\" /></p><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>SELECT *FROM \"default\".\"sql_errorlog\"WHERE message LIKE '%Login failed%'</code></pre></div></div><p><img src=\"/assets/Relationalize Unstructured Data In AWS Athena with GrokSerDe-3.jpg\" alt=\"\" /></p><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>SELECT concat ('Server started at: ',year,'-',month,'-',day,' ',time) AS StartupTimeFROM \"default\".\"sql_errorlog\"WHERE message LIKE '%Server process ID is%';</code></pre></div></div><p><img src=\"/assets/Relationalize Unstructured Data In AWS Athena with GrokSerDe-4.jpg\" alt=\"\" /></p><p>This is just a beginner guide, you can play around with windows logs, linux syslog, if you are a DBA then you may like to use this for MySQL, PostgreSQL, MongoDB logs.</p><h2 id=\"bonus-regex-serde\">BONUS: Regex Serde</h2><p>If you are a developer, then regex might be easy for you. You can create a table with Regex Serde. Thanks to LeftJoin <a href=\"https://stackoverflow.com/users/2700344/leftjoin\">Who helped to write this Regex</a></p><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>CREATE EXTERNAL TABLE `bhuvi`(  `date` string ,   `time` string ,   `user` string ,   `message` string )ROW FORMAT SERDE   'org.apache.hadoop.hive.serde2.RegexSerDe' WITH SERDEPROPERTIES (   'input.regex'='(.*\\\\-.*\\\\-.*)\\\\s+(\\\\d+:\\\\d+:\\\\d+.\\\\d+)\\\\s+(\\\\S+)\\\\s+(.*?)$') STORED AS INPUTFORMAT   'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT   'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'LOCATION  's3://bhuvi-datalake/bhuvi-1/'</code></pre></div></div><p><img src=\"/assets/Relationalize Unstructured Data In AWS Athena with GrokSerDe-5.jpg\" alt=\"\" /></p><h2 id=\"references\">References:</h2><ol>  <li><a href=\"https://aws.amazon.com/blogs/database/monitor-your-microsoft-sql-server-using-custom-metrics-with-amazon-cloudwatch-and-aws-systems-manager/\">Manage the SQL server workload with Customer Cloudwatch metrics.</a></li>  <li><a href=\"https://docs.aws.amazon.com/athena/latest/ug/grok.html\">AWS Athena Grok Serde.</a></li></ol>",
            "url": "/2019/09/22/relationalize-unstructured-data-in-aws-athena-with-grokserde",
            "image": "/assets/Relationalize Unstructured Data In AWS Athena with GrokSerDe.jpg",
            
            
            "tags": ["aws","athena","bigdata","Grok"],
            
            "date_published": "2019-09-22T18:30:00+00:00",
            "date_modified": "2019-09-22T18:30:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2019/08/27/redshift-unload-to-s3-with-partitions-stored-procedure-way",
            "title": "RedShift Unload to S3 With Partitions - Stored Procedure Way",
            "summary": "We can use redshift stored procedure to execute unload command and save the data in S3 with partitions. ",
            "content_text": "Redshift unload is the fastest way to export the data from Redshift cluster. In BigData world, generally people use the data in S3 for DataLake. So its important that we need to make sure the data in S3 should be partitioned. So we can use Athena, RedShift Spectrum or EMR External tables to access that data in an optimized way. If you are dealing with multiple tables, then you can loop the table names in a shell script or Python code. But as a SQL guy, I choose stored procedures to do this. It made export process simple.In this procedure I just used the options which are suitable for me, but you can use the same procedure and do whatever customization you want. Also you can track the activity of this unload in a metadata table.    CREATE TABLE unload_meta (    \tid INT IDENTITY(1, 1)    \t,tablename VARCHAR(500)    \t,start_time DATETIME    \t,end_time DATETIME    \t,export_query VARCHAR(65500)    \t);Define the parameters:  starttime - timestamp when the process started  endtime - timestamp when the process end  sql - SQL Query that you want to export its results to S3.  s3_path - location of S3 with prefix. Make sure you have end this string with /.  iamrole - IAM role ARN which has s3 write access.  delimiter - If you are exporting as CSV, you can define your delimiter.  un_year - Partition YEAR  un_month - Partition MONTH  un_day - Partition DAYIn this procedure, I used GETDATE() function to pass current day, month, year into partition variables. If you want custom one, you can get these variables from input in stored procedure.    CREATE OR REPLACE PROCEDURE sp_unload(v_tablename varchar)    LANGUAGE plpgsql AS    $$    DECLARE       starttime datetime;       endtime datetime;       sql text;       s3_path varchar(1000);       iamrole varchar(100);       delimiter varchar(10);       unload_query text;       max_filesize varchar(100);       un_year int;       un_month int;       un_day int;        BEGIN              select extract(year from  GETDATE()) into un_year;       select extract(month from  GETDATE()) into un_month;       select extract(day from  GETDATE()) into un_day;       select GETDATE() into starttime;              sql:='select * from '||v_tablename||'' ;       s3_path:='s3://bhuvi-datalake/clicksteam/';       iamrole:='arn:aws:iam::123123123123:role/myredshiftrole';       delimiter:='|';       unload_query := 'unload ('''||sql||''') to '''||s3_path||un_year||'/'||un_month||'/'||un_day||'/'||v_tablename||'_'' iam_role '''||iamrole||''' delimiter '''||delimiter||''' MAXFILESIZE 100 MB PARALLEL ADDQUOTES HEADER GZIP';             execute unload_query;       select GETDATE() into endtime;           Insert into unload_meta (tablename, start_time, end_time, export_query) values (v_tablename,starttime,endtime,unload_query);        END    $$;  Here, Im getting the table name from input. Then I used multiple options like parallel, max file size, include headers and compress. If you don’t want to use this, you can remove these options from the unload_query. Also if you need you can get the s3 location and other parameters from the user input. You can do many customization here.Lets try this.    test=# call sp_unload('bhuvi');    INFO:  UNLOAD completed, 400000 record(s) unloaded successfully.    CALL    test=#Get the unload History from Meta table:select * from unload_meta;id           | 1tablename    | bhuvistart_time   | 2019-08-27 03:42:57end_time     | 2019-08-27 03:43:03export_query | unload ('select * from bhuvi') to 's3://bhuvi-datalake/clicksteam/2019/8/27/bhuvi_' iam_role 'arn:aws:iam::123123123:role/myredshiftrole' delimiter '|' MAXFILESIZE 100 MB PARALLEL ADDQUOTES HEADER GZIPIn my next blog, I’ll write about how to automate this Unload Process in AWS Glue and convert the CSV to Parquet format.Update:I have written the updated version of this stored procedure to unload all of the tables in a database to S3.You can read it here: https://thedataguy.in/redshift-unload-all-tables-to-s3/",
            "content_html": "<p>Redshift unload is the fastest way to export the data from Redshift cluster. In BigData world, generally people use the data in S3 for DataLake. So its important that we need to make sure the data in S3 should be partitioned. So we can use Athena, RedShift Spectrum or EMR External tables to access that data in an optimized way. If you are dealing with multiple tables, then you can loop the table names in a shell script or Python code. But as a SQL guy, I choose stored procedures to do this. It made export process simple.</p><p>In this procedure I just used the options which are suitable for me, but you can use the same procedure and do whatever customization you want. Also you can track the activity of this unload in a metadata table.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\">    <span class=\"k\">CREATE</span> <span class=\"k\">TABLE</span> <span class=\"n\">unload_meta</span> <span class=\"p\">(</span>    \t<span class=\"n\">id</span> <span class=\"nb\">INT</span> <span class=\"k\">IDENTITY</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>    \t<span class=\"p\">,</span><span class=\"n\">tablename</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">500</span><span class=\"p\">)</span>    \t<span class=\"p\">,</span><span class=\"n\">start_time</span> <span class=\"nb\">DATETIME</span>    \t<span class=\"p\">,</span><span class=\"n\">end_time</span> <span class=\"nb\">DATETIME</span>    \t<span class=\"p\">,</span><span class=\"n\">export_query</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">65500</span><span class=\"p\">)</span>    \t<span class=\"p\">);</span></code></pre></figure><p>Define the parameters:</p><ul>  <li><code class=\"language-html highlighter-rouge\">starttime</code> - timestamp when the process started</li>  <li><code class=\"language-html highlighter-rouge\">endtime</code> - timestamp when the process end</li>  <li><code class=\"language-html highlighter-rouge\">sql</code> - SQL Query that you want to export its results to S3.</li>  <li><code class=\"language-html highlighter-rouge\">s3_path</code> - location of S3 with prefix. Make sure you have end this string with <code class=\"language-html highlighter-rouge\">/</code>.</li>  <li><code class=\"language-html highlighter-rouge\">iamrole</code> - IAM role ARN which has s3 write access.</li>  <li><code class=\"language-html highlighter-rouge\">delimiter</code> - If you are exporting as CSV, you can define your delimiter.</li>  <li><code class=\"language-html highlighter-rouge\">un_year</code> - Partition YEAR</li>  <li><code class=\"language-html highlighter-rouge\">un_month</code> - Partition MONTH</li>  <li><code class=\"language-html highlighter-rouge\">un_day</code> - Partition DAY</li></ul><p>In this procedure, I used <code class=\"language-html highlighter-rouge\">GETDATE()</code> function to pass current day, month, year into partition variables. If you want custom one, you can get these variables from input in stored procedure.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\">    <span class=\"k\">CREATE</span> <span class=\"k\">OR</span> <span class=\"k\">REPLACE</span> <span class=\"k\">PROCEDURE</span> <span class=\"n\">sp_unload</span><span class=\"p\">(</span><span class=\"n\">v_tablename</span> <span class=\"nb\">varchar</span><span class=\"p\">)</span>    <span class=\"k\">LANGUAGE</span> <span class=\"n\">plpgsql</span> <span class=\"k\">AS</span>    <span class=\"err\">$$</span>    <span class=\"k\">DECLARE</span>       <span class=\"n\">starttime</span> <span class=\"nb\">datetime</span><span class=\"p\">;</span>       <span class=\"n\">endtime</span> <span class=\"nb\">datetime</span><span class=\"p\">;</span>       <span class=\"k\">sql</span> <span class=\"nb\">text</span><span class=\"p\">;</span>       <span class=\"n\">s3_path</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">);</span>       <span class=\"n\">iamrole</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">);</span>       <span class=\"k\">delimiter</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">);</span>       <span class=\"n\">unload_query</span> <span class=\"nb\">text</span><span class=\"p\">;</span>       <span class=\"n\">max_filesize</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">);</span>       <span class=\"n\">un_year</span> <span class=\"nb\">int</span><span class=\"p\">;</span>       <span class=\"n\">un_month</span> <span class=\"nb\">int</span><span class=\"p\">;</span>       <span class=\"n\">un_day</span> <span class=\"nb\">int</span><span class=\"p\">;</span>        <span class=\"k\">BEGIN</span>              <span class=\"k\">select</span> <span class=\"k\">extract</span><span class=\"p\">(</span><span class=\"nb\">year</span> <span class=\"k\">from</span>  <span class=\"n\">GETDATE</span><span class=\"p\">())</span> <span class=\"k\">into</span> <span class=\"n\">un_year</span><span class=\"p\">;</span>       <span class=\"k\">select</span> <span class=\"k\">extract</span><span class=\"p\">(</span><span class=\"k\">month</span> <span class=\"k\">from</span>  <span class=\"n\">GETDATE</span><span class=\"p\">())</span> <span class=\"k\">into</span> <span class=\"n\">un_month</span><span class=\"p\">;</span>       <span class=\"k\">select</span> <span class=\"k\">extract</span><span class=\"p\">(</span><span class=\"k\">day</span> <span class=\"k\">from</span>  <span class=\"n\">GETDATE</span><span class=\"p\">())</span> <span class=\"k\">into</span> <span class=\"n\">un_day</span><span class=\"p\">;</span>       <span class=\"k\">select</span> <span class=\"n\">GETDATE</span><span class=\"p\">()</span> <span class=\"k\">into</span> <span class=\"n\">starttime</span><span class=\"p\">;</span>              <span class=\"k\">sql</span><span class=\"p\">:</span><span class=\"o\">=</span><span class=\"s1\">'select * from '</span><span class=\"o\">||</span><span class=\"n\">v_tablename</span><span class=\"o\">||</span><span class=\"s1\">''</span> <span class=\"p\">;</span>       <span class=\"n\">s3_path</span><span class=\"p\">:</span><span class=\"o\">=</span><span class=\"s1\">'s3://bhuvi-datalake/clicksteam/'</span><span class=\"p\">;</span>       <span class=\"n\">iamrole</span><span class=\"p\">:</span><span class=\"o\">=</span><span class=\"s1\">'arn:aws:iam::123123123123:role/myredshiftrole'</span><span class=\"p\">;</span>       <span class=\"k\">delimiter</span><span class=\"p\">:</span><span class=\"o\">=</span><span class=\"s1\">'|'</span><span class=\"p\">;</span>       <span class=\"n\">unload_query</span> <span class=\"p\">:</span><span class=\"o\">=</span> <span class=\"s1\">'unload (</span><span class=\"se\">''</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"k\">sql</span><span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">''</span><span class=\"s1\">) to </span><span class=\"se\">''</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"n\">s3_path</span><span class=\"o\">||</span><span class=\"n\">un_year</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">un_month</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">un_day</span><span class=\"o\">||</span><span class=\"s1\">'/'</span><span class=\"o\">||</span><span class=\"n\">v_tablename</span><span class=\"o\">||</span><span class=\"s1\">'_</span><span class=\"se\">''</span><span class=\"s1\"> iam_role </span><span class=\"se\">''</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"n\">iamrole</span><span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">''</span><span class=\"s1\"> delimiter </span><span class=\"se\">''</span><span class=\"s1\">'</span><span class=\"o\">||</span><span class=\"k\">delimiter</span><span class=\"o\">||</span><span class=\"s1\">'</span><span class=\"se\">''</span><span class=\"s1\"> MAXFILESIZE 100 MB PARALLEL ADDQUOTES HEADER GZIP'</span><span class=\"p\">;</span>             <span class=\"k\">execute</span> <span class=\"n\">unload_query</span><span class=\"p\">;</span>       <span class=\"k\">select</span> <span class=\"n\">GETDATE</span><span class=\"p\">()</span> <span class=\"k\">into</span> <span class=\"n\">endtime</span><span class=\"p\">;</span>           <span class=\"k\">Insert</span> <span class=\"k\">into</span> <span class=\"n\">unload_meta</span> <span class=\"p\">(</span><span class=\"n\">tablename</span><span class=\"p\">,</span> <span class=\"n\">start_time</span><span class=\"p\">,</span> <span class=\"n\">end_time</span><span class=\"p\">,</span> <span class=\"n\">export_query</span><span class=\"p\">)</span> <span class=\"k\">values</span> <span class=\"p\">(</span><span class=\"n\">v_tablename</span><span class=\"p\">,</span><span class=\"n\">starttime</span><span class=\"p\">,</span><span class=\"n\">endtime</span><span class=\"p\">,</span><span class=\"n\">unload_query</span><span class=\"p\">);</span>        <span class=\"k\">END</span>    <span class=\"err\">$$</span><span class=\"p\">;</span>  </code></pre></figure><p>Here, Im getting the table name from input. Then I used multiple options like parallel, max file size, include headers and compress. If you don’t want to use this, you can remove these options from the <code class=\"language-html highlighter-rouge\">unload_query</code>. Also if you need you can get the s3 location and other parameters from the user input. You can do many customization here.</p><p>Lets try this.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    <span class=\"nb\">test</span><span class=\"o\">=</span><span class=\"c\"># call sp_unload('bhuvi');</span>    INFO:  UNLOAD completed, 400000 record<span class=\"o\">(</span>s<span class=\"o\">)</span> unloaded successfully.    CALL    <span class=\"nb\">test</span><span class=\"o\">=</span><span class=\"c\">#</span></code></pre></figure><p><img src=\"/assets/RedShift Unload to S3 With Partitions.png\" alt=\"\" /></p><p>Get the unload History from Meta table:</p><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>select * from unload_meta;id           | 1tablename    | bhuvistart_time   | 2019-08-27 03:42:57end_time     | 2019-08-27 03:43:03export_query | unload ('select * from bhuvi') to 's3://bhuvi-datalake/clicksteam/2019/8/27/bhuvi_' iam_role 'arn:aws:iam::123123123:role/myredshiftrole' delimiter '|' MAXFILESIZE 100 MB PARALLEL ADDQUOTES HEADER GZIP</code></pre></div></div><p>In my next blog, I’ll write about how to automate this Unload Process in AWS Glue and convert the CSV to Parquet format.</p><h2 id=\"update\">Update:</h2><p>I have written the updated version of this stored procedure to unload all of the tables in a database to S3.You can read it here: <a href=\"https://thedataguy.in/redshift-unload-all-tables-to-s3/\">https://thedataguy.in/redshift-unload-all-tables-to-s3/</a></p>",
            "url": "/2019/08/27/redshift-unload-to-s3-with-partitions-stored-procedure-way",
            "image": "/assets/RedShift Unload to S3 With Partitions - Stored Procedure Way.jpg",
            
            
            "tags": ["aws","redshift","s3","sql"],
            
            "date_published": "2019-08-27T04:17:00+00:00",
            "date_modified": "2019-08-27T04:17:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2019/08/24/mysql-convert-binlog-based-replication-to-gtid-replication-without-downtime",
            "title": "MySQL Convert Binlog Based Replication To GTID Replication Without Downtime",
            "summary": "MySQL using binlog file and position for replication. For setting up GTID based replication we need to use master_auto_position is 1. ",
            "content_text": "This title may be suitable for the new age MySQL Users. Because in 5.7 onwards its already supported to enable GTID online. But still few of my mission critical databases are in 5.6 and handling 70k QPS. So I know enabling GTID needs downtime for this. But in my case, the GTID has been already implemented. But still the replication is using Binlog file name and position for replicating the data.This is my slave status. You can see the GTID has been enabled but the Auto_Position is still 0 which means still my replication is binlog filename and position. No issues with the replication. But the MySQL world already moved to GTID for better control on replication and Failover.             Master_Server_Id: 10010                  Master_UUID: c924545b-a3e3-11e8-8a39-42010a280410             Master_Info_File: mysql.slave_master_info                    SQL_Delay: 0          SQL_Remaining_Delay: NULL      Slave_SQL_Running_State:            Master_Retry_Count: 86400                  Master_Bind:       Last_IO_Error_Timestamp:      Last_SQL_Error_Timestamp:                Master_SSL_Crl:            Master_SSL_Crlpath:            Retrieved_Gtid_Set: c924545b-a3e3-11e8-8a39-42010a280410:1021245412-5365162807            Executed_Gtid_Set: c924545b-a3e3-11e8-8a39-42010a280410:4975917719-5053294256:5053294258-5365162769                Auto_Position: 0Im using MySQL Orchestrator for Failover. And we need to use either MySQL GTID for pesudo GTID for failover. I did googling for how can I change this auto position to 1 without breaking the replication. Initially I thought just get the last executed the GTID set from the slave status and purge it. Then change the auto position to 1. But it was a bad idea.WHY?I stopped the slave thread and got this last executed GTID from the slave status.c924545b-a3e3-11e8-8a39-42010a280410:3501467-3659834Then I ran,set global gtid_purged='c924545b-a3e3-11e8-8a39-42010a280410:3501467-3659834'; So what happened here is, it only purges ID 3501467 and 3659834.  Then if I start the slave thread, it’ll get the below error message.Got fatal error 1236 from master when reading data from binary log:  'The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1, but the  master has purged binary logs containing GTIDs that the slave requires.'MySQL tried to ignore the IDs But we want to purge all the Is till 3659834. Its not a big deal who all are familiar with GTID. It just a logical way thinking :)Just try to purge the GTID set beginning from 1. Follow these steps to convert binlog position based to auto position.  DISCLAIMER: This command invokes RESET MASTER. If you have any application or the slave is acting as a master for another slave(Cascading replication), then you will be fired from your organization. So make sure this is only a slave role and its binlog is not used for any streaming or other replication process.mysql&gt; stop slave;Query OK, 0 rows affected (0.05 sec)Get the binlog filename, position, GTID. So if something goes wrong we can resume the replication with binlog file name and position.mysql&gt; show slave status\\G;            Master_Log_File: mysql-bin.012187          Read_Master_Log_Pos: 450020919               Relay_Log_File: mysqld-relay-bin.007983                Relay_Log_Pos: 450002463        Relay_Master_Log_File: mysql-bin.012187             Slave_IO_Running: No            Slave_SQL_Running: No          Exec_Master_Log_Pos: 450002253              Relay_Log_Space: 450021421           Retrieved_Gtid_Set: c924545b-a3e3-11e8-8a39-42010a280410:1021245412-5365162807            Executed_Gtid_Set: c924545b-a3e3-11e8-8a39-42010a280410:4975917719-5053294256:5053294258-5365162769                Auto_Position: 0Reset the master and enable the GTID.mysql&gt; SHOW GLOBAL VARIABLES LIKE 'gtid%';+---------------+----------------------------------------------------------------------------------+| Variable_name | Value                                                                            |+---------------+----------------------------------------------------------------------------------+| gtid_executed | c924545b-a3e3-11e8-8a39-42010a280410:4975917719-5053294256:5053294258-5365162769 || gtid_mode     | ON                                                                               || gtid_owned    |                                                                                  || gtid_purged   |                                                                                  |+---------------+----------------------------------------------------------------------------------+4 rows in set (0.00 sec)mysql&gt; reset master;Query OK, 0 rows affected (17.23 sec)mysql&gt; set global gtid_purged='c924545b-a3e3-11e8-8a39-42010a280410:4975917719-5053294256:1-5365162769';Query OK, 0 rows affected (0.06 sec)mysql&gt; change master to master_auto_position=1;Query OK, 0 rows affected (0.15 sec)mysql&gt; start slave;Query OK, 0 rows affected (0.00 sec)mysql&gt; show slave status\\GExecuted_Gtid_Set: c924545b-a3e3-11e8-8a39-42010a280410:1-5365162769Auto_Position: 1",
            "content_html": "<p>This title may be suitable for the new age MySQL Users. Because in 5.7 onwards its already supported to enable GTID online. But still few of my mission critical databases are in 5.6 and handling <code class=\"language-html highlighter-rouge\">70k QPS</code>. So I know enabling GTID needs downtime for this. But in my case, the GTID has been already implemented. But still the replication is using Binlog file name and position for replicating the data.</p><p>This is my slave status. You can see the GTID has been enabled but the <code class=\"language-html highlighter-rouge\">Auto_Position</code> is still 0 which means still my replication is binlog filename and position. No issues with the replication. But the MySQL world already moved to GTID for better control on replication and Failover.</p><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>             Master_Server_Id: 10010                  Master_UUID: c924545b-a3e3-11e8-8a39-42010a280410             Master_Info_File: mysql.slave_master_info                    SQL_Delay: 0          SQL_Remaining_Delay: NULL      Slave_SQL_Running_State:            Master_Retry_Count: 86400                  Master_Bind:       Last_IO_Error_Timestamp:      Last_SQL_Error_Timestamp:                Master_SSL_Crl:            Master_SSL_Crlpath:            Retrieved_Gtid_Set: c924545b-a3e3-11e8-8a39-42010a280410:1021245412-5365162807            Executed_Gtid_Set: c924545b-a3e3-11e8-8a39-42010a280410:4975917719-5053294256:5053294258-5365162769                Auto_Position: 0</code></pre></div></div><p>Im using MySQL Orchestrator for Failover. And we need to use either <code class=\"language-html highlighter-rouge\">MySQL GTID</code> for <code class=\"language-html highlighter-rouge\">pesudo GTID</code> for failover. I did googling for how can I change this auto position to 1 without breaking the replication. Initially I thought just get the last executed the GTID set from the slave status and purge it. Then change the auto position to 1. But it was a bad idea.</p><h3 id=\"why\">WHY?</h3><p>I stopped the slave thread and got this last executed GTID from the slave status.</p><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>c924545b-a3e3-11e8-8a39-42010a280410:3501467-3659834</code></pre></div></div><p>Then I ran,</p><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>set global gtid_purged='c924545b-a3e3-11e8-8a39-42010a280410:3501467-3659834'; </code></pre></div></div><p>So what happened here is, it only purges ID 3501467 and 3659834.  Then if I start the slave thread, it’ll get the below error message.</p><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>Got fatal error 1236 from master when reading data from binary log:  'The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1, but the  master has purged binary logs containing GTIDs that the slave requires.'</code></pre></div></div><p>MySQL tried to ignore the IDs But we want to purge all the Is till 3659834. Its not a big deal who all are familiar with GTID. It just a logical way thinking :)</p><p>Just try to purge the GTID set beginning from 1. Follow these steps to convert binlog position based to auto position.</p><blockquote>  <p><strong>DISCLAIMER</strong>: This command invokes <code class=\"language-html highlighter-rouge\">RESET MASTER</code>. If you have any application or the slave is acting as a master for another slave(Cascading replication), then you will be fired from your organization. So make sure this is only a slave role and its binlog is not used for any streaming or other replication process.</p></blockquote><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>mysql&gt; stop slave;Query OK, 0 rows affected (0.05 sec)</code></pre></div></div><p>Get the binlog filename, position, GTID. So if something goes wrong we can resume the replication with binlog file name and position.</p><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>mysql&gt; show slave status\\G;            Master_Log_File: mysql-bin.012187          Read_Master_Log_Pos: 450020919               Relay_Log_File: mysqld-relay-bin.007983                Relay_Log_Pos: 450002463        Relay_Master_Log_File: mysql-bin.012187             Slave_IO_Running: No            Slave_SQL_Running: No          Exec_Master_Log_Pos: 450002253              Relay_Log_Space: 450021421           Retrieved_Gtid_Set: c924545b-a3e3-11e8-8a39-42010a280410:1021245412-5365162807            Executed_Gtid_Set: c924545b-a3e3-11e8-8a39-42010a280410:4975917719-5053294256:5053294258-5365162769                Auto_Position: 0</code></pre></div></div><p>Reset the master and enable the GTID.</p><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>mysql&gt; SHOW GLOBAL VARIABLES LIKE 'gtid%';+---------------+----------------------------------------------------------------------------------+| Variable_name | Value                                                                            |+---------------+----------------------------------------------------------------------------------+| gtid_executed | c924545b-a3e3-11e8-8a39-42010a280410:4975917719-5053294256:5053294258-5365162769 || gtid_mode     | ON                                                                               || gtid_owned    |                                                                                  || gtid_purged   |                                                                                  |+---------------+----------------------------------------------------------------------------------+4 rows in set (0.00 sec)mysql&gt; reset master;Query OK, 0 rows affected (17.23 sec)mysql&gt; set global gtid_purged='c924545b-a3e3-11e8-8a39-42010a280410:4975917719-5053294256:1-5365162769';Query OK, 0 rows affected (0.06 sec)mysql&gt; change master to master_auto_position=1;Query OK, 0 rows affected (0.15 sec)mysql&gt; start slave;Query OK, 0 rows affected (0.00 sec)mysql&gt; show slave status\\GExecuted_Gtid_Set: c924545b-a3e3-11e8-8a39-42010a280410:1-5365162769Auto_Position: 1</code></pre></div></div>",
            "url": "/2019/08/24/mysql-convert-binlog-based-replication-to-gtid-replication-without-downtime",
            "image": "/assets/MySQL Convert Binlog Based Replication To GTID Replication Without Downtime-cover.png",
            
            
            "tags": ["mysql","replication","gtid"],
            
            "date_published": "2019-08-24T07:58:00+00:00",
            "date_modified": "2019-08-24T07:58:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2019/08/19/mongodb-add-node-to-replica-set-without-initial-sync-in-gcp-aws",
            "title": "MongoDB Add Node To Replica Set Without Initial Sync In GCP/AWS",
            "summary": "This blog describes how can we add a new node to MongoDB replica set without initial sync in GCP or AWS with a huge amount of data",
            "content_text": "Adding a new node to the MongoDB replica set with huge amount of data will take a lot of time to perform the initial sync. Recently I was working on a replica set where I need to replace all the nodes in the existing shard and add a new node to the shard’s replica set. The data size is 3TB on each shard. The entire infra is on GCP. I planned to do this activity with native approach, but later I realized it won’t work as I expected.The native approach:In this approach, I just added the new node the replica set.PRIMARY&gt; rs.add(\"new-node:27017\")After 24hrs, I checked the amount of data that has been synced. You know what, its just 100GB. Even my nodes are having 16 core CPU and better network bandwidth.  In GCP, we can get the network throughput upto 16Gbps and its depends on the CPU Core. One core supports 2Gbps. So I can get the maximum throughput with my current configuration.In AWS, its a different case, we have separate instance types.MongoDB supports file system level backup to resync the replica set. Because Journal files will help to make the data is consistent.  FROM MONGODB:  Sync by Copying Data Files from Another Member  This approach “seeds” a new or stale member using the data files from an existing member of the replica set. The data files must be sufficiently recent to allow the new member to catch up with the oplog. Otherwise the member would need to perform an initial sync.  Also make sure the journal files should be located on the same data disk.Yes, I have journaling enabled on all the mongodb nodes.Rsync:This time, I want use rsync command to manually sync the files from any one of the secondary server to the new node. I know it’ll end up inconsistency files. But rsync knows which files got modified and we can run the rsync one more time. So it’ll sync the modified files.The plan was,  Run rsync from Secondary server to the new server.  Once the 1st rsync has been done, then stop the mongodb service on Secondary node.  Run the rsync again. This time it’ll sync only the modified files.  Start the mongodb service on the secondary once the rsync is done.  Start the mongodb service on the new node.root@new-node# rsync -Pavz -e \"ssh -i key.pem\" bhuvi@10.10.10.10:/mongodb/data/ /mongodb/data/This time, again the bandwidth problem. I got upto 200Mbps only. After 2.3TB it reduced to 10Mbps.GCS:This is same as the above method, but instead of sync the files between servers, this time sync to GCS using gsutil Its a command line to which supports rsync.root@new-node# gsutil -m rsync -r /mongodb/data/ gs://mongo-migration/secondary-node/In this case, the data transfer is pretty fast. I got 500Mbps to 1Gbps. So the sync was done in few hours.  But few journal data files were not uploaded due to permission issue. Even I added the root user to mongod group.  Some files were not uploaded, because that time those files are in use and was doing some changes.So inconsistent files which end up with corruption on the data files.2019-08-14T13:09:56.407+0000 E STORAGE  [initandlisten] WiredTiger error (-31803) [1565788196:406419][28166:0x7feda45c4600], file:WiredTiger.wt, WT_CURSOR.next: __schema_create_collapse, 111: metadata information for source configuration \"colgroup:collection-29962-5646082836428872281\" not found: WT_NOTFOUND: item not foundSnapshots:If you are familiar with public cloud platform then you already know that Snapshots are not consistent backups. I have already faced some issues with this snapshots when I used it for PostgreSQL. (Here is a blog about that)  NOTE Once the snapshot restored, it’ll not give the complete performance, it’s need some time to restore all the blocks. But yes, you can add it as a secondary node, then later you can promote this node as Master.But in mongodb, its a different case, Journal files will make your data consistent. So lets give a try.  Take snapshot of the data volume from the existing secondary server.  Create a volume from the snapshot.  Attach this volume to the New node.  Mount it on the dbpath and assign the permissions.  Make sure the mongod.conf has replica set and shard details.  Start the mongodb on the new node.  Add the new node to the replica set.In my case, I used CentOS for the new node. Here are the list of commands we need to use.On the new node:service mongod stopmount /dev/sdb /mongodb/data/  (replace the /dev/sdb with your block device and mount point as your dbpath)chown -R mongod:mongod /mongodb/data/ chcon system_u:object_r:mongod_var_lib_t:s0 /mongodb/data restorecon /mongodb/dataIf you are using mongodb with Key file auth for replication, then assign the below permissions for the key file.chown 400 mongodb-keyfile chown mongod:mongod mongodb-keyfile chcon system_u:object_r:mongod_var_lib_t:s0 mongodb-keyfileservice mongod startOn Primary:PRIMARY&gt; rs.add(\"new-node:27017\")It’ll take some time to catch up the lag. Then we are ready to use the new node.I remember 2 years before, I have done this on AWS. https://dba.stackexchange.com/a/164391/105575",
            "content_html": "<p>Adding a new node to the MongoDB replica set with huge amount of data will take a lot of time to perform the initial sync. Recently I was working on a replica set where I need to replace all the nodes in the existing shard and add a new node to the shard’s replica set. The data size is 3TB on each shard. The entire infra is on GCP. I planned to do this activity with native approach, but later I realized it won’t work as I expected.</p><h2 id=\"the-native-approach\">The native approach:</h2><p>In this approach, I just added the new node the replica set.</p><div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>PRIMARY&gt; rs.add<span class=\"o\">(</span><span class=\"s2\">\"new-node:27017\"</span><span class=\"o\">)</span></code></pre></div></div><p>After 24hrs, I checked the amount of data that has been synced. You know what, its just 100GB. Even my nodes are having 16 core CPU and better network bandwidth.</p><blockquote>  <p>In GCP, we can get the network throughput upto 16Gbps and its depends on the CPU Core. One core supports 2Gbps. So I can get the maximum throughput with my current configuration.</p></blockquote><p>In AWS, its a different case, we have separate instance types.</p><p>MongoDB supports file system level backup to resync the replica set. Because Journal files will help to make the data is consistent.</p><blockquote>  <h3 id=\"from-mongodb\">FROM MONGODB:</h3>  <p>Sync by Copying Data Files from Another Member</p>  <p>This approach “seeds” a new or stale member using the data files from an existing member of the replica set. The data files <strong>must</strong> be sufficiently recent to allow the new member to catch up with the <a href=\"https://docs.mongodb.com/manual/reference/glossary/#term-oplog\">oplog</a>. Otherwise the member would need to perform an initial sync.</p></blockquote><blockquote>  <p>Also make sure the journal files should be located on the same data disk.</p></blockquote><p>Yes, I have journaling enabled on all the mongodb nodes.</p><h2 id=\"rsync\">Rsync:</h2><p>This time, I want use rsync command to manually sync the files from any one of the secondary server to the new node. I know it’ll end up inconsistency files. But rsync knows which files got modified and we can run the rsync one more time. So it’ll sync the modified files.</p><p>The plan was,</p><ol>  <li>Run rsync from Secondary server to the new server.</li>  <li>Once the 1st rsync has been done, then stop the mongodb service on Secondary node.</li>  <li>Run the rsync again. This time it’ll sync only the modified files.</li>  <li>Start the mongodb service on the secondary once the rsync is done.</li>  <li>Start the mongodb service on the new node.</li></ol><div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>root@new-node# rsync <span class=\"nt\">-Pavz</span> <span class=\"nt\">-e</span> <span class=\"s2\">\"ssh -i key.pem\"</span> bhuvi@10.10.10.10:/mongodb/data/ /mongodb/data/</code></pre></div></div><p>This time, again the bandwidth problem. I got upto 200Mbps only. After 2.3TB it reduced to 10Mbps.</p><h3 id=\"gcs\">GCS:</h3><p>This is same as the above method, but instead of sync the files between servers, this time sync to GCS using <code class=\"language-html highlighter-rouge\">gsutil</code> Its a command line to which supports rsync.</p><div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>root@new-node# gsutil <span class=\"nt\">-m</span> rsync <span class=\"nt\">-r</span> /mongodb/data/ gs://mongo-migration/secondary-node/</code></pre></div></div><p>In this case, the data transfer is pretty fast. I got 500Mbps to 1Gbps. So the sync was done in few hours.</p><ul>  <li>But few journal data files were not uploaded due to permission issue. Even I added the root user to mongod group.</li>  <li>Some files were not uploaded, because that time those files are in use and was doing some changes.</li></ul><p>So inconsistent files which end up with corruption on the data files.</p><div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>2019-08-14T13:09:56.407+0000 E STORAGE  [initandlisten] WiredTiger error (-31803) [1565788196:406419][28166:0x7feda45c4600], file:WiredTiger.wt, WT_CURSOR.next: __schema_create_collapse, 111: metadata information for source configuration \"colgroup:collection-29962-5646082836428872281\" not found: WT_NOTFOUND: item not found</code></pre></div></div><h2 id=\"snapshots\">Snapshots:</h2><p>If you are familiar with public cloud platform then you already know that Snapshots are not consistent backups. I have already faced some issues with this snapshots when I used it for PostgreSQL. (<a href=\"https://thedataguy.in/dont-use-aws-ami-backup-ec2-database-server/\">Here is a blog about that</a>)</p><blockquote>  <p><strong>NOTE</strong> Once the snapshot restored, it’ll not give the complete performance, it’s need some time to restore all the blocks. But yes, you can add it as a secondary node, then later you can promote this node as Master.</p></blockquote><p>But in mongodb, its a different case, Journal files will make your data consistent. So lets give a try.</p><ol>  <li>Take snapshot of the data volume from the existing secondary server.</li>  <li>Create a volume from the snapshot.</li>  <li>Attach this volume to the New node.</li>  <li>Mount it on the <code class=\"language-html highlighter-rouge\">dbpath</code> and assign the permissions.</li>  <li>Make sure the <code class=\"language-html highlighter-rouge\">mongod.conf</code> has replica set and shard details.</li>  <li>Start the mongodb on the new node.</li>  <li>Add the new node to the replica set.</li></ol><p>In my case, I used CentOS for the new node. Here are the list of commands we need to use.</p><h4 id=\"on-the-new-node\">On the new node:</h4><div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>service mongod stopmount /dev/sdb /mongodb/data/  <span class=\"o\">(</span>replace the /dev/sdb with your block device and mount point as your dbpath<span class=\"o\">)</span><span class=\"nb\">chown</span> <span class=\"nt\">-R</span> mongod:mongod /mongodb/data/ <span class=\"nb\">chcon </span>system_u:object_r:mongod_var_lib_t:s0 /mongodb/data restorecon /mongodb/data</code></pre></div></div><p>If you are using mongodb with Key file auth for replication, then assign the below permissions for the key file.</p><div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code><span class=\"nb\">chown </span>400 mongodb-keyfile <span class=\"nb\">chown </span>mongod:mongod mongodb-keyfile <span class=\"nb\">chcon </span>system_u:object_r:mongod_var_lib_t:s0 mongodb-keyfileservice mongod start</code></pre></div></div><h4 id=\"on-primary\">On Primary:</h4><div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"syntax\"><code>PRIMARY&gt; rs.add<span class=\"o\">(</span><span class=\"s2\">\"new-node:27017\"</span><span class=\"o\">)</span></code></pre></div></div><p>It’ll take some time to catch up the lag. Then we are ready to use the new node.</p><p>I remember 2 years before, I have done this on AWS. <a href=\"https://dba.stackexchange.com/a/164391/105575\">https://dba.stackexchange.com/a/164391/105575</a></p>",
            "url": "/2019/08/19/mongodb-add-node-to-replica-set-without-initial-sync-in-gcp-aws",
            "image": "/assets/MongoDB Add Node To Replica Set Without Initial Sync In GCP_gcp.jpg",
            
            
            "tags": ["gcp","mongodb","aws","snapshot","replication"],
            
            "date_published": "2019-08-19T18:30:00+00:00",
            "date_modified": "2019-08-19T18:30:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2019/08/08/database-mirroring-is-still-a-mystery",
            "title": "Database Mirroring is still a Mystery",
            "summary": "The mistery in Database mirroring between SQL Server 2014 Standard and SQL Server 2017 Enterprise",
            "content_text": "Database Mirroring - A replication feature which supports automatic failover and supported in standard as well. During SQL server 2016, Microsoft announced that Mirror will be removed from further releases. But still, its there and Documentations are showing it’s going to deprecate soon. The Alwayson Availability Groups is the permanent High availability solution for SQL server.Then why am I writing this blog?Im doing a lot of database migrations to the Cloud. We can do this migration in many ways.  Backup and restore - More downtime.  Log Shipping - Lot of jobs needs to be created if you have more databases.  Log backup restore script - DBAtools or you can automate your script to restore log backups instead of going with native log shipping.  Mirroring - Need same version and edition for both Primary and Mirror.  Alwayson - Enterprise feature, Need active directory, cluster and etc.Every method has its own prod and cons. So for migrations, mostly I use Alwayson. But if the database infra is a small one, then I prefer Mirroring. Im sharing my horror story of migrating the SQL server 2014 standard to SQL server 2017 Enterprise.Background of the DB server:  SQL server 2014 RTM Standard.  60+ databases.  Mirroring has been configured with Witness.  Overall size 1TB.  NO active directory setup.Initial Migration Plan:  Im going to migrate this database server from On-Prem to GCP.  Target Database version SQL Server 2017 Enterprise.  On Primary server, stop mirroring and convert the existing Mirroring setup to log shipping. Here is an awesome blog for this.  Recreate Mirroring Endpoint with Certificate based authentication.  Setup Mirroring between SQL Server 2014 to 2017.I broke the existing Mirroring and created both endpoints with certificate based authentication. I took a small database’s full and log backup then restored it on Mirrored server.Adventure 1:My wrong estimation that SQL Server Mirroring supports higher versions, but failback is not possible. It was wrong, we must use the same edition.Adventure 2:We can’t setup this using SSMS, but no one will stop me to setup mirroring using T-SQL. Yes, I did it. But wait, again the database state changed from Mirror/Synchronized to Restoring. My bad!!!I tried to setup the mirroring again, but no luck within 5 to 10 seconds it was breaking.And here is the error from both servers.Database mirroring connection error 4 'An error occurred while receiving data: '10054(An existing connection was forcibly closed by the remote host.)'.' for 'TCP://10.10.10.14:5022'.Database mirroring has been terminated for database 'db_name'. This is an informational message only. No user action is required.After some research, we found this useful information from DBA StackOverflow and its answered by  Ola Hallengren.  Its a known bug and reported already.Adventure 3:So now we figured it out its a bug. But the strange thing is, we found that this bug is in SQL Server 2014, 2016 and SQL server 2017 RTM. We need to Patch our Primary and Mirror Server as well.Adventure 4:I already broke the replication. So no HA for now. If I do the Patch, its downtime. But its a very busy online booking system for travel, hotel and etc. So we got RED signal from the business team. So not even one minute of downtime for patch.Adventure 5:Here I was trying to mirror between two different versions of SQL Server and its not recommended. OK, let try with the exact version of SQL server version. I launched the SQL Server 2014 Standard from the GCP’s application images. It has the latest SP and CU. Shocking !!!It’s working fine.The Miracle:From the above statement, the SQL Server version and Editions are the same. But the only change is, it has the latest CU and SP.  Then let’s try SQL server 2017 Enterprise, do Patch there and then Mirror it. Woohoo!!!Its fixed.  But don’t sit back and relax, it worked only for 50% of the databases. Rest of the databases are still throwing the same error.Adventure 6:In Mirroring setup, to take Log backups, I used Ola Hallengren backup script. But the FULL backup I restored is taken from the SQL server Maintenance wizard. They configured this for Full backup and Compression is the Only Option is enabled.One more Miracle:Then I wanted to try the Mirroring once on the failed databases. I took a fresh copy of FULL and LOG backup using Ola Hallengren script. Then I setup the mirroring. And finally, It worked.Last but not least:Mirroring is running fine. But unfortunately, we can’t see the replication lag. Unsent / Unrestored log size. Even in Mirroring Monitor, its showing Secondary is not connected. So no other way to check the lag. But one option is there. sp_dbmmonitorresults will return the status of the mirroring.Lessons learned:Every migration has a lot of fun and adventures. Every time we are learning new.  I know SQL server always on can be established from lower version to higher version only for the version upgrade or migration. But mirroring will not officially recommend this practice.  Before testing/Poc, don’t break the existing HA setup for any reason.  Keep your SQL server in RTM is very Bad even the database is performing well.  Make sure you have at least the latest SP.  I don’t know why people still using SQL server maintenance wizard to setup backups. But whatever backup job you are using make sure Verify and checksum options are enabled.  But in my case, still its not clear how the mirroring worked after I took backups from the custom script.      Setting up mirroring between two different versions, will never show the lag, but sp_dbmmonitorresults will tell you the status of mirroring.        The best practice, before doing the cutover, just promote one of the Mirrored databases and check you have the latest data.  A simple trick, before doing the failover, create a test table and see its replication or not after the failover.",
            "content_html": "<p><strong>Database Mirroring</strong> - A replication feature which supports automatic failover and supported in standard as well. During SQL server 2016, Microsoft announced that Mirror will be removed from further releases. But still, its there and Documentations are showing it’s going to deprecate soon. The Alwayson Availability Groups is the permanent High availability solution for SQL server.</p><p>Then why am I writing this blog?</p><p>Im doing a lot of database migrations to the Cloud. We can do this migration in many ways.</p><ol>  <li>Backup and restore - More downtime.</li>  <li>Log Shipping - Lot of jobs needs to be created if you have more databases.</li>  <li>Log backup restore script - DBAtools or you can automate your script to restore log backups instead of going with native log shipping.</li>  <li>Mirroring - Need same version and edition for both Primary and Mirror.</li>  <li>Alwayson - Enterprise feature, Need active directory, cluster and etc.</li></ol><p>Every method has its own prod and cons. So for migrations, mostly I use Alwayson. But if the database infra is a small one, then I prefer Mirroring. Im sharing my horror story of migrating the SQL server 2014 standard to SQL server 2017 Enterprise.</p><h2 id=\"background-of-the-db-server\">Background of the DB server:</h2><ul>  <li>SQL server 2014 RTM Standard.</li>  <li>60+ databases.</li>  <li>Mirroring has been configured with Witness.</li>  <li>Overall size 1TB.</li>  <li>NO active directory setup.</li></ul><h2 id=\"initial-migration-plan\">Initial Migration Plan:</h2><ol>  <li>Im going to migrate this database server from On-Prem to GCP.</li>  <li>Target Database version SQL Server 2017 Enterprise.</li>  <li>On Primary server, stop mirroring and convert the existing Mirroring setup to log shipping. <a href=\"https://www.sqlservercentral.com/articles/convert-mirroring-to-log-shipping\">Here is an awesome blog for this</a>.</li>  <li>Recreate Mirroring Endpoint with Certificate based authentication.</li>  <li>Setup Mirroring between SQL Server 2014 to 2017.</li></ol><p>I broke the existing Mirroring and created both endpoints with certificate based authentication. I took a small database’s full and log backup then restored it on Mirrored server.</p><h3 id=\"adventure-1\">Adventure 1:</h3><p>My wrong estimation that SQL Server Mirroring supports higher versions, but failback is not possible. It was wrong, we must use the same edition.</p><h3 id=\"adventure-2\">Adventure 2:</h3><p>We can’t setup this using SSMS, but no one will stop me to setup mirroring using T-SQL. Yes, I did it. But wait, again the database state changed from Mirror/Synchronized to Restoring. My bad!!!</p><p>I tried to setup the mirroring again, but no luck within 5 to 10 seconds it was breaking.And here is the error from both servers.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">Database mirroring connection error 4 <span class=\"s1\">'An error occurred while receiving data: '</span>10054<span class=\"o\">(</span>An existing connection was forcibly closed by the remote host.<span class=\"o\">)</span><span class=\"s1\">'.'</span> <span class=\"k\">for</span> <span class=\"s1\">'TCP://10.10.10.14:5022'</span><span class=\"nb\">.</span>Database mirroring has been terminated <span class=\"k\">for </span>database <span class=\"s1\">'db_name'</span><span class=\"nb\">.</span> This is an informational message only. No user action is required.</code></pre></figure><p>After some research, we found this useful information from <a href=\"https://dba.stackexchange.com/a/161710/105575\">DBA StackOverflow</a> and its answered by  Ola Hallengren.  Its a known bug and reported already.</p><h3 id=\"adventure-3\">Adventure 3:</h3><p>So now we figured it out its a bug. But the strange thing is, we found that this bug is in <code class=\"language-html highlighter-rouge\">SQL Server 2014, 2016 and SQL server 2017 RTM</code>. We need to Patch our Primary and Mirror Server as well.</p><h3 id=\"adventure-4\">Adventure 4:</h3><p>I already broke the replication. So no HA for now. If I do the Patch, its downtime. But its a very busy online booking system for travel, hotel and etc. So we got RED signal from the business team. So not even one minute of downtime for patch.</p><h3 id=\"adventure-5\">Adventure 5:</h3><p>Here I was trying to mirror between two different versions of SQL Server and its not recommended. OK, let try with the exact version of SQL server version. I launched the SQL Server 2014 Standard from the GCP’s application images. It has the latest SP and CU. Shocking !!!</p><p>It’s working fine.</p><h3 id=\"the-miracle\">The Miracle:</h3><p>From the above statement, the SQL Server version and Editions are the same. But the only change is, it has the latest CU and SP.  Then let’s try SQL server 2017 Enterprise, do Patch there and then Mirror it. Woohoo!!!</p><p>Its fixed.  But don’t sit back and relax, it worked only for 50% of the databases. Rest of the databases are still throwing the same error.</p><h3 id=\"adventure-6\">Adventure 6:</h3><p>In Mirroring setup, to take Log backups, I used <a href=\"https://ola.hallengren.com/sql-server-backup.html\">Ola Hallengren backup script</a>. But the FULL backup I restored is taken from the SQL server Maintenance wizard. They configured this for Full backup and Compression is the Only Option is enabled.</p><p><img src=\"/assets/Database Mirroring is still a Mystery1.png\" alt=\"\" /></p><h3 id=\"one-more-miracle\">One more Miracle:</h3><p>Then I wanted to try the Mirroring once on the failed databases. I took a fresh copy of FULL and LOG backup using Ola Hallengren script. Then I setup the mirroring. And finally, It worked.</p><h3 id=\"last-but-not-least\">Last but not least:</h3><p>Mirroring is running fine. But unfortunately, we can’t see the replication lag. Unsent / Unrestored log size. Even in Mirroring Monitor, its showing Secondary is not connected. So no other way to check the lag. But one option is there. sp_dbmmonitorresults will return the status of the mirroring.</p><h3 id=\"lessons-learned\">Lessons learned:</h3><p>Every migration has a lot of fun and adventures. Every time we are learning new.</p><ol>  <li>I know SQL server always on can be established from lower version to higher version only for the version upgrade or migration. But mirroring will not officially recommend this practice.</li>  <li>Before testing/Poc, don’t break the existing HA setup for any reason.</li>  <li>Keep your SQL server in RTM is very Bad even the database is performing well.  Make sure you have at least the latest SP.</li>  <li>I don’t know why people still using SQL server maintenance wizard to setup backups. But whatever backup job you are using make sure Verify and checksum options are enabled.</li>  <li>But in my case, still its not clear how the mirroring worked after I took backups from the custom script.</li>  <li>    <p>Setting up mirroring between two different versions, will never show the lag, but sp_dbmmonitorresults will tell you the status of mirroring.</p>    <p><img src=\"/assets/Database Mirroring is still a Mystery2.png\" alt=\"\" /></p>  </li>  <li>The best practice, before doing the cutover, just promote one of the Mirrored databases and check you have the latest data.  A simple trick, before doing the failover, create a test table and see its replication or not after the failover.</li></ol>",
            "url": "/2019/08/08/database-mirroring-is-still-a-mystery",
            "image": "/assets/Database Mirroring is still a Mystery.jpg",
            
            
            "tags": ["sqlserver","mirroring","migration","GCP","Backup"],
            
            "date_published": "2019-08-08T04:30:00+00:00",
            "date_modified": "2019-08-08T04:30:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2019/05/16/monitor-cassandra-clusters-with-percona-pmm-jmx-grafana-and-prometheus",
            "title": "Monitor Cassandra Clusters with Percona PMM - JMX Grafana and Prometheus",
            "summary": "Monitoring Cassandra clusters using JMX exporter is good, But here we used Grafana, prometheus with percona PMM monitoring. ",
            "content_text": "While reading this title you may think about what this guy is going to do? Its all about JMX exporter from prometheus and Grafana. Yes, its already implemented by many companies and Grafana has some cool dashboards. But as a DBA, in Searce we are managing many customers and all of them are using many types of databases. So from a DBA’s perspective to monitor  all databases in one place is always a great thing right. If you are a MySQL DBA, then you must have heard about PMM. Its an awesome monitoring tool and its open source. Also it has Dashboards for Linux metrics, MongoDB and PostgreSQL.I remember, 2 years back(2017) I was trying to setup monitoring for a huge Cassandra cluster with MX4J. It was very hard that time to understand the metrics. But now, when I stated using PMM, I became a big fan of it. So I want to integrate the Cassandra monitoring dashboard with PMM.Before starting this, I was searching about how JMX exporter is behaving and will make any trouble for my cluster and etc. Unfortunately it does in few cases. Then I found this amazing custom JMX exporter which is developed by Criteo.  Its has better control over the metrics. So I decided to use this.Stage 1: Install cassandra_exporterYou don’t need to change anything from your Cassandra’s setting or cassandra-env.shDownload the exporter:1234 mkdir /opt/cassandra_exportercd  /opt/cassandra_exporterwget https://github.com/criteo/cassandra_exporter/releases/download/2.2.1/cassandra_exporter-2.2.1-all.jarmv cassandra_exporter-2.2.1-all.jar  cassandra_exporter.jar Create config filevi config.ymlhost: localhost:7199ssl: Falseuser:password:listenAddress: 0.0.0.0listenPort: 8080blacklist:   - java:lang:memorypool:.*usagethreshold.*   - .*:999thpercentile   - .*:95thpercentile   - .*:fifteenminuterate   - .*:fiveminuterate   - .*:durationunit   - .*:rateunit   - .*:stddev   - .*:meanrate   - .*:mean   - .*:min   - com:criteo:nosql:cassandra:exporter:.*maxScrapFrequencyInSec:  10:    - .*  300:    - .*:snapshotssize:.*    - .*:estimated.*    - .*:totaldiskspaceused:.*  blacklist - These metrics are never been collected.  maxScrapFrequencyInSec - Metrics collection frequency.          Here all the metrics are collected every 10sec and metrics are under 300 are collected every 300 sec.      Start the Exporter:I have executed this using nohup but you can create a service for this.nohup java -jar /opt/cassandra_exporter/cassandra_exporter.jar config.yml &amp;Stage 2:Install and Configure PMM Server:docker pull percona/pmm-server:1docker create \\   -v /opt/prometheus/data \\   -v /opt/consul-data \\   -v /var/lib/mysql \\   -v /var/lib/grafana \\   --name pmm-data \\   percona/pmm-server:1 /bin/truedocker run -d \\   -p 80:80 \\   --volumes-from pmm-data \\   --name pmm-server \\   --restart always \\   percona/pmm-server:1  More customized installation: https://www.percona.com/doc/percona-monitoring-and-management/deploy/server/docker.setting-up.htmlInstall PMM Client on all Cassandra Nodes:DEB Package:wget https://repo.percona.com/apt/percona-release_latest.generic_all.debdpkg -i percona-release_latest.generic_all.debapt-get updateapt-get install pmm-clientRPM Package:sudo yum install https://repo.percona.com/yum/percona-release-latest.noarch.rpmyum install pmm-clientAdd Cassandra Node to PMM Server:pmm-admin config --server PMM-Server-IPEnable Linux Metrics:It contains common linux monitoring metrics. So 42000 port should be opened to the PMM server.pmm-admin add linux:metricsStage 3: Add Cassandra to PMMPMM is having a feature called External services, So the PMM will capture the metrics from your own external services. Enable the Cassandra metrics as an external service.In config.yml file, we have added the listen port as 8080, so our external service will use this port to get the metrics. And this 8080 port should be opened to the PMM server.pmm-admin add external:service cassandra --service-port=8080Now, metrics are collecting by PMM, but we can’t visualize this without the proper Dashboard. So the critro team has already build a dashboard and published it in Grafana repo. So we can import it from there.  Go to Grafana -&gt; Click on Plus(+) button -&gt; Import.  Paste this dashboard id here: 6400  Click Prometheus as datasource.Wait to 5 to 10mins. Then you’ll see the data.Author’s Comment:Generally its not a good practice to scan the metrics very frequently, For my workload 10sec is fine, But do a complete test in your infra before going to prod setup. Also learn more about this custom exporter from here: https://github.com/criteo/cassandra_exporter/",
            "content_html": "<p>While reading this title you may think about what this guy is going to do? Its all about <a href=\"https://github.com/prometheus/jmx_exporter\">JMX exporter</a> from prometheus and Grafana. Yes, its already implemented by many companies and Grafana has some cool dashboards. But as a DBA, in <a href=\"https://medium.com/Searce\">Searce</a> we are managing many customers and all of them are using many types of databases. So from a DBA’s perspective to monitor  all databases in one place is always a great thing right. If you are a MySQL DBA, then you must have heard about <a href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\">PMM</a>. Its an awesome monitoring tool and its open source. Also it has Dashboards for Linux metrics, MongoDB and PostgreSQL.</p><p>I remember, 2 years back(2017) I was trying to setup monitoring for a huge Cassandra cluster with MX4J. It was very hard that time to understand the metrics. But now, when I stated using PMM, I became a big fan of it. So I want to integrate the Cassandra monitoring dashboard with PMM.</p><p>Before starting this, I was searching about how JMX exporter is behaving and will make any trouble for my cluster and etc. Unfortunately it does in few cases. Then I found this amazing custom JMX exporter which is developed by <a href=\"https://github.com/criteo/cassandra_exporter/\">Criteo</a>.  Its has better control over the metrics. So I decided to use this.</p><h2 id=\"stage-1-install-cassandra_exporter\">Stage 1: Install cassandra_exporter</h2><p>You don’t need to change anything from your Cassandra’s setting or cassandra-env.sh</p><h3 id=\"download-the-exporter\">Download the exporter:</h3><figure class=\"highlight\"><pre><code class=\"language-html\" data-lang=\"html\"><table class=\"rouge-table\"><tbody><tr><td class=\"gutter gl\"><pre class=\"lineno\">1234</pre></td><td class=\"code\"><pre> mkdir /opt/cassandra_exportercd  /opt/cassandra_exporterwget https://github.com/criteo/cassandra_exporter/releases/download/2.2.1/cassandra_exporter-2.2.1-all.jarmv cassandra_exporter-2.2.1-all.jar  cassandra_exporter.jar </pre></td></tr></tbody></table></code></pre></figure><h3 id=\"create-config-file\">Create config file</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">vi config.ymlhost: localhost:7199ssl: Falseuser:password:listenAddress: 0.0.0.0listenPort: 8080blacklist:   - java:lang:memorypool:.<span class=\"k\">*</span>usagethreshold.<span class=\"k\">*</span>   - .<span class=\"k\">*</span>:999thpercentile   - .<span class=\"k\">*</span>:95thpercentile   - .<span class=\"k\">*</span>:fifteenminuterate   - .<span class=\"k\">*</span>:fiveminuterate   - .<span class=\"k\">*</span>:durationunit   - .<span class=\"k\">*</span>:rateunit   - .<span class=\"k\">*</span>:stddev   - .<span class=\"k\">*</span>:meanrate   - .<span class=\"k\">*</span>:mean   - .<span class=\"k\">*</span>:min   - com:criteo:nosql:cassandra:exporter:.<span class=\"k\">*</span>maxScrapFrequencyInSec:  10:    - .<span class=\"k\">*</span>  300:    - .<span class=\"k\">*</span>:snapshotssize:.<span class=\"k\">*</span>    - .<span class=\"k\">*</span>:estimated.<span class=\"k\">*</span>    - .<span class=\"k\">*</span>:totaldiskspaceused:.<span class=\"k\">*</span></code></pre></figure><ul>  <li>blacklist - These metrics are never been collected.</li>  <li>maxScrapFrequencyInSec - Metrics collection frequency.    <ul>      <li>Here all the metrics are collected every 10sec and metrics are under 300 are collected every 300 sec.</li>    </ul>  </li></ul><h3 id=\"start-the-exporter\">Start the Exporter:</h3><p>I have executed this using nohup but you can create a service for this.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nb\">nohup </span>java <span class=\"nt\">-jar</span> /opt/cassandra_exporter/cassandra_exporter.jar config.yml &amp;</code></pre></figure><h2 id=\"stage-2\">Stage 2:</h2><h3 id=\"install-and-configure-pmm-server\">Install and Configure PMM Server:</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">docker pull percona/pmm-server:1docker create <span class=\"se\">\\</span>   <span class=\"nt\">-v</span> /opt/prometheus/data <span class=\"se\">\\</span>   <span class=\"nt\">-v</span> /opt/consul-data <span class=\"se\">\\</span>   <span class=\"nt\">-v</span> /var/lib/mysql <span class=\"se\">\\</span>   <span class=\"nt\">-v</span> /var/lib/grafana <span class=\"se\">\\</span>   <span class=\"nt\">--name</span> pmm-data <span class=\"se\">\\</span>   percona/pmm-server:1 /bin/truedocker run <span class=\"nt\">-d</span> <span class=\"se\">\\</span>   <span class=\"nt\">-p</span> 80:80 <span class=\"se\">\\</span>   <span class=\"nt\">--volumes-from</span> pmm-data <span class=\"se\">\\</span>   <span class=\"nt\">--name</span> pmm-server <span class=\"se\">\\</span>   <span class=\"nt\">--restart</span> always <span class=\"se\">\\</span>   percona/pmm-server:1</code></pre></figure><blockquote>  <p>More customized installation: <a href=\"https://github.com/criteo/cassandra_exporter/releases/download/2.2.1/cassandra_exporter-2.2.1-all.jar\" title=\"https://github.com/criteo/cassandra_exporter/releases/download/2.2.1/cassandra_exporter-2.2.1-all.jar\">https://www.percona.com/doc/percona-monitoring-and-management/deploy/server/docker.setting-up.html</a></p></blockquote><h3 id=\"install-pmm-client-on-all-cassandra-nodes\">Install PMM Client on all Cassandra Nodes:</h3><p><strong>DEB Package:</strong></p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">wget https://repo.percona.com/apt/percona-release_latest.generic_all.debdpkg <span class=\"nt\">-i</span> percona-release_latest.generic_all.debapt-get updateapt-get <span class=\"nb\">install </span>pmm-client</code></pre></figure><p><strong>RPM Package:</strong></p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nb\">sudo </span>yum <span class=\"nb\">install </span>https://repo.percona.com/yum/percona-release-latest.noarch.rpmyum <span class=\"nb\">install </span>pmm-client</code></pre></figure><h3 id=\"add-cassandra-node-to-pmm-server\"><strong>Add Cassandra Node to PMM Server:</strong></h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">pmm-admin config <span class=\"nt\">--server</span> PMM-Server-IP</code></pre></figure><p><strong>Enable Linux Metrics:</strong></p><p>It contains common linux monitoring metrics. So 42000 port should be opened to the PMM server.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">pmm-admin add linux:metrics</code></pre></figure><h2 id=\"stage-3-add-cassandra-to-pmm\">Stage 3: Add Cassandra to PMM</h2><p>PMM is having a feature called External services, So the PMM will capture the metrics from your own external services. Enable the Cassandra metrics as an external service.</p><p>In <code class=\"language-html highlighter-rouge\">config.yml</code> file, we have added the listen port as 8080, so our external service will use this port to get the metrics. And this 8080 port should be opened to the PMM server.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">pmm-admin add external:service cassandra <span class=\"nt\">--service-port</span><span class=\"o\">=</span>8080</code></pre></figure><p>Now, metrics are collecting by PMM, but we can’t visualize this without the proper Dashboard. So the critro team has already build a dashboard and published it in Grafana repo. So we can import it from there.</p><ul>  <li>Go to Grafana -&gt; Click on Plus(+) button -&gt; Import.</li>  <li><a href=\"https://grafana.com/dashboards/6400\">Paste this dashboard id </a>here: <code class=\"language-html highlighter-rouge\">6400</code></li>  <li>Click Prometheus as datasource.</li></ul><p><img src=\"/assets/Monitor Cassandra Cluster with Grafana Prometheus Using Percona PMM-1.png\" alt=\"\" /></p><p><img src=\"/assets/Monitor Cassandra Cluster with Grafana Prometheus Using Percona PMM-2.png\" alt=\"\" /></p><p><strong>Wait to 5 to 10mins. Then you’ll see the data.</strong></p><p><img src=\"/assets/Monitor Cassandra Cluster with Grafana Prometheus Using Percona PMM-3.png\" alt=\"\" /></p><p><img src=\"/assets/Monitor Cassandra Cluster with Grafana Prometheus Using Percona PMM-4.png\" alt=\"\" /></p><p><img src=\"/assets/Monitor Cassandra Cluster with Grafana Prometheus Using Percona PMM-5.png\" alt=\"\" /></p><h2 id=\"authors-comment\">Author’s Comment:</h2><p>Generally its not a good practice to scan the metrics very frequently, For my workload 10sec is fine, But do a complete test in your infra before going to prod setup. Also learn more about this custom exporter from here: <a href=\"https://github.com/criteo/cassandra_exporter/\" title=\"https://github.com/criteo/cassandra_exporter/\">https://github.com/criteo/cassandra_exporter/</a></p>",
            "url": "/2019/05/16/monitor-cassandra-clusters-with-percona-pmm-jmx-grafana-and-prometheus",
            "image": "/assets/Monitor Cassandra Cluster with Grafana Prometheus Using Percona PMM.png",
            
            
            "tags": ["cassandra","monitoring","grafana","prometheus"," percona"],
            
            "date_published": "2019-05-16T18:30:00+00:00",
            "date_modified": "2019-05-16T18:30:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2019/03/03/mysql-pitr-the-fastest-way-with-devops",
            "title": "MySQL PITR The Fastest Way With DevOps",
            "summary": "Find out the Binlog file name, a position from backup and perform PITR will take more time. Now its simplified PITR in one click.",
            "content_text": "Point In Time Recovery - is a nightmare for DBAs if the MySQL clusters are self managed. It was 10PM, after had my dinner I was simply watching some shows in YouTube. And my phone was ringing, the customer on other side. Due to some bad queries, one of the main table get updated without where clause. Then suddenly everyone joined the call and asking me to bring the data back. That day it took 6 to 8 Hours to bring the data. Yes, every DBAs will do one or two biggest mistakes. In my carrier I would say this was that day. So here is my MySQL PITR the fastest way with DevOps.Where I failed in this DR setup?  PITR starts with last full backup + binlogs  I missed in my backup script to add --master-data, So I don’t know how to start applying binlogs.  No Delay replica. I got the call within 10mins when the data has been messed up. But all of my replicas are real time sync. Its affected all of them.  And the effort, launching new server, Install MySQL and configure MySQL  parameters. It took 15-20mins.  Restoring the logical backup of a pretty huge database. Its few hours.  Then copy binlogs(or read binlog from remote server), get the start position and find the stop position and execute them on DR server. Again the effort matters here.After that incident, I compared my scenario with AWS RDS. How simple is that? Just few clicks it’ll provision an instance and apply the binlogs. But this is also fail in one case. What happen if need to restore till a position. Its not possible. You have option for select the time not binlog position. But anyhow I like this option RDS(in CloudSQL it sucks). Then I build this Simplified PITR in one click with the help of RunDeck.DR setup:I done modification in my DR site. My entire Infra in GCP and backup files are sync with GCS bucket.  Setup a delay replica for 15mins.  Replaced logical backup with Percona XtraBackup.  Image(in AWS terms its AMI) is ready with MySQL and Optimized MySQL parameters.  Finally a RunDeck Job which will do this magic.Before you start implementing this solution, you need to setup the below things.  Setup a backup job with Percona Xtrabackup along with incremental backups. (Naming conversion must be same as I mentioned in that blog).  Setup Rundeck and add your DR server in RunDeck.(Read my rundeck kickstart series)  Make your DR server to pull the binlog files from Master. (You can use read binlog from remote server, SCP or something like that).Percona XtraBackup:My PITR depends on my xtrabackup. I have configured my backup job with  12.00AM full backup (runtime 15mins)  Every 1Hr incremental Backup(run time 10mins)The backup process will take 10-15mins to complete. If I want to restore till 1.04AM, then I should restore the FULL Backup(12AM), but this 1AM incremental backup is still going on. I should not use this backup to perform PITR. Then what should we do?  Restore 12AM full backup  Apply binlogs after 12AM to 1.04AMThen we don’t need to bother about the on going incremental backup.Read Here :Automation Script For Percona Xtrabackup FULL/IncrementalWhat Backup files needs to Restore?Now in my scripting, based on the above scenario, I have added a condition for which backup files are needs to be restored. For safer side I considered 15mins as my backup complete time.1. If hour !=00(not equal to 12AM),  then check if minutes &gt;15. Now I can use FULL backup + last Incremental backups.1234   ex: 2019-03-04 02:20:30  hour =02 (!=00)  minutes =20 (&gt;15)  Restore: FULL Backup + 01 Inc + 02 Inc   2. If hour !=00(not equal to 12AM),  then check if minutes &lt;15. Then the incremental backup is going on this time. So we should avoid this current Inc backup and use FULL Backup alone + Current Hour -1 Inc backup1234   ex: 2019-03-04 05:10:30  hour=01 (!=0)  minutes=10 (&lt;15)  Restore: FULL backup + Inc1 to Inc4   3. If hour=00 and minute&lt;15, then this time FULL Backup process is going on, so we should not use this backup. In this case we should sync yesterday’s FULL backup + Yesterday’s Inc 1 to Inc 23.So this is my IF condition to select which file needs to sync.1234567891011121314151617181920212223 d='2019-03-04 20:42:53\"s_d=`echo $d | awk -F ' ' '{print $1}'`s_h=`echo $d | awk -F ' ' '{print $2}'| awk -F ':' '{print $1}'`s_m=`echo $d | awk -F ' ' '{print $2}' | awk -F ':' '{print $2}'`if [ $s_h -ne 00 ]then if [ $s_m -gt 15 ]then echo \"sync FULL + actual hour\"else if [ $s_h -eq 01 ]then echo \"sync actual full only\"elseecho \"sync FULL + hour -1\"fifielse if [ $s_m -gt 15 ]then echo \"sync actual full only\"else echo \"last day full + 23inc\" fifi Transfer.sh:For my database infra, I have configured transfer.sh file sharing. because if your MySQL is down, then we should not use read-from-remote-server. SCP also needs shell based access and we configured SSH session recording with 2FA, so SCP will not work. Its not a big deal you can read how I configured Transfer.sh for my internal network from the below link.CloudOps At Scale: Secure access patterns using Bastion Host and Transfer.ShAgain its your choice that how to bring Binlog files to DR Server.Get the Binlogs:Now its another question, What/How many binlog files do I need to copy from Master?Its actually from your last backup type(Full or Inc) to next backup hour.XtraBackup will have the info about the binlog file name and position. For this PITR, we should restore FULL Backup + Inc1 to Inc5. Once we executed the xtrabackup prepare command(restore), the xtrabackup-binlog_info will contains the exact binlog file and position during that backup. So we need to copy from that backup to what are all other files created within our PITR time.Step Involved:  Once we decided the time, Download the backup files from GCS bucket.  Restore the XtraBackup.  Point MySQL DataDir to Restored Xtrabackup directory.  Get the necessary binlog files from Master Server.  Decode the Binlog files using mysqlbinlog utility.  Restore the decoded binlog file.On DR Server:Before run this job, please make sure the below things:  Master/DR server are added to RunDeck Server.  RunDeck user on DR and Master server should have root access  Install wget on DR server.  RunDeck user’s Private Key must be located on DR Servers RunDeck users home directory (/home/rundeck/.ssh/id_rsa) This is for login to Master sever without password and export the Binlog files.  I used Transfer.sh, so I have created an alias to run curl upload command  by calling transfer filename. So you can use transfer.sh or bring your own copy mechanism.If you are not using transfer.sh then ignore this step.On Master:Replace 10.10.10.10 with your transfer.sh server IP.12345678   vi /home/rundeck/.bashrc  transfer() {   curl --progress-bar --upload-file \"$1\" http://10.10.10.10/$(basename $1) |  tee /dev/null;  }  alias transfer=transfer   Save and close.Add rundeck user to mysql group.  usermod -aG mysql rundeck  Lets create the RunDeck Job:  Go to RunDeck –&gt; Jobs –&gt; New Job  JobName –&gt; Point In Time Recovery  In the Options section add as Option.  Option Name/Option Label –&gt; datetime  Under the input type, select DATE  Date Format: YYYY-MM-DD HH:mm:ss  Required: YesUnder the Workflow –&gt; Node steps, click on script. Copy and Paste the below shell script.Step 1: Download Backup files from GCSd='@option.datetime@'echo $ds_d=`echo $d | awk -F ' ' '{print $1}'`s_h=`echo $d | awk -F ' ' '{print $2}'| awk -F ':' '{print $1}'`s_m=`echo $d | awk -F ' ' '{print $2}' | awk -F ':' '{print $2}'`if [ $s_h -ne 00 ]then if [ $s_m -gt 15 ]then echo \"sync FULL + actual hour\"mkdir -p /mysqldata/FULLgsutil -m rsync -r gs://xtrabackup/$s_d/FULL/ /mysqldata/FULL/for i in $(seq  1 $s_h); do   echo \"inc\"$imkdir -p /mysqldata/inc$igsutil -m rsync -r gs://xtrabackup/$s_d/inc$i/ /mysqldata/inc$i/doneelse if [ $s_h -eq 01 ]then echo \"sync actual full only\"mkdir -p /mysqldata/FULLgsutil -m rsync -r gs://xtrabackup/$s_d/FULL/ /mysqldata/FULL/else echo \"sunc FULL + hour -1\"inc=$(expr $s_h - 1)mkdir -p /mysqldata/FULLgsutil -m rsync -r gs://xtrabackup/$s_d/FULL/ /mysqldata/FULL/for i in $(seq  1 $inc); do   mkdir -p /mysqldata/inc$iecho \"inc\"$igsutil -m rsync -r gs://xtrabackup/$s_d/inc$i/ /mysqldata/inc$i/donefifielse if [ $s_m -gt 15 ]then echo \"sync actual full only\"mkdir -p /mysqldata/FULLgsutil -m rsync -r gs://xtrabackup/$s_d/FULL/ /mysqldata/FULL/else echo \"last day full + 23inc\" yesterday=`date -d \"$s_d -1 days\" +%Y-%m-%d`mkdir -p /mysqldata/FULLgsutil -m rsync -r gs://xtrabackup/$yesterday/FULL/ /mysqldata/FULL/for i in $(seq  1 23); do   mkdir -p /mysqldata/inc$iecho \"inc\"$igsutil -m rsync -r gs://xtrabackup/$yesterday/inc$i/ /mysqldata/inc$i/donefifiIf you are using AWS, Azure or FTP, then replace this gsutil -m rsync -r gs://xtrabackup/$s_d/FULL/ /mysqldata/FULL/ line with your commands.Also replace /mysqldata for where you need to download Backup files.Step 2: Restore the XtrabackupBACKUP_DIR='/mysqldata'echo `date '+%Y-%m-%d %H:%M:%S:%s'`\": Decompressing the FULL backup\" &gt;&gt; $BACKUP_DIR/xtrabackup-restore.logxtrabackup --decompress --remove-original --parallel=30 --target-dir=$BACKUP_DIR/FULL echo `date '+%Y-%m-%d %H:%M:%S:%s'`\": Decompressing Done !!!\" &gt;&gt; $BACKUP_DIR/xtrabackup-restore.log echo `date '+%Y-%m-%d %H:%M:%S:%s'`\": Prepareing FULL Backup ...\" &gt;&gt; $BACKUP_DIR/xtrabackup-restore.logxtrabackup --prepare  --apply-log-only --target-dir=$BACKUP_DIR/FULL echo `date '+%Y-%m-%d %H:%M:%S:%s'`\": FULL Backup Preparation Done!!!\" &gt;&gt; $BACKUP_DIR/xtrabackup-restore.log          P=1 while [ -d $BACKUP_DIR/inc$P ] &amp;&amp; [ -d $BACKUP_DIR/inc$(($P+1)) ] do       echo `date '+%Y-%m-%d %H:%M:%S:%s'`\": Decompressing incremental:$P\" &gt;&gt; $BACKUP_DIR/xtrabackup-restore.log       xtrabackup --decompress --remove-original --parallel=30 --target-dir=$BACKUP_DIR/inc$P        echo `date '+%Y-%m-%d %H:%M:%S:%s'`\": Decompressing incremental:$P Done !!!\" &gt;&gt; $BACKUP_DIR/xtrabackup-restore.log              echo `date '+%Y-%m-%d %H:%M:%S:%s'`\": Prepareing incremental:$P\"  &gt;&gt; $BACKUP_DIR/xtrabackup-restore.log       xtrabackup --prepare --apply-log-only --target-dir=$BACKUP_DIR/FULL --incremental-dir=$BACKUP_DIR/inc$P        echo `date '+%Y-%m-%d %H:%M:%S:%s'`\": incremental:$P Preparation Done!!!\" &gt;&gt; $BACKUP_DIR/xtrabackup-restore.log       P=$(($P+1)) done if [ -d $BACKUP_DIR/inc$P ] then     echo `date '+%Y-%m-%d %H:%M:%S:%s'`\": Decompressing the last incremental:$P\" &gt;&gt; $BACKUP_DIR/xtrabackup-restore.log     xtrabackup --decompress --remove-original --parallel=30 --target-dir=$BACKUP_DIR/inc$P      echo `date '+%Y-%m-%d %H:%M:%S:%s'`\": Decompressing the last incremental:$P Done !!!\" &gt;&gt; $BACKUP_DIR/xtrabackup-restore.log          echo `date '+%Y-%m-%d %H:%M:%S:%s'`\": Prepareing the last incremental:$P\"  &gt;&gt; $BACKUP_DIR/xtrabackup-restore.log     xtrabackup --prepare --target-dir=$BACKUP_DIR/FULL --incremental-dir=$BACKUP_DIR/inc$P      echo `date '+%Y-%m-%d %H:%M:%S:%s'`\": Last incremental:$P Preparation Done!!!\" &gt;&gt; $BACKUP_DIR/xtrabackup-restore.log fiBACKUP_DIR='/mysqldata' - Replace /mysqldata with your location on DR server where is you backups are downloaded from the step 1.I have 40 core CPU, so I used 30 parallel threads. You can modify based on you DR server’s CPU in --parallel=30Step 3: Start MySQL with Restored DataBACKUP_DIR='/mysqldata'  DATADIR='/mysqldata/data'  sudo service mysql stop  sudo mv $BACKUP_DIR/FULL $BACKUP_DIR/data  sudo sed -i.bak \"s|.*datadir.*|datadir=$DATADIR|\" /etc/my.cnf  sudo semanage fcontext -a -t mysqld_db_t \"/mysqldata/data(/.*)?\"  sudo restorecon -R -v /mysqldata/data  sudo chown -R mysql:mysql /mysqldata/data  sudo chmod 750 /mysqldata/data  sudo service mysql startAgain /mysqldata replace this with your backup file location.  And the Restored backup will be in the directory called FULL. Im renaming it to data.Step 4: Download binlog files from Master:  d='@option.datetime@'DATADIR='/mysqldata/data'[ -e /tmp/out  ] &amp;&amp; rm /tmp/out start_binlog=`head -n 1 $DATADIR/xtrabackup_binlog_info  | awk -F ' ' '{print $1}'`stop_binlog=`ssh 10.10.10.40 \"find /mysql-binlog/ -type f  -newermt \\\"$d\\\"  -exec basename {} \\;  | sort | head -1\"`files=$(ssh 10.10.10.40 \"seq -w `echo $start_binlog | awk -F'.' '{print $2}'` `echo $stop_binlog | awk -F'.' '{print $2}'`\")for x in `echo $files`; do  ssh 10.10.10.40 \"transfer /mysql-binlog/mysql-bin.$x\" ; done &gt;&gt; /tmp/outbinlogfiles=`perl -pe 's#(?&lt;=.)(?=http://)#\\n#g' /tmp/out`for x in `echo $binlogfiles`; do wget -P /mysqldata/binlogdump $x ; done  Replace /mysqldata/data with your data directory of mysql.  Replace 10.10.10.40 with your master server IP.  Replace /mysql-binlog/ your master server’s binlog location.  transfer /mysql-binlog/mysql-bin.$x this command will run the transfer.sh alias and upload the binlog file to transfer.sh serve. If you want to use your own copy process then replace transfer and /mysql-binlog/ location of binlog location. And $x is the files. So don’t replace that.  /mysqldata/binlogdump location on the DR server to download binlog filesStep 5: Decode the Binlog files:d='@option.datetime@'DATADIR='/mysqldata/data'mkdir -p /mysqldata/binlogdumpbinlogfilename=`head -n 1 $DATADIR/xtrabackup_binlog_info  | awk -F ' ' '{print $1}'`binlogposition=`head -n 1 $DATADIR/xtrabackup_binlog_info  | awk -F ' ' '{print $2}'`files=`ls /mysqldata/binlogdump/`cd /mysqldata/binlogdumpmysqlbinlog -d Eztaxi --start-position=\"${binlogposition}\" --stop-datetime=\"${d}\" ${files} --disable-log-bin &gt; mysqldata-delta.sql  /mysqldata/data replace with your data directory of MySQL.  /mysqldata/binlogdump Downloaded binlog file location.Step 6: Restore the Binlog file to MySQL:  mysql -u root -p'password' &lt; /mysqldata/binlogdump/mysqldata-delta.sql    p'password' Replace with your MySQL root password. You can use Parameterized password in Rundeck. See here.  '/mysqldata/binlogdump/ location of the decoded binlog file.Steps are done.Now on Matched Nodes select the DR server and click on Create button.Trigger the PITR:Click on the Date Picker Icon and select the date and time for your PITR.Now click on Run Job Now button and go for a Cup of Coffee.Here is my job execution has been done in 18mins.Further improvements and development:  In my case, I have my DR server ready with MySQL installed. (as an Image). Before trigger this job, I’ll launch a VM with this image and validate the connectivity between Rundeck and GCS bucket. Im planning to use Terraform template which is also a part of this RunDeck job.  My complete setup is in GCP, you can perform the same on AWS, Azure or even On-Prem and comment below how it goes.  I have hardcoded all of mysql data directory, binlog location and everything. If you have enough time, use Options in Rundeck to get these things from an Input during the job execution.  finally, this is also same as RDS PITR, I never gave option for restore binlog till this position. But we can achieve this on Step 5, just add a variable called position and give your position number. You can use Options to get this value as an Input. and replace --stop-datetime=\"${d}\" with --stop-position=\"${pos}\".If you have any difficulties in understanding the steps and scripts, please comment below.Want to learn more basics of RunDeck Setup? here you go.Happy Disaster Recovery and PITR :)",
            "content_html": "<p>Point In Time Recovery - is a nightmare for DBAs if the MySQL clusters are self managed. It was 10PM, after had my dinner I was simply watching some shows in YouTube. And my phone was ringing, the customer on other side. Due to some bad queries, one of the main table get updated without where clause. Then suddenly everyone joined the call and asking me to bring the data back. That day it took 6 to 8 Hours to bring the data. Yes, every DBAs will do one or two biggest mistakes. In my carrier I would say this was that day. So here is my MySQL PITR the fastest way with DevOps.</p><h2 id=\"where-i-failed-in-this-dr-setup\">Where I failed in this DR setup?</h2><ul>  <li>PITR starts with last full backup + binlogs</li>  <li>I missed in my backup script to add <code class=\"language-html highlighter-rouge\">--master-data</code>, So I don’t know how to start applying binlogs.</li>  <li>No Delay replica. I got the call within 10mins when the data has been messed up. But all of my replicas are real time sync. Its affected all of them.</li>  <li>And the effort, launching new server, Install MySQL and configure MySQL  parameters. It took 15-20mins.</li>  <li>Restoring the logical backup of a pretty huge database. Its few hours.</li>  <li>Then copy binlogs(or read binlog from remote server), get the start position and find the stop position and execute them on DR server. Again the effort matters here.</li></ul><p>After that incident, I compared my scenario with AWS RDS. How simple is that? Just few clicks it’ll provision an instance and apply the binlogs. But this is also fail in one case. What happen if need to restore till a position. Its not possible. You have option for select the time not binlog position. But anyhow I like this option RDS(in CloudSQL it sucks). Then I build this Simplified PITR in one click with the help of RunDeck.</p><h2 id=\"dr-setup\">DR setup:</h2><p>I done modification in my DR site. My entire Infra in GCP and backup files are sync with GCS bucket.</p><ul>  <li>Setup a delay replica for 15mins.</li>  <li>Replaced logical backup with Percona XtraBackup.</li>  <li>Image(in AWS terms its AMI) is ready with MySQL and Optimized MySQL parameters.</li>  <li>Finally a RunDeck Job which will do this magic.</li></ul><p>Before you start implementing this solution, you need to setup the below things.</p><ol>  <li>Setup a backup job with <a href=\"https://thedataguy.in/automation-script-for-percona-xtrabackup-full-incremental/\">Percona Xtrabackup along with incremental backups</a>. (Naming conversion must be same as I mentioned in that blog).</li>  <li>Setup Rundeck and add your DR server in RunDeck.(Read my rundeck kickstart series)</li>  <li>Make your DR server to pull the binlog files from Master. (You can use read binlog from remote server, SCP or something like that).</li></ol><h2 id=\"percona-xtrabackup\">Percona XtraBackup:</h2><p>My PITR depends on my xtrabackup. I have configured my backup job with</p><ul>  <li>12.00AM full backup (runtime 15mins)</li>  <li>Every 1Hr incremental Backup(run time 10mins)</li></ul><p>The backup process will take 10-15mins to complete. If I want to restore till 1.04AM, then I should restore the FULL Backup(12AM), but this 1AM incremental backup is still going on. I should not use this backup to perform PITR. Then what should we do?</p><ol>  <li>Restore 12AM full backup</li>  <li>Apply binlogs after 12AM to 1.04AM</li></ol><p>Then we don’t need to bother about the on going incremental backup.</p><p><strong>Read Here</strong> :<a href=\"https://thedataguy.in/automation-script-for-percona-xtrabackup-full-incremental/\">Automation Script For Percona Xtrabackup FULL/Incremental</a></p><h2 id=\"what-backup-files-needs-to-restore\">What Backup files needs to Restore?</h2><p>Now in my scripting, based on the above scenario, I have added a condition for which backup files are needs to be restored. For safer side I considered 15mins as my backup complete time.</p><p>1. If hour !=00(not equal to 12AM),  then check if minutes &gt;15. Now I can use FULL backup + last Incremental backups.</p><figure class=\"highlight\"><pre><code class=\"language-html\" data-lang=\"html\"><table class=\"rouge-table\"><tbody><tr><td class=\"gutter gl\"><pre class=\"lineno\">1234</pre></td><td class=\"code\"><pre>   ex: 2019-03-04 02:20:30  hour =02 (!=00)  minutes =20 (&gt;15)  Restore: FULL Backup + 01 Inc + 02 Inc   </pre></td></tr></tbody></table></code></pre></figure><p>2. If hour !=00(not equal to 12AM),  then check if minutes &lt;15. Then the incremental backup is going on this time. So we should avoid this current Inc backup and use FULL Backup alone + Current Hour -1 Inc backup</p><figure class=\"highlight\"><pre><code class=\"language-html\" data-lang=\"html\"><table class=\"rouge-table\"><tbody><tr><td class=\"gutter gl\"><pre class=\"lineno\">1234</pre></td><td class=\"code\"><pre>   ex: 2019-03-04 05:10:30  hour=01 (!=0)  minutes=10 (<span class=\"nt\">&lt;15</span><span class=\"err\">)</span>  <span class=\"na\">Restore:</span> <span class=\"na\">FULL</span> <span class=\"na\">backup</span> <span class=\"err\">+</span> <span class=\"na\">Inc1</span> <span class=\"na\">to</span> <span class=\"na\">Inc4</span>   </pre></td></tr></tbody></table></code></pre></figure><p>3. If hour=00 and minute&lt;15, then this time FULL Backup process is going on, so we should not use this backup. In this case we should sync yesterday’s FULL backup + Yesterday’s Inc 1 to Inc 23.</p><p>So this is my IF condition to select which file needs to sync.</p><figure class=\"highlight\"><pre><code class=\"language-html\" data-lang=\"html\"><table class=\"rouge-table\"><tbody><tr><td class=\"gutter gl\"><pre class=\"lineno\">1234567891011121314151617181920212223</pre></td><td class=\"code\"><pre> d='2019-03-04 20:42:53\"s_d=`echo $d | awk -F ' ' '{print $1}'`s_h=`echo $d | awk -F ' ' '{print $2}'| awk -F ':' '{print $1}'`s_m=`echo $d | awk -F ' ' '{print $2}' | awk -F ':' '{print $2}'`if [ $s_h -ne 00 ]then if [ $s_m -gt 15 ]then echo \"sync FULL + actual hour\"else if [ $s_h -eq 01 ]then echo \"sync actual full only\"elseecho \"sync FULL + hour -1\"fifielse if [ $s_m -gt 15 ]then echo \"sync actual full only\"else echo \"last day full + 23inc\" fifi </pre></td></tr></tbody></table></code></pre></figure><h2 id=\"transfersh\">Transfer.sh:</h2><p>For my database infra, I have configured transfer.sh file sharing. because if your MySQL is down, then we should not use <code class=\"language-html highlighter-rouge\">read-from-remote-server</code>. SCP also needs shell based access and we configured SSH session recording with 2FA, so SCP will not work. Its not a big deal you can read how I configured Transfer.sh for my internal network from the below link.</p><p><strong><a href=\"https://medium.com/searce/how-we-manage-and-automated-cloud-infra-ssh-with-bastion-and-transfer-sh-a80c0dc3a5ef\">CloudOps At Scale: Secure access patterns using Bastion Host and Transfer.Sh</a></strong></p><p>Again its your choice that how to bring Binlog files to DR Server.</p><h2 id=\"get-the-binlogs\">Get the Binlogs:</h2><p>Now its another question, What/How many binlog files do I need to copy from Master?</p><p>Its actually from your last backup type(Full or Inc) to next backup hour.</p><p>XtraBackup will have the info about the binlog file name and position. For this PITR, we should restore FULL Backup + Inc1 to Inc5. Once we executed the xtrabackup prepare command(restore), the xtrabackup-binlog_info will contains the exact binlog file and position during that backup. So we need to copy from that backup to what are all other files created within our PITR time.</p><h3 id=\"step-involved\">Step Involved:</h3><ol>  <li>Once we decided the time, Download the backup files from GCS bucket.</li>  <li>Restore the XtraBackup.</li>  <li>Point MySQL DataDir to Restored Xtrabackup directory.</li>  <li>Get the necessary binlog files from Master Server.</li>  <li>Decode the Binlog files using <code class=\"language-html highlighter-rouge\">mysqlbinlog utility</code>.</li>  <li>Restore the decoded binlog file.</li></ol><h3 id=\"on-dr-server\">On DR Server:</h3><p>Before run this job, please make sure the below things:</p><ol>  <li>Master/DR server are added to RunDeck Server.</li>  <li>RunDeck user on DR and Master server should have root access</li>  <li>Install <code class=\"language-html highlighter-rouge\">wget</code> on DR server.</li>  <li>RunDeck user’s Private Key must be located on DR Servers RunDeck users home directory (<code class=\"language-html highlighter-rouge\">/home/rundeck/.ssh/id_rsa</code>) This is for login to Master sever without password and export the Binlog files.</li>  <li>I used Transfer.sh, so I have created an alias to run curl upload command  by calling <code class=\"language-html highlighter-rouge\">transfer filename</code>. So you can use transfer.sh or bring your own copy mechanism.</li></ol><p>If you are not using transfer.sh then ignore this step.</p><h3 id=\"on-master\">On Master:</h3><p>Replace <code class=\"language-html highlighter-rouge\">10.10.10.10</code> with your transfer.sh server IP.</p><figure class=\"highlight\"><pre><code class=\"language-html\" data-lang=\"html\"><table class=\"rouge-table\"><tbody><tr><td class=\"gutter gl\"><pre class=\"lineno\">12345678</pre></td><td class=\"code\"><pre>   vi /home/rundeck/.bashrc  transfer() {   curl --progress-bar --upload-file \"$1\" http://10.10.10.10/$(basename $1) |  tee /dev/null;  }  alias transfer=transfer   </pre></td></tr></tbody></table></code></pre></figure><p>Save and close.</p><p>Add rundeck user to mysql group.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">  usermod <span class=\"nt\">-aG</span> mysql rundeck  </code></pre></figure><h2 id=\"lets-create-the-rundeck-job\">Lets create the RunDeck Job:</h2><ol>  <li>Go to RunDeck –&gt; Jobs –&gt; New Job</li>  <li>JobName –&gt; Point In Time Recovery</li>  <li>In the Options section add as Option.</li>  <li>Option Name/Option Label –&gt; <code class=\"language-html highlighter-rouge\">datetime</code></li>  <li>Under the input type, select <code class=\"language-html highlighter-rouge\">DATE</code></li>  <li>Date Format: <code class=\"language-html highlighter-rouge\">YYYY-MM-DD HH:mm:ss</code></li>  <li>Required: Yes</li></ol><p>Under the Workflow –&gt; Node steps, click on script. Copy and Paste the below shell script.</p><h3 id=\"step-1-download-backup-files-from-gcs\">Step 1: Download Backup files from GCS</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nv\">d</span><span class=\"o\">=</span><span class=\"s1\">'@option.datetime@'</span><span class=\"nb\">echo</span> <span class=\"nv\">$d</span><span class=\"nv\">s_d</span><span class=\"o\">=</span><span class=\"sb\">`</span><span class=\"nb\">echo</span> <span class=\"nv\">$d</span> | <span class=\"nb\">awk</span> <span class=\"nt\">-F</span> <span class=\"s1\">' '</span> <span class=\"s1\">'{print $1}'</span><span class=\"sb\">`</span><span class=\"nv\">s_h</span><span class=\"o\">=</span><span class=\"sb\">`</span><span class=\"nb\">echo</span> <span class=\"nv\">$d</span> | <span class=\"nb\">awk</span> <span class=\"nt\">-F</span> <span class=\"s1\">' '</span> <span class=\"s1\">'{print $2}'</span>| <span class=\"nb\">awk</span> <span class=\"nt\">-F</span> <span class=\"s1\">':'</span> <span class=\"s1\">'{print $1}'</span><span class=\"sb\">`</span><span class=\"nv\">s_m</span><span class=\"o\">=</span><span class=\"sb\">`</span><span class=\"nb\">echo</span> <span class=\"nv\">$d</span> | <span class=\"nb\">awk</span> <span class=\"nt\">-F</span> <span class=\"s1\">' '</span> <span class=\"s1\">'{print $2}'</span> | <span class=\"nb\">awk</span> <span class=\"nt\">-F</span> <span class=\"s1\">':'</span> <span class=\"s1\">'{print $2}'</span><span class=\"sb\">`</span><span class=\"k\">if</span> <span class=\"o\">[</span> <span class=\"nv\">$s_h</span> <span class=\"nt\">-ne</span> 00 <span class=\"o\">]</span><span class=\"k\">then if</span> <span class=\"o\">[</span> <span class=\"nv\">$s_m</span> <span class=\"nt\">-gt</span> 15 <span class=\"o\">]</span><span class=\"k\">then </span><span class=\"nb\">echo</span> <span class=\"s2\">\"sync FULL + actual hour\"</span><span class=\"nb\">mkdir</span> <span class=\"nt\">-p</span> /mysqldata/FULLgsutil <span class=\"nt\">-m</span> rsync <span class=\"nt\">-r</span> gs://xtrabackup/<span class=\"nv\">$s_d</span>/FULL/ /mysqldata/FULL/<span class=\"k\">for </span>i <span class=\"k\">in</span> <span class=\"si\">$(</span><span class=\"nb\">seq  </span>1 <span class=\"nv\">$s_h</span><span class=\"si\">)</span><span class=\"p\">;</span> <span class=\"k\">do   </span><span class=\"nb\">echo</span> <span class=\"s2\">\"inc\"</span><span class=\"nv\">$i</span><span class=\"nb\">mkdir</span> <span class=\"nt\">-p</span> /mysqldata/inc<span class=\"nv\">$i</span>gsutil <span class=\"nt\">-m</span> rsync <span class=\"nt\">-r</span> gs://xtrabackup/<span class=\"nv\">$s_d</span>/inc<span class=\"nv\">$i</span>/ /mysqldata/inc<span class=\"nv\">$i</span>/<span class=\"k\">doneelse if</span> <span class=\"o\">[</span> <span class=\"nv\">$s_h</span> <span class=\"nt\">-eq</span> 01 <span class=\"o\">]</span><span class=\"k\">then </span><span class=\"nb\">echo</span> <span class=\"s2\">\"sync actual full only\"</span><span class=\"nb\">mkdir</span> <span class=\"nt\">-p</span> /mysqldata/FULLgsutil <span class=\"nt\">-m</span> rsync <span class=\"nt\">-r</span> gs://xtrabackup/<span class=\"nv\">$s_d</span>/FULL/ /mysqldata/FULL/<span class=\"k\">else </span><span class=\"nb\">echo</span> <span class=\"s2\">\"sunc FULL + hour -1\"</span><span class=\"nv\">inc</span><span class=\"o\">=</span><span class=\"si\">$(</span><span class=\"nb\">expr</span> <span class=\"nv\">$s_h</span> - 1<span class=\"si\">)</span><span class=\"nb\">mkdir</span> <span class=\"nt\">-p</span> /mysqldata/FULLgsutil <span class=\"nt\">-m</span> rsync <span class=\"nt\">-r</span> gs://xtrabackup/<span class=\"nv\">$s_d</span>/FULL/ /mysqldata/FULL/<span class=\"k\">for </span>i <span class=\"k\">in</span> <span class=\"si\">$(</span><span class=\"nb\">seq  </span>1 <span class=\"nv\">$inc</span><span class=\"si\">)</span><span class=\"p\">;</span> <span class=\"k\">do   </span><span class=\"nb\">mkdir</span> <span class=\"nt\">-p</span> /mysqldata/inc<span class=\"nv\">$i</span><span class=\"nb\">echo</span> <span class=\"s2\">\"inc\"</span><span class=\"nv\">$i</span>gsutil <span class=\"nt\">-m</span> rsync <span class=\"nt\">-r</span> gs://xtrabackup/<span class=\"nv\">$s_d</span>/inc<span class=\"nv\">$i</span>/ /mysqldata/inc<span class=\"nv\">$i</span>/<span class=\"k\">donefifielse if</span> <span class=\"o\">[</span> <span class=\"nv\">$s_m</span> <span class=\"nt\">-gt</span> 15 <span class=\"o\">]</span><span class=\"k\">then </span><span class=\"nb\">echo</span> <span class=\"s2\">\"sync actual full only\"</span><span class=\"nb\">mkdir</span> <span class=\"nt\">-p</span> /mysqldata/FULLgsutil <span class=\"nt\">-m</span> rsync <span class=\"nt\">-r</span> gs://xtrabackup/<span class=\"nv\">$s_d</span>/FULL/ /mysqldata/FULL/<span class=\"k\">else </span><span class=\"nb\">echo</span> <span class=\"s2\">\"last day full + 23inc\"</span> <span class=\"nv\">yesterday</span><span class=\"o\">=</span><span class=\"sb\">`</span><span class=\"nb\">date</span> <span class=\"nt\">-d</span> <span class=\"s2\">\"</span><span class=\"nv\">$s_d</span><span class=\"s2\"> -1 days\"</span> +%Y-%m-%d<span class=\"sb\">`</span><span class=\"nb\">mkdir</span> <span class=\"nt\">-p</span> /mysqldata/FULLgsutil <span class=\"nt\">-m</span> rsync <span class=\"nt\">-r</span> gs://xtrabackup/<span class=\"nv\">$yesterday</span>/FULL/ /mysqldata/FULL/<span class=\"k\">for </span>i <span class=\"k\">in</span> <span class=\"si\">$(</span><span class=\"nb\">seq  </span>1 23<span class=\"si\">)</span><span class=\"p\">;</span> <span class=\"k\">do   </span><span class=\"nb\">mkdir</span> <span class=\"nt\">-p</span> /mysqldata/inc<span class=\"nv\">$i</span><span class=\"nb\">echo</span> <span class=\"s2\">\"inc\"</span><span class=\"nv\">$i</span>gsutil <span class=\"nt\">-m</span> rsync <span class=\"nt\">-r</span> gs://xtrabackup/<span class=\"nv\">$yesterday</span>/inc<span class=\"nv\">$i</span>/ /mysqldata/inc<span class=\"nv\">$i</span>/<span class=\"k\">donefifi</span></code></pre></figure><p>If you are using AWS, Azure or FTP, then replace this <code class=\"language-html highlighter-rouge\">gsutil -m rsync -r gs://xtrabackup/$s_d/FULL/ /mysqldata/FULL/</code> line with your commands.</p><p>Also replace <code class=\"language-html highlighter-rouge\">/mysqldata</code> for where you need to download Backup files.</p><h3 id=\"step-2-restore-the-xtrabackup\">Step 2: Restore the Xtrabackup</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nv\">BACKUP_DIR</span><span class=\"o\">=</span><span class=\"s1\">'/mysqldata'</span><span class=\"nb\">echo</span> <span class=\"sb\">`</span><span class=\"nb\">date</span> <span class=\"s1\">'+%Y-%m-%d %H:%M:%S:%s'</span><span class=\"sb\">`</span><span class=\"s2\">\": Decompressing the FULL backup\"</span> <span class=\"o\">&gt;&gt;</span> <span class=\"nv\">$BACKUP_DIR</span>/xtrabackup-restore.logxtrabackup <span class=\"nt\">--decompress</span> <span class=\"nt\">--remove-original</span> <span class=\"nt\">--parallel</span><span class=\"o\">=</span>30 <span class=\"nt\">--target-dir</span><span class=\"o\">=</span><span class=\"nv\">$BACKUP_DIR</span>/FULL <span class=\"nb\">echo</span> <span class=\"sb\">`</span><span class=\"nb\">date</span> <span class=\"s1\">'+%Y-%m-%d %H:%M:%S:%s'</span><span class=\"sb\">`</span><span class=\"s2\">\": Decompressing Done !!!\"</span> <span class=\"o\">&gt;&gt;</span> <span class=\"nv\">$BACKUP_DIR</span>/xtrabackup-restore.log <span class=\"nb\">echo</span> <span class=\"sb\">`</span><span class=\"nb\">date</span> <span class=\"s1\">'+%Y-%m-%d %H:%M:%S:%s'</span><span class=\"sb\">`</span><span class=\"s2\">\": Prepareing FULL Backup ...\"</span> <span class=\"o\">&gt;&gt;</span> <span class=\"nv\">$BACKUP_DIR</span>/xtrabackup-restore.logxtrabackup <span class=\"nt\">--prepare</span>  <span class=\"nt\">--apply-log-only</span> <span class=\"nt\">--target-dir</span><span class=\"o\">=</span><span class=\"nv\">$BACKUP_DIR</span>/FULL <span class=\"nb\">echo</span> <span class=\"sb\">`</span><span class=\"nb\">date</span> <span class=\"s1\">'+%Y-%m-%d %H:%M:%S:%s'</span><span class=\"sb\">`</span><span class=\"s2\">\": FULL Backup Preparation Done!!!\"</span> <span class=\"o\">&gt;&gt;</span> <span class=\"nv\">$BACKUP_DIR</span>/xtrabackup-restore.log          <span class=\"nv\">P</span><span class=\"o\">=</span>1 <span class=\"k\">while</span> <span class=\"o\">[</span> <span class=\"nt\">-d</span> <span class=\"nv\">$BACKUP_DIR</span>/inc<span class=\"nv\">$P</span> <span class=\"o\">]</span> <span class=\"o\">&amp;&amp;</span> <span class=\"o\">[</span> <span class=\"nt\">-d</span> <span class=\"nv\">$BACKUP_DIR</span>/inc<span class=\"k\">$((</span><span class=\"nv\">$P</span><span class=\"o\">+</span><span class=\"m\">1</span><span class=\"k\">))</span> <span class=\"o\">]</span> <span class=\"k\">do       </span><span class=\"nb\">echo</span> <span class=\"sb\">`</span><span class=\"nb\">date</span> <span class=\"s1\">'+%Y-%m-%d %H:%M:%S:%s'</span><span class=\"sb\">`</span><span class=\"s2\">\": Decompressing incremental:</span><span class=\"nv\">$P</span><span class=\"s2\">\"</span> <span class=\"o\">&gt;&gt;</span> <span class=\"nv\">$BACKUP_DIR</span>/xtrabackup-restore.log       xtrabackup <span class=\"nt\">--decompress</span> <span class=\"nt\">--remove-original</span> <span class=\"nt\">--parallel</span><span class=\"o\">=</span>30 <span class=\"nt\">--target-dir</span><span class=\"o\">=</span><span class=\"nv\">$BACKUP_DIR</span>/inc<span class=\"nv\">$P</span>        <span class=\"nb\">echo</span> <span class=\"sb\">`</span><span class=\"nb\">date</span> <span class=\"s1\">'+%Y-%m-%d %H:%M:%S:%s'</span><span class=\"sb\">`</span><span class=\"s2\">\": Decompressing incremental:</span><span class=\"nv\">$P</span><span class=\"s2\"> Done !!!\"</span> <span class=\"o\">&gt;&gt;</span> <span class=\"nv\">$BACKUP_DIR</span>/xtrabackup-restore.log              <span class=\"nb\">echo</span> <span class=\"sb\">`</span><span class=\"nb\">date</span> <span class=\"s1\">'+%Y-%m-%d %H:%M:%S:%s'</span><span class=\"sb\">`</span><span class=\"s2\">\": Prepareing incremental:</span><span class=\"nv\">$P</span><span class=\"s2\">\"</span>  <span class=\"o\">&gt;&gt;</span> <span class=\"nv\">$BACKUP_DIR</span>/xtrabackup-restore.log       xtrabackup <span class=\"nt\">--prepare</span> <span class=\"nt\">--apply-log-only</span> <span class=\"nt\">--target-dir</span><span class=\"o\">=</span><span class=\"nv\">$BACKUP_DIR</span>/FULL <span class=\"nt\">--incremental-dir</span><span class=\"o\">=</span><span class=\"nv\">$BACKUP_DIR</span>/inc<span class=\"nv\">$P</span>        <span class=\"nb\">echo</span> <span class=\"sb\">`</span><span class=\"nb\">date</span> <span class=\"s1\">'+%Y-%m-%d %H:%M:%S:%s'</span><span class=\"sb\">`</span><span class=\"s2\">\": incremental:</span><span class=\"nv\">$P</span><span class=\"s2\"> Preparation Done!!!\"</span> <span class=\"o\">&gt;&gt;</span> <span class=\"nv\">$BACKUP_DIR</span>/xtrabackup-restore.log       <span class=\"nv\">P</span><span class=\"o\">=</span><span class=\"k\">$((</span><span class=\"nv\">$P</span><span class=\"o\">+</span><span class=\"m\">1</span><span class=\"k\">))</span> <span class=\"k\">done if</span> <span class=\"o\">[</span> <span class=\"nt\">-d</span> <span class=\"nv\">$BACKUP_DIR</span>/inc<span class=\"nv\">$P</span> <span class=\"o\">]</span> <span class=\"k\">then     </span><span class=\"nb\">echo</span> <span class=\"sb\">`</span><span class=\"nb\">date</span> <span class=\"s1\">'+%Y-%m-%d %H:%M:%S:%s'</span><span class=\"sb\">`</span><span class=\"s2\">\": Decompressing the last incremental:</span><span class=\"nv\">$P</span><span class=\"s2\">\"</span> <span class=\"o\">&gt;&gt;</span> <span class=\"nv\">$BACKUP_DIR</span>/xtrabackup-restore.log     xtrabackup <span class=\"nt\">--decompress</span> <span class=\"nt\">--remove-original</span> <span class=\"nt\">--parallel</span><span class=\"o\">=</span>30 <span class=\"nt\">--target-dir</span><span class=\"o\">=</span><span class=\"nv\">$BACKUP_DIR</span>/inc<span class=\"nv\">$P</span>      <span class=\"nb\">echo</span> <span class=\"sb\">`</span><span class=\"nb\">date</span> <span class=\"s1\">'+%Y-%m-%d %H:%M:%S:%s'</span><span class=\"sb\">`</span><span class=\"s2\">\": Decompressing the last incremental:</span><span class=\"nv\">$P</span><span class=\"s2\"> Done !!!\"</span> <span class=\"o\">&gt;&gt;</span> <span class=\"nv\">$BACKUP_DIR</span>/xtrabackup-restore.log          <span class=\"nb\">echo</span> <span class=\"sb\">`</span><span class=\"nb\">date</span> <span class=\"s1\">'+%Y-%m-%d %H:%M:%S:%s'</span><span class=\"sb\">`</span><span class=\"s2\">\": Prepareing the last incremental:</span><span class=\"nv\">$P</span><span class=\"s2\">\"</span>  <span class=\"o\">&gt;&gt;</span> <span class=\"nv\">$BACKUP_DIR</span>/xtrabackup-restore.log     xtrabackup <span class=\"nt\">--prepare</span> <span class=\"nt\">--target-dir</span><span class=\"o\">=</span><span class=\"nv\">$BACKUP_DIR</span>/FULL <span class=\"nt\">--incremental-dir</span><span class=\"o\">=</span><span class=\"nv\">$BACKUP_DIR</span>/inc<span class=\"nv\">$P</span>      <span class=\"nb\">echo</span> <span class=\"sb\">`</span><span class=\"nb\">date</span> <span class=\"s1\">'+%Y-%m-%d %H:%M:%S:%s'</span><span class=\"sb\">`</span><span class=\"s2\">\": Last incremental:</span><span class=\"nv\">$P</span><span class=\"s2\"> Preparation Done!!!\"</span> <span class=\"o\">&gt;&gt;</span> <span class=\"nv\">$BACKUP_DIR</span>/xtrabackup-restore.log <span class=\"k\">fi</span></code></pre></figure><p><code class=\"language-html highlighter-rouge\">BACKUP_DIR='/mysqldata'</code> - Replace <code class=\"language-html highlighter-rouge\">/mysqldata</code> with your location on DR server where is you backups are downloaded from the step 1.I have 40 core CPU, so I used 30 parallel threads. You can modify based on you DR server’s CPU in <code class=\"language-html highlighter-rouge\">--parallel=30</code></p><h3 id=\"step-3-start-mysql-with-restored-data\">Step 3: Start MySQL with Restored Data</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nv\">BACKUP_DIR</span><span class=\"o\">=</span><span class=\"s1\">'/mysqldata'</span>  <span class=\"nv\">DATADIR</span><span class=\"o\">=</span><span class=\"s1\">'/mysqldata/data'</span>  <span class=\"nb\">sudo </span>service mysql stop  <span class=\"nb\">sudo mv</span> <span class=\"nv\">$BACKUP_DIR</span>/FULL <span class=\"nv\">$BACKUP_DIR</span>/data  <span class=\"nb\">sudo sed</span> <span class=\"nt\">-i</span>.bak <span class=\"s2\">\"s|.*datadir.*|datadir=</span><span class=\"nv\">$DATADIR</span><span class=\"s2\">|\"</span> /etc/my.cnf  <span class=\"nb\">sudo </span>semanage fcontext <span class=\"nt\">-a</span> <span class=\"nt\">-t</span> mysqld_db_t <span class=\"s2\">\"/mysqldata/data(/.*)?\"</span>  <span class=\"nb\">sudo </span>restorecon <span class=\"nt\">-R</span> <span class=\"nt\">-v</span> /mysqldata/data  <span class=\"nb\">sudo chown</span> <span class=\"nt\">-R</span> mysql:mysql /mysqldata/data  <span class=\"nb\">sudo chmod </span>750 /mysqldata/data  <span class=\"nb\">sudo </span>service mysql start</code></pre></figure><p>Again <code class=\"language-html highlighter-rouge\">/mysqldata</code> replace this with your backup file location.  And the Restored backup will be in the directory called FULL. Im renaming it to <code class=\"language-html highlighter-rouge\">data</code>.</p><h3 id=\"step-4-download-binlog-files-from-master\">Step 4: Download binlog files from Master:</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">  <span class=\"nv\">d</span><span class=\"o\">=</span><span class=\"s1\">'@option.datetime@'</span><span class=\"nv\">DATADIR</span><span class=\"o\">=</span><span class=\"s1\">'/mysqldata/data'</span><span class=\"o\">[</span> <span class=\"nt\">-e</span> /tmp/out  <span class=\"o\">]</span> <span class=\"o\">&amp;&amp;</span> <span class=\"nb\">rm</span> /tmp/out <span class=\"nv\">start_binlog</span><span class=\"o\">=</span><span class=\"sb\">`</span><span class=\"nb\">head</span> <span class=\"nt\">-n</span> 1 <span class=\"nv\">$DATADIR</span>/xtrabackup_binlog_info  | <span class=\"nb\">awk</span> <span class=\"nt\">-F</span> <span class=\"s1\">' '</span> <span class=\"s1\">'{print $1}'</span><span class=\"sb\">`</span><span class=\"nv\">stop_binlog</span><span class=\"o\">=</span><span class=\"sb\">`</span>ssh 10.10.10.40 <span class=\"s2\">\"find /mysql-binlog/ -type f  -newermt </span><span class=\"se\">\\\"</span><span class=\"nv\">$d</span><span class=\"se\">\\\"</span><span class=\"s2\">  -exec basename {} </span><span class=\"se\">\\;</span><span class=\"s2\">  | sort | head -1\"</span><span class=\"sb\">`</span><span class=\"nv\">files</span><span class=\"o\">=</span><span class=\"si\">$(</span>ssh 10.10.10.40 <span class=\"s2\">\"seq -w </span><span class=\"sb\">`</span><span class=\"nb\">echo</span> <span class=\"nv\">$start_binlog</span> | <span class=\"nb\">awk</span> <span class=\"nt\">-F</span><span class=\"s1\">'.'</span> <span class=\"s1\">'{print $2}'</span><span class=\"sb\">`</span><span class=\"s2\"> </span><span class=\"sb\">`</span><span class=\"nb\">echo</span> <span class=\"nv\">$stop_binlog</span> | <span class=\"nb\">awk</span> <span class=\"nt\">-F</span><span class=\"s1\">'.'</span> <span class=\"s1\">'{print $2}'</span><span class=\"sb\">`</span><span class=\"s2\">\"</span><span class=\"si\">)</span><span class=\"k\">for </span>x <span class=\"k\">in</span> <span class=\"sb\">`</span><span class=\"nb\">echo</span> <span class=\"nv\">$files</span><span class=\"sb\">`</span><span class=\"p\">;</span> <span class=\"k\">do  </span>ssh 10.10.10.40 <span class=\"s2\">\"transfer /mysql-binlog/mysql-bin.</span><span class=\"nv\">$x</span><span class=\"s2\">\"</span> <span class=\"p\">;</span> <span class=\"k\">done</span> <span class=\"o\">&gt;&gt;</span> /tmp/out<span class=\"nv\">binlogfiles</span><span class=\"o\">=</span><span class=\"sb\">`</span>perl <span class=\"nt\">-pe</span> <span class=\"s1\">'s#(?&lt;=.)(?=http://)#\\n#g'</span> /tmp/out<span class=\"sb\">`</span><span class=\"k\">for </span>x <span class=\"k\">in</span> <span class=\"sb\">`</span><span class=\"nb\">echo</span> <span class=\"nv\">$binlogfiles</span><span class=\"sb\">`</span><span class=\"p\">;</span> <span class=\"k\">do </span>wget <span class=\"nt\">-P</span> /mysqldata/binlogdump <span class=\"nv\">$x</span> <span class=\"p\">;</span> <span class=\"k\">done</span></code></pre></figure><ul>  <li>Replace <code class=\"language-html highlighter-rouge\">/mysqldata/data</code> with your data directory of mysql.</li>  <li>Replace <code class=\"language-html highlighter-rouge\">10.10.10.40</code> with your master server IP.</li>  <li>Replace <code class=\"language-html highlighter-rouge\">/mysql-binlog/</code> your master server’s binlog location.</li>  <li><code class=\"language-html highlighter-rouge\">transfer /mysql-binlog/mysql-bin.$x</code> this command will run the transfer.sh alias and upload the binlog file to transfer.sh serve. If you want to use your own copy process then replace <code class=\"language-html highlighter-rouge\">transfer</code> and <code class=\"language-html highlighter-rouge\">/mysql-binlog/</code> location of binlog location. And <code class=\"language-html highlighter-rouge\">$x</code> is the files. So don’t replace that.</li>  <li><code class=\"language-html highlighter-rouge\">/mysqldata/binlogdump</code> location on the DR server to download binlog files</li></ul><h3 id=\"step-5-decode-the-binlog-files\">Step 5: Decode the Binlog files:</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\"><span class=\"nv\">d</span><span class=\"o\">=</span><span class=\"s1\">'@option.datetime@'</span><span class=\"nv\">DATADIR</span><span class=\"o\">=</span><span class=\"s1\">'/mysqldata/data'</span><span class=\"nb\">mkdir</span> <span class=\"nt\">-p</span> /mysqldata/binlogdump<span class=\"nv\">binlogfilename</span><span class=\"o\">=</span><span class=\"sb\">`</span><span class=\"nb\">head</span> <span class=\"nt\">-n</span> 1 <span class=\"nv\">$DATADIR</span>/xtrabackup_binlog_info  | <span class=\"nb\">awk</span> <span class=\"nt\">-F</span> <span class=\"s1\">' '</span> <span class=\"s1\">'{print $1}'</span><span class=\"sb\">`</span><span class=\"nv\">binlogposition</span><span class=\"o\">=</span><span class=\"sb\">`</span><span class=\"nb\">head</span> <span class=\"nt\">-n</span> 1 <span class=\"nv\">$DATADIR</span>/xtrabackup_binlog_info  | <span class=\"nb\">awk</span> <span class=\"nt\">-F</span> <span class=\"s1\">' '</span> <span class=\"s1\">'{print $2}'</span><span class=\"sb\">`</span><span class=\"nv\">files</span><span class=\"o\">=</span><span class=\"sb\">`</span><span class=\"nb\">ls</span> /mysqldata/binlogdump/<span class=\"sb\">`</span><span class=\"nb\">cd</span> /mysqldata/binlogdumpmysqlbinlog <span class=\"nt\">-d</span> Eztaxi <span class=\"nt\">--start-position</span><span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"k\">${</span><span class=\"nv\">binlogposition</span><span class=\"k\">}</span><span class=\"s2\">\"</span> <span class=\"nt\">--stop-datetime</span><span class=\"o\">=</span><span class=\"s2\">\"</span><span class=\"k\">${</span><span class=\"nv\">d</span><span class=\"k\">}</span><span class=\"s2\">\"</span> <span class=\"k\">${</span><span class=\"nv\">files</span><span class=\"k\">}</span> <span class=\"nt\">--disable-log-bin</span> <span class=\"o\">&gt;</span> mysqldata-delta.sql</code></pre></figure><ul>  <li><code class=\"language-html highlighter-rouge\">/mysqldata/data</code> replace with your data directory of MySQL.</li>  <li><code class=\"language-html highlighter-rouge\">/mysqldata/binlogdump</code> Downloaded binlog file location.</li></ul><h3 id=\"step-6-restore-the-binlog-file-to-mysql\">Step 6: Restore the Binlog file to MySQL:</h3><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\">  <span class=\"n\">mysql</span> <span class=\"o\">-</span><span class=\"n\">u</span> <span class=\"n\">root</span> <span class=\"o\">-</span><span class=\"n\">p</span><span class=\"s1\">'password'</span> <span class=\"o\">&lt;</span> <span class=\"o\">/</span><span class=\"n\">mysqldata</span><span class=\"o\">/</span><span class=\"n\">binlogdump</span><span class=\"o\">/</span><span class=\"n\">mysqldata</span><span class=\"o\">-</span><span class=\"n\">delta</span><span class=\"p\">.</span><span class=\"k\">sql</span>  </code></pre></figure><ul>  <li><code class=\"language-html highlighter-rouge\">p'password'</code> Replace with your MySQL root password. You can use Parameterized password in Rundeck. <a href=\"https://thedataguy.in/encrypt-key-files-and-passwords-in-rundeck/\" title=\"See here\">See here</a>.</li>  <li><code class=\"language-html highlighter-rouge\">'/mysqldata/binlogdump/</code> location of the decoded binlog file.</li></ul><p>Steps are done.</p><p>Now on Matched Nodes select the DR server and click on Create button.</p><h2 id=\"trigger-the-pitr\">Trigger the PITR:</h2><p>Click on the Date Picker Icon and select the date and time for your PITR.</p><p><img src=\"/assets/MySQL With DevOps 2 - Simplified MySQl PITR In One Click_date.png\" alt=\"\" /></p><p>Now click on <strong>Run Job Now</strong> button and go for a Cup of Coffee.</p><p>Here is my job execution has been done in 18mins.</p><p><img src=\"/assets/MySQL With DevOps 2 - Simplified MySQl PITR In One Click_output.png\" alt=\"\" /></p><h2 id=\"further-improvements-and-development\">Further improvements and development:</h2><ul>  <li>In my case, I have my DR server ready with MySQL installed. (as an Image). Before trigger this job, I’ll launch a VM with this image and validate the connectivity between Rundeck and GCS bucket. Im planning to use Terraform template which is also a part of this RunDeck job.</li>  <li>My complete setup is in GCP, you can perform the same on AWS, Azure or even On-Prem and comment below how it goes.</li>  <li>I have hardcoded all of mysql data directory, binlog location and everything. If you have enough time, use Options in Rundeck to get these things from an Input during the job execution.</li>  <li>finally, this is also same as RDS PITR, I never gave option for restore binlog till this position. But we can achieve this on <strong>Step 5</strong>, just add a variable called position and give your position number. You can use Options to get this value as an Input. and replace <code class=\"language-html highlighter-rouge\">--stop-datetime=\"${d}\"</code> with <code class=\"language-html highlighter-rouge\">--stop-position=\"${pos}\"</code>.</li></ul><p>If you have any difficulties in understanding the steps and scripts, please comment below.</p><p>Want to learn more basics of RunDeck Setup? <a href=\"https://thedataguy.in/tags/#rundeck-series\">here you go</a>.</p><p>Happy Disaster Recovery and PITR :)</p>",
            "url": "/2019/03/03/mysql-pitr-the-fastest-way-with-devops",
            "image": "/assets/MySQL With DevOps 2 - Simplified MySQl PITR In One Click.png",
            
            
            "tags": ["mysql","automation","rundeck","backup and recovery","pitr","shellscript","GCP"],
            
            "date_published": "2019-03-03T18:30:00+00:00",
            "date_modified": "2019-03-03T18:30:00+00:00",
            
                "author": "Bhuvanesh"
            
        },
    
        {
            "id": "/2019/02/25/mysql-exact-row-count-for-all-the-tables",
            "title": "MySQL Exact Row Count For All The Tables",
            "summary": "Use this stored procedure to get the exact row count of all the tables in mysql. Get the table names using cursor and run select count.",
            "content_text": "Getting the row count from mysql tables are not a big deal and even there is no need for a blog for this. Simply go and query the INFORMATION_SCHEMA and get the row count for the tables. But this is not your actual row counts. It’ll show the row count of the tables during the last statistics update. So if you want to track your tables growth then you should do select count(*) from table_name for all the tables and insert the results to somewhere. There are a lot of ways available. Im just make this as a blog post. So others can benefit from it.Row Count - From Stored Procedure:We’ll get the list of table names from the information_schema and use cursor to run select count(*) on that table and save the row count value to a table.In this example, Im going to collect the row count of the tables from the database called prod_db. And this procedure and tracking table will be saved on the database called dbadmin.use dbadmin;CREATE TABLE table_growth (  id INT (11) NOT NULL AUTO_INCREMENT  ,timestamp DATETIME DEFAULT CURRENT_TIMESTAMP  ,table_name VARCHAR(50) DEFAULT NULL  ,rows INT (11) DEFAULT NULL  ,PRIMARY KEY (id)  );delimiter //  CREATE PROCEDURE rows_growth()  BEGIN  DECLARE start INTEGER DEFAULT 0;  DECLARE t_name varchar(255);  DEClARE table_names CURSOR FOR      \tSELECT table_name FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA='prod_db';  DECLARE CONTINUE HANDLER          FOR NOT FOUND SET start=1;  OPEN table_names;  get_tablename: table_name  FETCH table_names INTO t_name;  SET @query =CONCAT('insert into table_growth (table_name, rows)  select \"',t_name,'\" as tablename ', ',', 'count(*) from prod_db.', t_name);  select @query;  PREPARE insert_stmt FROM @query;  EXECUTE insert_stmt;  IF start = 1 THEN  LEAVE get_tablename;  END IF;  END table_name get_tablename;  CLOSE table_names;  END//  delimiter ;Row Count - From Shell Scriptmysql  -h IP_ADDRESS -usqladmin  -p'password!' Foodie  -N -e \"SELECT table_name FROM INFORMATION_SCHEMA.tables where table_schema='Foodie';\" | while read -r table_name  do  count=$(mysql -h IP_ADDRESS -usqladmin  -p'password' Foodie -N -e \"SELECT COUNT(*) FROM $table_name;\")  mysql  -h IP_ADDRESS -usqladmin  -p'pass!'  dbadmin -e \"INSERT INTO table_growth (table_name, rows)  VALUES ('$table_name',$count);\"  doneIf your tables are having a huge amount of data and running with one or two read replica, then use replica’s IP address for doing the select count(*).",
            "content_html": "<p><img src=\"/uploads/MySQL Exact Row Count For All The Tables.jpg\" alt=\"MySQL Exact Row Count For All The Tables\" title=\"MySQL Exact Row Count For All The Tables\" /></p><p>Getting the row count from mysql tables are not a big deal and even there is no need for a blog for this. Simply go and query the <code class=\"language-html highlighter-rouge\">INFORMATION</code><code class=\"language-html highlighter-rouge\">_SCHEMA</code> <em>and get the row count for the tables. But this is not your actual row counts. It’ll show the row count of the tables during the last statistics update. So if you want to track your tables growth then you should do</em> <code class=\"language-html highlighter-rouge\">select count(*) from table_name</code> for all the tables and insert the results to somewhere. There are a lot of ways available. Im just make this as a blog post. So others can benefit from it.</p><h2 id=\"row-count---from-stored-procedure\">Row Count - From Stored Procedure:</h2><p>We’ll get the list of table names from the <code class=\"language-html highlighter-rouge\">information_schema</code> and use <code class=\"language-html highlighter-rouge\">cursor</code> to run <code class=\"language-html highlighter-rouge\">select count(*)</code> on that table and save the row count value to a table.</p><p>In this example, Im going to collect the row count of the tables from the database called <code class=\"language-html highlighter-rouge\">prod_db</code>. And this procedure and tracking table will be saved on the database called <code class=\"language-html highlighter-rouge\">dbadmin</code>.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\"><span class=\"n\">use</span> <span class=\"n\">dbadmin</span><span class=\"p\">;</span><span class=\"k\">CREATE</span> <span class=\"k\">TABLE</span> <span class=\"n\">table_growth</span> <span class=\"p\">(</span>  <span class=\"n\">id</span> <span class=\"nb\">INT</span> <span class=\"p\">(</span><span class=\"mi\">11</span><span class=\"p\">)</span> <span class=\"k\">NOT</span> <span class=\"k\">NULL</span> <span class=\"n\">AUTO_INCREMENT</span>  <span class=\"p\">,</span><span class=\"nb\">timestamp</span> <span class=\"nb\">DATETIME</span> <span class=\"k\">DEFAULT</span> <span class=\"k\">CURRENT_TIMESTAMP</span>  <span class=\"p\">,</span><span class=\"k\">table_name</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">50</span><span class=\"p\">)</span> <span class=\"k\">DEFAULT</span> <span class=\"k\">NULL</span>  <span class=\"p\">,</span><span class=\"k\">rows</span> <span class=\"nb\">INT</span> <span class=\"p\">(</span><span class=\"mi\">11</span><span class=\"p\">)</span> <span class=\"k\">DEFAULT</span> <span class=\"k\">NULL</span>  <span class=\"p\">,</span><span class=\"k\">PRIMARY</span> <span class=\"k\">KEY</span> <span class=\"p\">(</span><span class=\"n\">id</span><span class=\"p\">)</span>  <span class=\"p\">);</span><span class=\"k\">delimiter</span> <span class=\"o\">//</span>  <span class=\"k\">CREATE</span> <span class=\"k\">PROCEDURE</span> <span class=\"n\">rows_growth</span><span class=\"p\">()</span>  <span class=\"k\">BEGIN</span>  <span class=\"k\">DECLARE</span> <span class=\"k\">start</span> <span class=\"nb\">INTEGER</span> <span class=\"k\">DEFAULT</span> <span class=\"mi\">0</span><span class=\"p\">;</span>  <span class=\"k\">DECLARE</span> <span class=\"n\">t_name</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">255</span><span class=\"p\">);</span>  <span class=\"k\">DEClARE</span> <span class=\"n\">table_names</span> <span class=\"k\">CURSOR</span> <span class=\"k\">FOR</span>      \t<span class=\"k\">SELECT</span> <span class=\"k\">table_name</span> <span class=\"k\">FROM</span> <span class=\"n\">INFORMATION_SCHEMA</span><span class=\"p\">.</span><span class=\"n\">TABLES</span> <span class=\"k\">WHERE</span> <span class=\"n\">TABLE_SCHEMA</span><span class=\"o\">=</span><span class=\"s1\">'prod_db'</span><span class=\"p\">;</span>  <span class=\"k\">DECLARE</span> <span class=\"k\">CONTINUE</span> <span class=\"k\">HANDLER</span>          <span class=\"k\">FOR</span> <span class=\"k\">NOT</span> <span class=\"k\">FOUND</span> <span class=\"k\">SET</span> <span class=\"k\">start</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">;</span>  <span class=\"k\">OPEN</span> <span class=\"n\">table_names</span><span class=\"p\">;</span>  <span class=\"n\">get_tablename</span><span class=\"p\">:</span> <span class=\"k\">table_name</span>  <span class=\"k\">FETCH</span> <span class=\"n\">table_names</span> <span class=\"k\">INTO</span> <span class=\"n\">t_name</span><span class=\"p\">;</span>  <span class=\"k\">SET</span> <span class=\"o\">@</span><span class=\"n\">query</span> <span class=\"o\">=</span><span class=\"n\">CONCAT</span><span class=\"p\">(</span><span class=\"s1\">'insert into table_growth (table_name, rows)  select \"'</span><span class=\"p\">,</span><span class=\"n\">t_name</span><span class=\"p\">,</span><span class=\"s1\">'\" as tablename '</span><span class=\"p\">,</span> <span class=\"s1\">','</span><span class=\"p\">,</span> <span class=\"s1\">'count(*) from prod_db.'</span><span class=\"p\">,</span> <span class=\"n\">t_name</span><span class=\"p\">);</span>  <span class=\"k\">select</span> <span class=\"o\">@</span><span class=\"n\">query</span><span class=\"p\">;</span>  <span class=\"k\">PREPARE</span> <span class=\"n\">insert_stmt</span> <span class=\"k\">FROM</span> <span class=\"o\">@</span><span class=\"n\">query</span><span class=\"p\">;</span>  <span class=\"k\">EXECUTE</span> <span class=\"n\">insert_stmt</span><span class=\"p\">;</span>  <span class=\"n\">IF</span> <span class=\"k\">start</span> <span class=\"o\">=</span> <span class=\"mi\">1</span> <span class=\"k\">THEN</span>  <span class=\"n\">LEAVE</span> <span class=\"n\">get_tablename</span><span class=\"p\">;</span>  <span class=\"k\">END</span> <span class=\"n\">IF</span><span class=\"p\">;</span>  <span class=\"k\">END</span> <span class=\"k\">table_name</span> <span class=\"n\">get_tablename</span><span class=\"p\">;</span>  <span class=\"k\">CLOSE</span> <span class=\"n\">table_names</span><span class=\"p\">;</span>  <span class=\"k\">END</span><span class=\"o\">//</span>  <span class=\"k\">delimiter</span> <span class=\"p\">;</span></code></pre></figure><h2 id=\"row-count---from-shell-script\">Row Count - From Shell Script</h2><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">mysql  <span class=\"nt\">-h</span> IP_ADDRESS <span class=\"nt\">-usqladmin</span>  <span class=\"nt\">-p</span><span class=\"s1\">'password!'</span> Foodie  <span class=\"nt\">-N</span> <span class=\"nt\">-e</span> <span class=\"s2\">\"SELECT table_name FROM INFORMATION_SCHEMA.tables where table_schema='Foodie';\"</span> | <span class=\"k\">while </span><span class=\"nb\">read</span> <span class=\"nt\">-r</span> table_name  <span class=\"k\">do  </span><span class=\"nv\">count</span><span class=\"o\">=</span><span class=\"si\">$(</span>mysql <span class=\"nt\">-h</span> IP_ADDRESS <span class=\"nt\">-usqladmin</span>  <span class=\"nt\">-p</span><span class=\"s1\">'password'</span> Foodie <span class=\"nt\">-N</span> <span class=\"nt\">-e</span> <span class=\"s2\">\"SELECT COUNT(*) FROM </span><span class=\"nv\">$table_name</span><span class=\"s2\">;\"</span><span class=\"si\">)</span>  mysql  <span class=\"nt\">-h</span> IP_ADDRESS <span class=\"nt\">-usqladmin</span>  <span class=\"nt\">-p</span><span class=\"s1\">'pass!'</span>  dbadmin <span class=\"nt\">-e</span> <span class=\"s2\">\"INSERT INTO table_growth (table_name, rows)  VALUES ('</span><span class=\"nv\">$table_name</span><span class=\"s2\">',</span><span class=\"nv\">$count</span><span class=\"s2\">);\"</span>  <span class=\"k\">done</span></code></pre></figure><p>If your tables are having a huge amount of data and running with one or two read replica, then use replica’s IP address for doing the <code class=\"language-html highlighter-rouge\">select count(*)</code>.</p>",
            "url": "/2019/02/25/mysql-exact-row-count-for-all-the-tables",
            
            
            
            "tags": ["mysql"," shellscript"],
            
            "date_published": "2019-02-25T11:35:00+00:00",
            "date_modified": "2019-02-25T11:35:00+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "/2019/02/02/mysql-devops-automate-database-archive",
            "title": "MySQL With DevOps 1 - Automate Database Archive",
            "summary": "Database archive is another challenging job for DBAs. Here Im writting about how I automated database archive with Rundeck and shell script.",
            "content_text": "This is my next blog series. Im going to write about how I automated many complex tasks in MySQL with Rundeck. In my last series, I have explained RunDeck basics. You can find those articles here. In this blog Im writing about how I automated MySQL archive for multiple tables in one Rundeck job.Challeange with Replication:My MySQL database setup has 1 Master 4 Read Replica and the 3’rd replica is an intermediate Master for Replica 4. I don’t want to archive this data on Replica 3 and 4. Because these replicas are using for generating historical reports also some internal application.Disable Log-Bin:To prevent archive data on Replica 3 and 4, I decided to disable binlog on my archive session. But another challenge is, it won’t replicate to Replica 1 and 2. So my final solution is Archive the data on Master, then execute the archive scripts on Replica 1 and Replica 2.Archive Condition:I have 50+ tables which need to be archived older than 30days records. Each table has a timestamp column. But the column name is different on all the tables. For few tables, I need to select older than 30days based on multiple columns.Example:    Delete from table_name where date_col &lt; DATE(Now() - INTERVAL 1 month);    Delete from table_name where date_col &lt; DATE(Now() - INTERVAL 1 month) AND date_col2 &lt; DATE(Now() - INTERVAL 1 month);RunDeck Options:To securely pass MySQL password in shell script we are going to use RunDeck key storage. Read here how to save the password in Rundeck.Process Flow:The job should follow the below steps and whenever a step failed, then stop executing further steps.  Table dump with where clause (we are removing older than 30days data, so dump those 30days data).  Sync the Dump files to cloud storage(here my complete infra in GCP).  Restore the dump on Archive DB Server.  Delete the records on Master.  Delete the records on Replica 1.  Delete the records on Replica 2.Lets create the automation job.Table name and Column names:Create a file /opt/rundeckfiles/archive_tables.txt contains table name and columns. We need to mention the table and use a comma to separate column names. The file structure would be,    table_name1,column1    table_name2,column1,column2    table_name3,column1Stored procedure to delete in chunks:I have written a blog on how to perform archive operation in the right way. You can read it here. So we are going to delete this data with 1000 rows per chunk. Im maintaining DBA related functions and procedures in a separate database called dbadmin##NoteMy database server has only one master db and going to archive this particular database. So in the procedure, I have mentioned my database name. Also, I used SET sql_log_bin =OFF; because I don’t want to replica it to Replica 3 and 4. So if your use case is just archive on all the servers you can remove this line.    DROP PROCEDURE    IF EXISTS archive_tables;    delimiter //      CREATE PROCEDURE        archive_tables(IN delete_query varchar(500))    begin        DECLARE rows INT;        SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;           SET sql_log_bin =OFF;        SET rows = 1;        WHILE rows &gt; 0            do            SET autocommit=1;            SET @query =CONCAT('DELETE FROM DBname.',delete_query,' LIMIT  10000;');            select @query;            PREPARE arcive_stmt FROM @query;            EXECUTE arcive_stmt;            SET rows = row_count();        select sleep(1);        commit;       DEALLOCATE PREPARE arcive_stmt;      END WHILE;     END //    delimiter ; The above procedure will get the where clause from the shell script and prepare the complete delete statement.Grab Table name &amp; Column name from the file:Shell script should get the first value from the file as table and rest of the values are column names in a line. So we need to separate this table and column names with comma.    # $line means read the complete line from the text file    #Get the first value as table name:    table_name=`echo $line | awk -F',' '{print $1}'`    #Generate the where clause for archive:    archive_query=`echo $line |sed 's/,/ where /' |awk -F ',' -v OFS=',' -vsuf=\" &lt;  DATE(Now() - INTERVAL 1 month)\"     '{ for (i=1;i&lt;=NF;++i) $i = pre $i suf; print }'  |sed 's/,/ and /g'`    #Generate the where clause for dump:    dump_command=`echo $line  |awk -F, '{$1=\"\"; print}' |awk -F ' ' -v OFS=',' -vsuf=\" &lt; DATE(Now() - INTERVAL 1 month)\"     '{ for (i=1;i&lt;=NF;++i) $i = pre $i suf; print }'  |sed 's/,/ and /g'`Example:To understand this process in more detail, Im giving an example. For executing archive stored procedure it should generate the delete_query with table and where clause. For dump we just generate the where clause alone.    [root@sqladmin]# line='table_name,col1,col2'    [root@sqladmin]# table_name=`echo $line | awk -F',' '{print $1}'`    [root@sqladmin]# archive_query=`echo $line |sed 's/,/ where /' |awk -F ',' -v OFS=',' -vsuf=\" &lt;  DATE(Now() - INTERVAL 1 month)\"     '{ for (i=1;i&lt;=NF;++i) $i = pre $i suf; print }'  |sed 's/,/ and /g'`    [root@sqladmin]# dump_command=`echo $line  |awk -F, '{$1=\"\"; print}' |awk -F ' ' -v OFS=',' -vsuf=\" &lt; DATE(Now() - INTERVAL 1 month)\"     '{ for (i=1;i&lt;=NF;++i) $i = pre $i suf; print }'  |sed 's/,/ and /g'`See the output:    [root@sqladmin]# echo $archive_query    table_name where col1 &lt; DATE(Now() - INTERVAL 1 month) and col2 &lt; DATE(Now() - INTERVAL 1 month)    [root@sqladmin]# echo $dump_command    col1 &lt; DATE(Now() - INTERVAL 3 month) and col2 &lt; DATE(Now() - INTERVAL 1 month)    [root@sqladmin]# Setup the Rundeck job:  Go to Jobs –&gt; Create Job.  Give a name for this job.  In the Options, add option.  Option Name: sqladmin-secret  Option Label: SQLadmin-Secret  Input Type: Secure –&gt; Select the mysql password from key storage.  Go to Workflow –&gt; Add step.The complete process will be running from Rundeck server itself. It’ll use -hto talk to DB servers.My archive flow is to archive Master first and Replica1, 2. If your use case is just archive it on all the servers, then you must SET sql_log_bin =OFF; in the archive procedure. And in step 3 add the step for Master IP address. Step 4 would be Delete old dump(See Step 6)Step 1: Dump the Data &amp; Upload to GCS Bucket    set -e    set -u    #Password    secret=@option.sqladmin-secret@    #Parameters for Dump Path    DATE=`date +%Y%m%d`    gcs_folder=`date +%Y-%m-%d`    dump_path=/data/my_db/$gcs_folder    mkdir -p /data/my_db/$gcs_folder    while read line     do       table_name=`echo $line | awk -F',' '{print $1}'`      archive_query=`echo $line |sed 's/,/ where /' |awk -F ',' -v OFS=',' -vsuf=\" &lt;  DATE(Now() - INTERVAL 1 month)\"     '{ for (i=1;i&lt;=NF;++i) $i = pre $i suf; print }'  |sed 's/,/ and /g'`      dump_command=`echo $line  |awk -F, '{$1=\"\"; print}' |awk -F ' ' -v OFS=',' -vsuf=\" &lt; DATE(Now() - INTERVAL 1 month)\"     '{ for (i=1;i&lt;=NF;++i) $i = pre $i suf; print }'  |sed 's/,/ and /g'`      mysqldump -h 10.0.0.1 -u admin -p$secret --set-gtid-purged=OFF  --single-transaction my_db -t $table_name --where \"$dump_command\" &gt; $dump_path/$table_name.$DATE.sql      /bin/gsutil cp $dump_path/$table_name.$DATE.sql gs://bucket-name/my_db/$gcs_folder/     done &lt; /opt/rundeckfiles/archive_tables.txt Here my backup files are upload to GCS bucket with current date’s folder. You can use AWS  Cli, if you want to use it in AWS. Change the IP address of MySQL Master or if you want to take dump from slave use Slave IP in -hStep 2: Restore the dump files to Archive DB:Please use a separate db server for archive data. Restore the schema on the db server. Change the IP address of archive DB.    set -e    set -u    #mysql secret    secret=@option.sqladmin-secret@    #restore dump to archive db    gcs_folder=`date +%Y-%m-%d`    dump_path=/data/my_db/$gcs_folder    for i in $dump_path/*.sql    do      echo \"importing $i\"      mysql -h 10.0.0.10 -u admin -p$secret db_name &lt; $i     doneStep 3,4,5 : Archive it on Master/Replica1/Replica2:    set -e    set -u    #password    secret=@option.sqladmin-secret@    #archive master    while read line     do     table_name=`echo $line | awk -F',' '{print $1}'`    echo \"Deleteing $table_name\"    archive_query=`echo $line |sed 's/,/ where /' |awk -F ',' -v OFS=',' -vsuf=\" &lt;  DATE(Now() - INTERVAL 1 month)\"     '{ for (i=1;i&lt;=NF;++i) $i = pre $i suf; print }'  |sed 's/,/ and /g'`    mysql -h 10.0.0.1 -u admin -p$secret dbadmin -e\"CALL archive_tables ('$archive_query');\"     done &lt; /opt/rundeckfiles/archive_tables.txtCopy the above script and use the same for step 4 and 5 but change the IP address of the MySQL server 10.0.0.1.Step 6: delete older than 1day dumps:We don’t want to keep the dump files. So the last step will remove those dump files.    old_folder=`date --date=\"1 day ago\"  +%Y-%m-%d`    dump_path=/data/my_db/$old_folder    #delete yesterday's dump_path    rm -rf $dump_pathAdd tables in future:If you want to add tables in future, just add the table name and archive where clause column name in the txt file.",
            "content_html": "<p><img src=\"/assets/MySQL-With-DevOps-1-Automate-Database-Archive_cover-1024x398.jpg\" alt=\"MySQL With DevOps 1 - Automate Database Archive\" /></p><p>This is my next blog series. Im going to write about how I automated many complex tasks in MySQL with Rundeck. In my last series, I have explained RunDeck basics. You can find those articles <a href=\"https://www.sqlgossip.com/tag/rundeck-series/\">here</a>. In this blog Im writing about how I automated MySQL archive for multiple tables in one Rundeck job.</p><h2 id=\"challeange-with-replication\">Challeange with Replication:</h2><p>My MySQL database setup has 1 Master 4 Read Replica and the 3’rd replica is an intermediate Master for Replica 4. I don’t want to archive this data on Replica 3 and 4. Because these replicas are using for generating historical reports also some internal application.</p><p><img src=\"/assets/MySQL-With-DevOps-1-Automate-Database-Archive-.png\" alt=\"MySQL With DevOps 1 - Automate Database Archive\" /></p><h2 id=\"disable-log-bin\">Disable Log-Bin:</h2><p>To prevent archive data on Replica 3 and 4, I decided to disable binlog on my archive session. But another challenge is, it won’t replicate to Replica 1 and 2. So my final solution is Archive the data on Master, then execute the archive scripts on Replica 1 and Replica 2.</p><h2 id=\"archive-condition\">Archive Condition:</h2><p>I have 50+ tables which need to be archived older than 30days records. Each table has a timestamp column. But the column name is different on all the tables. For few tables, I need to select older than 30days based on multiple columns.</p><h3 id=\"example\">Example:</h3><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\">    <span class=\"k\">Delete</span> <span class=\"k\">from</span> <span class=\"k\">table_name</span> <span class=\"k\">where</span> <span class=\"n\">date_col</span> <span class=\"o\">&lt;</span> <span class=\"nb\">DATE</span><span class=\"p\">(</span><span class=\"n\">Now</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"n\">INTERVAL</span> <span class=\"mi\">1</span> <span class=\"k\">month</span><span class=\"p\">);</span>    <span class=\"k\">Delete</span> <span class=\"k\">from</span> <span class=\"k\">table_name</span> <span class=\"k\">where</span> <span class=\"n\">date_col</span> <span class=\"o\">&lt;</span> <span class=\"nb\">DATE</span><span class=\"p\">(</span><span class=\"n\">Now</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"n\">INTERVAL</span> <span class=\"mi\">1</span> <span class=\"k\">month</span><span class=\"p\">)</span> <span class=\"k\">AND</span> <span class=\"n\">date_col2</span> <span class=\"o\">&lt;</span> <span class=\"nb\">DATE</span><span class=\"p\">(</span><span class=\"n\">Now</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"n\">INTERVAL</span> <span class=\"mi\">1</span> <span class=\"k\">month</span><span class=\"p\">);</span></code></pre></figure><h2 id=\"rundeck-options\">RunDeck Options:</h2><p>To securely pass MySQL password in shell script we are going to use RunDeck key storage. <a href=\"https://www.sqlgossip.com/encrypt-key-files-and-passwords-in-rundeck/\">Read here</a> how to save the password in Rundeck.</p><h2 id=\"process-flow\">Process Flow:</h2><p>The job should follow the below steps and whenever a step failed, then stop executing further steps.</p><ul>  <li>Table dump with where clause (we are removing older than 30days data, so dump those 30days data).</li>  <li>Sync the Dump files to cloud storage(here my complete infra in GCP).</li>  <li>Restore the dump on Archive DB Server.</li>  <li>Delete the records on Master.</li>  <li>Delete the records on Replica 1.</li>  <li>Delete the records on Replica 2.</li></ul><p><img src=\"/assets/MySQL-With-DevOps-1-Automate-Database-Archive_gif.gif\" alt=\"MySQL With DevOps 1 - Automate Database Archive\" /></p><p>Lets create the automation job.</p><h2 id=\"table-name-and-column-names\">Table name and Column names:</h2><p>Create a file <code class=\"language-html highlighter-rouge\">/opt/rundeckfiles/archive_tables.txt</code> contains table name and columns. We need to mention the table and use a comma to separate column names. The file structure would be,</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    table_name1,column1    table_name2,column1,column2    table_name3,column1</code></pre></figure><h2 id=\"stored-procedure-to-delete-in-chunks\">Stored procedure to delete in chunks:</h2><p>I have written a blog on how to perform archive operation in the right way. You can read it <a href=\"https://www.sqlgossip.com/archive-mysql-data-in-chunks/\">here</a>. So we are going to delete this data with 1000 rows per chunk. Im maintaining DBA related functions and procedures in a separate database called <code class=\"language-html highlighter-rouge\">dbadmin</code></p><p>##Note</p><p>My database server has only one master db and going to archive this particular database. So in the procedure, I have mentioned my database name. Also, I used <code class=\"language-html highlighter-rouge\">SET sql_log_bin =OFF;</code> because I don’t want to replica it to Replica 3 and 4. So if your use case is just archive on all the servers you can remove this line.</p><figure class=\"highlight\"><pre><code class=\"language-sql\" data-lang=\"sql\">    <span class=\"k\">DROP</span> <span class=\"k\">PROCEDURE</span>    <span class=\"n\">IF</span> <span class=\"k\">EXISTS</span> <span class=\"n\">archive_tables</span><span class=\"p\">;</span>    <span class=\"k\">delimiter</span> <span class=\"o\">//</span>      <span class=\"k\">CREATE</span> <span class=\"k\">PROCEDURE</span>        <span class=\"n\">archive_tables</span><span class=\"p\">(</span><span class=\"k\">IN</span> <span class=\"n\">delete_query</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">500</span><span class=\"p\">))</span>    <span class=\"k\">begin</span>        <span class=\"k\">DECLARE</span> <span class=\"k\">rows</span> <span class=\"nb\">INT</span><span class=\"p\">;</span>        <span class=\"k\">SET</span> <span class=\"k\">SESSION</span> <span class=\"n\">TRANSACTION</span> <span class=\"k\">ISOLATION</span> <span class=\"k\">LEVEL</span> <span class=\"k\">READ</span> <span class=\"k\">COMMITTED</span><span class=\"p\">;</span>           <span class=\"k\">SET</span> <span class=\"n\">sql_log_bin</span> <span class=\"o\">=</span><span class=\"k\">OFF</span><span class=\"p\">;</span>        <span class=\"k\">SET</span> <span class=\"k\">rows</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">;</span>        <span class=\"n\">WHILE</span> <span class=\"k\">rows</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span>            <span class=\"k\">do</span>            <span class=\"k\">SET</span> <span class=\"n\">autocommit</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">;</span>            <span class=\"k\">SET</span> <span class=\"o\">@</span><span class=\"n\">query</span> <span class=\"o\">=</span><span class=\"n\">CONCAT</span><span class=\"p\">(</span><span class=\"s1\">'DELETE FROM DBname.'</span><span class=\"p\">,</span><span class=\"n\">delete_query</span><span class=\"p\">,</span><span class=\"s1\">' LIMIT  10000;'</span><span class=\"p\">);</span>            <span class=\"k\">select</span> <span class=\"o\">@</span><span class=\"n\">query</span><span class=\"p\">;</span>            <span class=\"k\">PREPARE</span> <span class=\"n\">arcive_stmt</span> <span class=\"k\">FROM</span> <span class=\"o\">@</span><span class=\"n\">query</span><span class=\"p\">;</span>            <span class=\"k\">EXECUTE</span> <span class=\"n\">arcive_stmt</span><span class=\"p\">;</span>            <span class=\"k\">SET</span> <span class=\"k\">rows</span> <span class=\"o\">=</span> <span class=\"k\">row_count</span><span class=\"p\">();</span>        <span class=\"k\">select</span> <span class=\"n\">sleep</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">);</span>        <span class=\"k\">commit</span><span class=\"p\">;</span>       <span class=\"k\">DEALLOCATE</span> <span class=\"k\">PREPARE</span> <span class=\"n\">arcive_stmt</span><span class=\"p\">;</span>      <span class=\"k\">END</span> <span class=\"n\">WHILE</span><span class=\"p\">;</span>     <span class=\"k\">END</span> <span class=\"o\">//</span>    <span class=\"k\">delimiter</span> <span class=\"p\">;</span> </code></pre></figure><p>The above procedure will get the where clause from the shell script and prepare the complete delete statement.</p><h2 id=\"grab-table-name--column-name-from-the-file\">Grab Table name &amp; Column name from the file:</h2><p>Shell script should get the first value from the file as table and rest of the values are column names in a line. So we need to separate this table and column names with comma.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    <span class=\"c\"># $line means read the complete line from the text file</span>    <span class=\"c\">#Get the first value as table name:</span>    <span class=\"nv\">table_name</span><span class=\"o\">=</span><span class=\"sb\">`</span><span class=\"nb\">echo</span> <span class=\"nv\">$line</span> | <span class=\"nb\">awk</span> <span class=\"nt\">-F</span><span class=\"s1\">','</span> <span class=\"s1\">'{print $1}'</span><span class=\"sb\">`</span>    <span class=\"c\">#Generate the where clause for archive:</span>    <span class=\"nv\">archive_query</span><span class=\"o\">=</span><span class=\"sb\">`</span><span class=\"nb\">echo</span> <span class=\"nv\">$line</span> |sed <span class=\"s1\">'s/,/ where /'</span> |awk <span class=\"nt\">-F</span> <span class=\"s1\">','</span> <span class=\"nt\">-v</span> <span class=\"nv\">OFS</span><span class=\"o\">=</span><span class=\"s1\">','</span> <span class=\"nt\">-vsuf</span><span class=\"o\">=</span><span class=\"s2\">\" &lt;  DATE(Now() - INTERVAL 1 month)\"</span>     <span class=\"s1\">'{ for (i=1;i&lt;=NF;++i) $i = pre $i suf; print }'</span>  |sed <span class=\"s1\">'s/,/ and /g'</span><span class=\"sb\">`</span>    <span class=\"c\">#Generate the where clause for dump:</span>    <span class=\"nv\">dump_command</span><span class=\"o\">=</span><span class=\"sb\">`</span><span class=\"nb\">echo</span> <span class=\"nv\">$line</span>  |awk <span class=\"nt\">-F</span>, <span class=\"s1\">'{$1=\"\"; print}'</span> |awk <span class=\"nt\">-F</span> <span class=\"s1\">' '</span> <span class=\"nt\">-v</span> <span class=\"nv\">OFS</span><span class=\"o\">=</span><span class=\"s1\">','</span> <span class=\"nt\">-vsuf</span><span class=\"o\">=</span><span class=\"s2\">\" &lt; DATE(Now() - INTERVAL 1 month)\"</span>     <span class=\"s1\">'{ for (i=1;i&lt;=NF;++i) $i = pre $i suf; print }'</span>  |sed <span class=\"s1\">'s/,/ and /g'</span><span class=\"sb\">`</span></code></pre></figure><h3 id=\"example-1\">Example:</h3><p>To understand this process in more detail, Im giving an example. For executing archive stored procedure it should generate the <code class=\"language-html highlighter-rouge\">delete_query</code> with table and where clause. For dump we just generate the where clause alone.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    <span class=\"o\">[</span>root@sqladmin]# <span class=\"nv\">line</span><span class=\"o\">=</span><span class=\"s1\">'table_name,col1,col2'</span>    <span class=\"o\">[</span>root@sqladmin]# <span class=\"nv\">table_name</span><span class=\"o\">=</span><span class=\"sb\">`</span><span class=\"nb\">echo</span> <span class=\"nv\">$line</span> | <span class=\"nb\">awk</span> <span class=\"nt\">-F</span><span class=\"s1\">','</span> <span class=\"s1\">'{print $1}'</span><span class=\"sb\">`</span>    <span class=\"o\">[</span>root@sqladmin]# <span class=\"nv\">archive_query</span><span class=\"o\">=</span><span class=\"sb\">`</span><span class=\"nb\">echo</span> <span class=\"nv\">$line</span> |sed <span class=\"s1\">'s/,/ where /'</span> |awk <span class=\"nt\">-F</span> <span class=\"s1\">','</span> <span class=\"nt\">-v</span> <span class=\"nv\">OFS</span><span class=\"o\">=</span><span class=\"s1\">','</span> <span class=\"nt\">-vsuf</span><span class=\"o\">=</span><span class=\"s2\">\" &lt;  DATE(Now() - INTERVAL 1 month)\"</span>     <span class=\"s1\">'{ for (i=1;i&lt;=NF;++i) $i = pre $i suf; print }'</span>  |sed <span class=\"s1\">'s/,/ and /g'</span><span class=\"sb\">`</span>    <span class=\"o\">[</span>root@sqladmin]# <span class=\"nv\">dump_command</span><span class=\"o\">=</span><span class=\"sb\">`</span><span class=\"nb\">echo</span> <span class=\"nv\">$line</span>  |awk <span class=\"nt\">-F</span>, <span class=\"s1\">'{$1=\"\"; print}'</span> |awk <span class=\"nt\">-F</span> <span class=\"s1\">' '</span> <span class=\"nt\">-v</span> <span class=\"nv\">OFS</span><span class=\"o\">=</span><span class=\"s1\">','</span> <span class=\"nt\">-vsuf</span><span class=\"o\">=</span><span class=\"s2\">\" &lt; DATE(Now() - INTERVAL 1 month)\"</span>     <span class=\"s1\">'{ for (i=1;i&lt;=NF;++i) $i = pre $i suf; print }'</span>  |sed <span class=\"s1\">'s/,/ and /g'</span><span class=\"sb\">`</span></code></pre></figure><h3 id=\"see-the-output\">See the output:</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    <span class=\"o\">[</span>root@sqladmin]# <span class=\"nb\">echo</span> <span class=\"nv\">$archive_query</span>    table_name where col1 &lt; DATE<span class=\"o\">(</span>Now<span class=\"o\">()</span> - INTERVAL 1 month<span class=\"o\">)</span> and col2 &lt; DATE<span class=\"o\">(</span>Now<span class=\"o\">()</span> - INTERVAL 1 month<span class=\"o\">)</span>    <span class=\"o\">[</span>root@sqladmin]# <span class=\"nb\">echo</span> <span class=\"nv\">$dump_command</span>    col1 &lt; DATE<span class=\"o\">(</span>Now<span class=\"o\">()</span> - INTERVAL 3 month<span class=\"o\">)</span> and col2 &lt; DATE<span class=\"o\">(</span>Now<span class=\"o\">()</span> - INTERVAL 1 month<span class=\"o\">)</span>    <span class=\"o\">[</span>root@sqladmin]# </code></pre></figure><h2 id=\"setup-the-rundeck-job\">Setup the Rundeck job:</h2><ul>  <li>Go to Jobs –&gt; Create Job.</li>  <li>Give a name for this job.</li>  <li>In the Options, add option.</li>  <li>Option Name: sqladmin-secret</li>  <li>Option Label: SQLadmin-Secret</li>  <li>Input Type: Secure –&gt; Select the mysql password from <a href=\"https://www.sqlgossip.com/encrypt-key-files-and-passwords-in-rundeck/\">key storage</a>.</li>  <li>Go to Workflow –&gt; Add step.</li></ul><p>The complete process will be running from Rundeck server itself. It’ll use <code class=\"language-html highlighter-rouge\">-h</code>to talk to DB servers.</p><p>My archive flow is to archive Master first and Replica1, 2. If your use case is just archive it on all the servers, then you must <code class=\"language-html highlighter-rouge\">SET sql_log_bin =OFF;</code> in the archive procedure. And in step 3 add the step for Master IP address. Step 4 would be Delete old dump(See Step 6)</p><h3 id=\"step-1-dump-the-data--upload-to-gcs-bucket\">Step 1: Dump the Data &amp; Upload to GCS Bucket</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    <span class=\"nb\">set</span> <span class=\"nt\">-e</span>    <span class=\"nb\">set</span> <span class=\"nt\">-u</span>    <span class=\"c\">#Password</span>    <span class=\"nv\">secret</span><span class=\"o\">=</span>@option.sqladmin-secret@    <span class=\"c\">#Parameters for Dump Path</span>    <span class=\"nv\">DATE</span><span class=\"o\">=</span><span class=\"sb\">`</span><span class=\"nb\">date</span> +%Y%m%d<span class=\"sb\">`</span>    <span class=\"nv\">gcs_folder</span><span class=\"o\">=</span><span class=\"sb\">`</span><span class=\"nb\">date</span> +%Y-%m-%d<span class=\"sb\">`</span>    <span class=\"nv\">dump_path</span><span class=\"o\">=</span>/data/my_db/<span class=\"nv\">$gcs_folder</span>    <span class=\"nb\">mkdir</span> <span class=\"nt\">-p</span> /data/my_db/<span class=\"nv\">$gcs_folder</span>    <span class=\"k\">while </span><span class=\"nb\">read </span>line     <span class=\"k\">do       </span><span class=\"nv\">table_name</span><span class=\"o\">=</span><span class=\"sb\">`</span><span class=\"nb\">echo</span> <span class=\"nv\">$line</span> | <span class=\"nb\">awk</span> <span class=\"nt\">-F</span><span class=\"s1\">','</span> <span class=\"s1\">'{print $1}'</span><span class=\"sb\">`</span>      <span class=\"nv\">archive_query</span><span class=\"o\">=</span><span class=\"sb\">`</span><span class=\"nb\">echo</span> <span class=\"nv\">$line</span> |sed <span class=\"s1\">'s/,/ where /'</span> |awk <span class=\"nt\">-F</span> <span class=\"s1\">','</span> <span class=\"nt\">-v</span> <span class=\"nv\">OFS</span><span class=\"o\">=</span><span class=\"s1\">','</span> <span class=\"nt\">-vsuf</span><span class=\"o\">=</span><span class=\"s2\">\" &lt;  DATE(Now() - INTERVAL 1 month)\"</span>     <span class=\"s1\">'{ for (i=1;i&lt;=NF;++i) $i = pre $i suf; print }'</span>  |sed <span class=\"s1\">'s/,/ and /g'</span><span class=\"sb\">`</span>      <span class=\"nv\">dump_command</span><span class=\"o\">=</span><span class=\"sb\">`</span><span class=\"nb\">echo</span> <span class=\"nv\">$line</span>  |awk <span class=\"nt\">-F</span>, <span class=\"s1\">'{$1=\"\"; print}'</span> |awk <span class=\"nt\">-F</span> <span class=\"s1\">' '</span> <span class=\"nt\">-v</span> <span class=\"nv\">OFS</span><span class=\"o\">=</span><span class=\"s1\">','</span> <span class=\"nt\">-vsuf</span><span class=\"o\">=</span><span class=\"s2\">\" &lt; DATE(Now() - INTERVAL 1 month)\"</span>     <span class=\"s1\">'{ for (i=1;i&lt;=NF;++i) $i = pre $i suf; print }'</span>  |sed <span class=\"s1\">'s/,/ and /g'</span><span class=\"sb\">`</span>      mysqldump <span class=\"nt\">-h</span> 10.0.0.1 <span class=\"nt\">-u</span> admin <span class=\"nt\">-p</span><span class=\"nv\">$secret</span> <span class=\"nt\">--set-gtid-purged</span><span class=\"o\">=</span>OFF  <span class=\"nt\">--single-transaction</span> my_db <span class=\"nt\">-t</span> <span class=\"nv\">$table_name</span> <span class=\"nt\">--where</span> <span class=\"s2\">\"</span><span class=\"nv\">$dump_command</span><span class=\"s2\">\"</span> <span class=\"o\">&gt;</span> <span class=\"nv\">$dump_path</span>/<span class=\"nv\">$table_name</span>.<span class=\"nv\">$DATE</span>.sql      /bin/gsutil <span class=\"nb\">cp</span> <span class=\"nv\">$dump_path</span>/<span class=\"nv\">$table_name</span>.<span class=\"nv\">$DATE</span>.sql gs://bucket-name/my_db/<span class=\"nv\">$gcs_folder</span>/     <span class=\"k\">done</span> &lt; /opt/rundeckfiles/archive_tables.txt </code></pre></figure><p>Here my backup files are upload to GCS bucket with current date’s folder. You can use AWS  Cli, if you want to use it in AWS. Change the IP address of MySQL Master or if you want to take dump from slave use Slave IP in <code class=\"language-html highlighter-rouge\">-h</code></p><h3 id=\"step-2-restore-the-dump-files-to-archive-db\">Step 2: Restore the dump files to Archive DB:</h3><p>Please use a separate db server for archive data. Restore the schema on the db server. Change the IP address of archive DB.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    <span class=\"nb\">set</span> <span class=\"nt\">-e</span>    <span class=\"nb\">set</span> <span class=\"nt\">-u</span>    <span class=\"c\">#mysql secret</span>    <span class=\"nv\">secret</span><span class=\"o\">=</span>@option.sqladmin-secret@    <span class=\"c\">#restore dump to archive db</span>    <span class=\"nv\">gcs_folder</span><span class=\"o\">=</span><span class=\"sb\">`</span><span class=\"nb\">date</span> +%Y-%m-%d<span class=\"sb\">`</span>    <span class=\"nv\">dump_path</span><span class=\"o\">=</span>/data/my_db/<span class=\"nv\">$gcs_folder</span>    <span class=\"k\">for </span>i <span class=\"k\">in</span> <span class=\"nv\">$dump_path</span>/<span class=\"k\">*</span>.sql    <span class=\"k\">do      </span><span class=\"nb\">echo</span> <span class=\"s2\">\"importing </span><span class=\"nv\">$i</span><span class=\"s2\">\"</span>      mysql <span class=\"nt\">-h</span> 10.0.0.10 <span class=\"nt\">-u</span> admin <span class=\"nt\">-p</span><span class=\"nv\">$secret</span> db_name &lt; <span class=\"nv\">$i</span>     <span class=\"k\">done</span></code></pre></figure><h3 id=\"step-345--archive-it-on-masterreplica1replica2\">Step 3,4,5 : Archive it on Master/Replica1/Replica2:</h3><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    <span class=\"nb\">set</span> <span class=\"nt\">-e</span>    <span class=\"nb\">set</span> <span class=\"nt\">-u</span>    <span class=\"c\">#password</span>    <span class=\"nv\">secret</span><span class=\"o\">=</span>@option.sqladmin-secret@    <span class=\"c\">#archive master</span>    <span class=\"k\">while </span><span class=\"nb\">read </span>line     <span class=\"k\">do     </span><span class=\"nv\">table_name</span><span class=\"o\">=</span><span class=\"sb\">`</span><span class=\"nb\">echo</span> <span class=\"nv\">$line</span> | <span class=\"nb\">awk</span> <span class=\"nt\">-F</span><span class=\"s1\">','</span> <span class=\"s1\">'{print $1}'</span><span class=\"sb\">`</span>    <span class=\"nb\">echo</span> <span class=\"s2\">\"Deleteing </span><span class=\"nv\">$table_name</span><span class=\"s2\">\"</span>    <span class=\"nv\">archive_query</span><span class=\"o\">=</span><span class=\"sb\">`</span><span class=\"nb\">echo</span> <span class=\"nv\">$line</span> |sed <span class=\"s1\">'s/,/ where /'</span> |awk <span class=\"nt\">-F</span> <span class=\"s1\">','</span> <span class=\"nt\">-v</span> <span class=\"nv\">OFS</span><span class=\"o\">=</span><span class=\"s1\">','</span> <span class=\"nt\">-vsuf</span><span class=\"o\">=</span><span class=\"s2\">\" &lt;  DATE(Now() - INTERVAL 1 month)\"</span>     <span class=\"s1\">'{ for (i=1;i&lt;=NF;++i) $i = pre $i suf; print }'</span>  |sed <span class=\"s1\">'s/,/ and /g'</span><span class=\"sb\">`</span>    mysql <span class=\"nt\">-h</span> 10.0.0.1 <span class=\"nt\">-u</span> admin <span class=\"nt\">-p</span><span class=\"nv\">$secret</span> dbadmin <span class=\"nt\">-e</span><span class=\"s2\">\"CALL archive_tables ('</span><span class=\"nv\">$archive_query</span><span class=\"s2\">');\"</span>     <span class=\"k\">done</span> &lt; /opt/rundeckfiles/archive_tables.txt</code></pre></figure><p>Copy the above script and use the same for step 4 and 5 but change the IP address of the MySQL server <code class=\"language-html highlighter-rouge\">10.0.0.1</code>.</p><h3 id=\"step-6-delete-older-than-1day-dumps\">Step 6: delete older than 1day dumps:</h3><p>We don’t want to keep the dump files. So the last step will remove those dump files.</p><figure class=\"highlight\"><pre><code class=\"language-shell\" data-lang=\"shell\">    <span class=\"nv\">old_folder</span><span class=\"o\">=</span><span class=\"sb\">`</span><span class=\"nb\">date</span> <span class=\"nt\">--date</span><span class=\"o\">=</span><span class=\"s2\">\"1 day ago\"</span>  +%Y-%m-%d<span class=\"sb\">`</span>    <span class=\"nv\">dump_path</span><span class=\"o\">=</span>/data/my_db/<span class=\"nv\">$old_folder</span>    <span class=\"c\">#delete yesterday's dump_path</span>    <span class=\"nb\">rm</span> <span class=\"nt\">-rf</span> <span class=\"nv\">$dump_path</span></code></pre></figure><p><img src=\"/assets/MySQL-With-DevOps-1-Automate-Database-Archive_1.png\" alt=\"MySQL With DevOps 1 - Automate Database Archive\" /></p><h2 id=\"add-tables-in-future\">Add tables in future:</h2><p>If you want to add tables in future, just add the table name and archive where clause column name in the txt file.</p>",
            "url": "/2019/02/02/mysql-devops-automate-database-archive",
            
            
            
            "tags": ["archive","automation","devops","gcp","mysql","rundeck","shel"],
            
            "date_published": "2019-02-02T13:45:21+00:00",
            "date_modified": "2019-02-02T13:45:21+00:00",
            
                "author":  {
                "name": "Bhuvanesh",
                "url": null,
                "avatar": null
                }
                
            
        }
    
    ]
}