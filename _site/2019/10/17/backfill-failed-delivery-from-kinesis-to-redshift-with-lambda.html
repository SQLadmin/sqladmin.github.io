<!DOCTYPE html><html lang="en" ><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><link href="/assets/css/syntax.css" rel="stylesheet"><title>BackFill Failed Delivery From Kinesis To RedShift With Lambda | The Data Guy</title><meta name="generator" content="Jekyll v4.0.0" /><meta property="og:title" content="BackFill Failed Delivery From Kinesis To RedShift With Lambda" /><meta name="author" content="Bhuvanesh" /><meta property="og:locale" content="en_US" /><meta name="description" content="Automatically backfill failed delivery from kinesis firehose to Redshift using AWS lambda with boto3 and psycopg2" /><meta property="og:description" content="Automatically backfill failed delivery from kinesis firehose to Redshift using AWS lambda with boto3 and psycopg2" /><meta property="og:site_name" content="The Data Guy" /><meta property="og:image" content="/assets/BackFill%20Failed%20Delivery%20From%20Kinesis%20To%20RedShift%20With%20Lambda.jpg" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2019-10-17T16:26:00+00:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:image" content="/assets/BackFill%20Failed%20Delivery%20From%20Kinesis%20To%20RedShift%20With%20Lambda.jpg" /><meta property="twitter:title" content="BackFill Failed Delivery From Kinesis To RedShift With Lambda" /><meta name="twitter:site" content="@bhuvithedataguy" /><meta name="twitter:creator" content="@https://twitter.com/BhuviTheDataGuy" /> <script type="application/ld+json"> {"dateModified":"2019-10-17T16:26:00+00:00","datePublished":"2019-10-17T16:26:00+00:00","image":"/assets/BackFill%20Failed%20Delivery%20From%20Kinesis%20To%20RedShift%20With%20Lambda.jpg","url":"/2019/10/17/backfill-failed-delivery-from-kinesis-to-redshift-with-lambda","mainEntityOfPage":{"@type":"WebPage","@id":"/2019/10/17/backfill-failed-delivery-from-kinesis-to-redshift-with-lambda"},"@type":"BlogPosting","author":{"@type":"Person","name":"Bhuvanesh"},"description":"Automatically backfill failed delivery from kinesis firehose to Redshift using AWS lambda with boto3 and psycopg2","headline":"BackFill Failed Delivery From Kinesis To RedShift With Lambda","@context":"https://schema.org"}</script><style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.7;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}#share-bar{font-size:20px}#share-bar h4{margin-bottom:10px;font-weight:500}.share-button{margin:0px;margin-bottom:10px;margin-right:3px;border:1px solid #D3D6D2;padding:5px 10px 5px 10px}.share-button:hover{opacity:1;color:#ffffff}.fa-facebook-official{color:#3b5998}.fa-facebook-official:hover{background-color:#3b5998}.fa-twitter{color:#55acee}.fa-twitter:hover{background-color:#55acee}.fa-google-plus{color:#dd4b39}.fa-google-plus:hover{background-color:#dd4b39}.fa-pinterest-p{color:#cb2027}.fa-pinterest-p:hover{background-color:#cb2027}.fa-tumblr{color:#32506d}.fa-tumblr:hover{background-color:#32506d}.fa-reddit-alien{color:#ff4500}.fa-reddit-alien:hover{background-color:#ff4500}.fa-linkedin{color:#007bb5}.fa-linkedin:hover{background-color:#007bb5}.fa-envelope{color:#444444}.fa-envelope:hover{background-color:#444444}</style></head><body><main role="main"><header role="banner"><h1 class="logo">The Data Guy</h1><div class="page-author h-card p-author"><img src="/assets/circle-cropped.png" class="author-avatar u-photo" alt="Bhuvanesh"></div><nav role="navigation"><ul><li><a href="/" >Home</a></li><li><a href="/posts/" >Posts</a></li><li><a href="/categories/" >Categories</a></li><li><a href="/tags/" >Tags</a></li><li><a href="https://medium.com/@bhuvithedataguy" >Medium Blog</a></li><li><a href="/search" >Search Here</a></li></ul></nav></header><section class="post"><h2>BackFill Failed Delivery From Kinesis To RedShift With Lambda</h2><p>If you are dealing with the realtime data stream from Kinesis to RedShift, then you may face this situation where Redshift was down due to some maintenance activity and kinesis firehose was not able to ingest the data. But it has awesome features to retry after the next 60 Minutes. I had a situation that, there was a password change on the RedShift cluster on staging infra and we didn’t notice that the data load was failing. Now we need to backfill all those data into redshift using manifest files. One simple way, list all the files in errors manifest folders and generate the copy command and run those commands in loop or import as a <code class="language-html highlighter-rouge">.sql</code> file. But I wanted to automate this process with lambda.</p><h2 id="why-lambda">Why Lambda?</h2><p>I know it has 15mins of total runtime, but if you have fewer files then its good to use lambda. It’s serverless this is the only reason I choose lambda. Alternatively, you can use shell scripts with <code class="language-html highlighter-rouge">aws cli</code> or any other language to process this.</p><h2 id="solution-overview">Solution Overview:</h2><p>Lambda will put the failed deliveries manifest files in a directory called <code class="language-html highlighter-rouge">errors</code>. The files are proper in a partitioned way <code class="language-html highlighter-rouge">error/manifest/yyyy/mm/dd/hh/</code></p><ol><li>Lambda should read all the manifest files for importing.(no manual process for mention the location every time)</li><li>list-objects-v2 will not return more than 1000 files, so we used <code class="language-html highlighter-rouge">paginate</code> to loop this and get all the objects.</li><li>Redshift’s password is encrypted with KMS.</li><li>Lambda needs <code class="language-html highlighter-rouge">psychopg2</code> to access Redshift, but the official version will not support redshift. We used a custom compiled version of psychopg2.</li><li>Once you imported the data with a manifest file, the next execution should not load the same file again and again. So we are moving the file once it’s imported.</li><li>I’m a great fan of track everything into a metadata table. So every import process will be tracked along with what <code class="language-html highlighter-rouge">COPY</code> command it used.</li></ol><h2 id="lambda-setup">Lambda Setup:</h2><ul><li>If you are thinking to launch lambda outside the VPC, then please don’t consider this blog.</li><li>lambda needs to access KMS to decrypt the password. Its mandatory that the subnets which you are going to use launch the Lambda should have the NAT Gateway.</li><li>Create a KMS key for the region where you are going to create this lambda function.</li><li>Add the following variables in Lambda’s environment variables.</li></ul><table><thead><tr><th>Variable Name</th><th>Value</th></tr></thead><tbody><tr><td>REDSHIFT_DATABASE</td><td>Your database name</td></tr><tr><td>REDSHIFT_TABLE</td><td>Your table name to import the data</td></tr><tr><td>REDSHIFT_USER</td><td>Redshift User Name</td></tr><tr><td>REDSHIFT_PASSWD</td><td>Redshift user’s password</td></tr><tr><td>REDSHIFT_PORT</td><td>Redshift port</td></tr><tr><td>REDSHIFT_ENDPOINT</td><td>Redshift Endpoint</td></tr><tr><td>REDSHIFT_IAMROLE</td><td>IAM role to access S3 inside Redshfit</td></tr><tr><td>SOURCE_BUCKET</td><td>Bucket name where you have the manifest file</td></tr><tr><td>SOURCE_PREFIX</td><td>Location of the error manifest files</td></tr><tr><td>TARGET_PREFIX</td><td>Location where to move the loaded manifest files</td></tr></tbody></table><ul><li>For this blog, I just encrypt the password only. Under encryption, configuration checks the <code class="language-html highlighter-rouge">Enable helpers for encryption in transit</code> and <code class="language-html highlighter-rouge">Use a customer master key</code>. Choose the KMS key which you created for this.</li><li>Then you can see a button called <code class="language-html highlighter-rouge">Encrypt</code> on all the environment variables. Just click encrypt on the password variable.</li><li>Lambda’s IAM role should have the predefined policy <code class="language-html highlighter-rouge">AWSLambdaVPCAccessExecutionRole</code> , <code class="language-html highlighter-rouge">AWSLambdaBasicExecutionRole</code> and one custom policy to access the KMS for decrypting it.</li></ul><figure class="highlight"><pre><code class="language-shell" data-lang="shell">        <span class="o">{</span>
            <span class="s2">"Version"</span>: <span class="s2">"2012-10-17"</span>,
            <span class="s2">"Statement"</span>: <span class="o">[</span>
                <span class="o">{</span>
                    <span class="s2">"Sid"</span>: <span class="s2">"VisualEditor0"</span>,
                    <span class="s2">"Effect"</span>: <span class="s2">"Allow"</span>,
                    <span class="s2">"Action"</span>: <span class="s2">"kms:Decrypt"</span>,
                    <span class="s2">"Resource"</span>: <span class="s2">"&lt;your-kms-arn&gt;"</span>
                <span class="o">}</span>
            <span class="o">]</span>
        <span class="o">}</span></code></pre></figure><ul><li>From the network choose the VPC and subnets(must be attached with NAT/NAT Gateway) and a security group where all traffic allowed to its own ID.</li><li>Make sure the redshift cluster’s security group should accept the connections from the lambda subnet IP range.</li><li><code class="language-html highlighter-rouge">128MB</code> Memory fine for me, but this memory and the timeout can be configured as per your workload.</li></ul><h2 id="code-for-the-function">Code for the Function:</h2><figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">boto3</span>
<span class="kn">import</span> <span class="nn">psycopg2</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">base64</span> <span class="kn">import</span> <span class="n">b64decode</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>

<span class="n">s3</span>  <span class="o">=</span> <span class="n">boto3</span><span class="p">.</span><span class="n">client</span><span class="p">(</span><span class="s">'s3'</span><span class="p">)</span>
<span class="n">kms</span> <span class="o">=</span> <span class="n">boto3</span><span class="p">.</span><span class="n">client</span><span class="p">(</span><span class="s">'kms'</span><span class="p">)</span>


<span class="c1"># Get values from Env
</span><span class="n">REDSHIFT_DATABASE</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'REDSHIFT_DATABASE'</span><span class="p">]</span>
<span class="n">REDSHIFT_TABLE</span>    <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'REDSHIFT_TABLE'</span><span class="p">]</span>
<span class="n">REDSHIFT_USER</span>     <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'REDSHIFT_USER'</span><span class="p">]</span>
<span class="n">REDSHIFT_PASSWD</span>   <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'REDSHIFT_PASSWD'</span><span class="p">]</span>
<span class="n">REDSHIFT_PORT</span>     <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'REDSHIFT_PORT'</span><span class="p">]</span>
<span class="n">REDSHIFT_ENDPOINT</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'REDSHIFT_ENDPOINT'</span><span class="p">]</span>
<span class="n">REDSHIFT_CLUSTER</span>  <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'REDSHIFT_CLUSTER'</span><span class="p">]</span>
<span class="n">REDSHIFT_IAMROLE</span>  <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'REDSHIFT_IAMROLE'</span><span class="p">]</span>
<span class="n">DE_PASS</span> <span class="o">=</span> <span class="n">kms</span><span class="p">.</span><span class="n">decrypt</span><span class="p">(</span><span class="n">CiphertextBlob</span><span class="o">=</span><span class="n">b64decode</span><span class="p">(</span><span class="n">REDSHIFT_PASSWD</span><span class="p">))[</span><span class="s">'Plaintext'</span><span class="p">]</span>

<span class="c1"># Declare other parameters
</span><span class="n">TRIGGER_TIME</span>  <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">datetime</span><span class="p">.</span><span class="n">now</span><span class="p">())</span>
<span class="n">SOURCE_BUCKET</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'SOURCE_BUCKET'</span><span class="p">]</span>
<span class="n">SOURCE_PREFIX</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'SOURCE_PREFIX'</span><span class="p">]</span>
<span class="n">TARGET_PREFIX</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'TARGET_PREFIX'</span><span class="p">]</span>


<span class="c1"># Define the Functions
</span><span class="s">"""
Function 1: Get all manifest files
This fuction is written by alexwlchan
(https://alexwlchan.net/2019/07/listing-s3-keys/)
list_objects_v2  won't support more than 1000 files,
so it'll paginate to next 1000 and so on.

"""</span>
<span class="k">def</span> <span class="nf">get_matching_s3_objects</span><span class="p">(</span><span class="n">bucket</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s">""</span><span class="p">,</span> <span class="n">suffix</span><span class="o">=</span><span class="s">""</span><span class="p">):</span>
    <span class="n">paginator</span> <span class="o">=</span> <span class="n">s3</span><span class="p">.</span><span class="n">get_paginator</span><span class="p">(</span><span class="s">"list_objects_v2"</span><span class="p">)</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s">'Bucket'</span><span class="p">:</span> <span class="n">bucket</span><span class="p">}</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">prefixes</span> <span class="o">=</span> <span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">prefixes</span> <span class="o">=</span> <span class="n">prefix</span>

    <span class="k">for</span> <span class="n">key_prefix</span> <span class="ow">in</span> <span class="n">prefixes</span><span class="p">:</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s">"Prefix"</span><span class="p">]</span> <span class="o">=</span> <span class="n">key_prefix</span>

        <span class="k">for</span> <span class="n">page</span> <span class="ow">in</span> <span class="n">paginator</span><span class="p">.</span><span class="n">paginate</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">contents</span> <span class="o">=</span> <span class="n">page</span><span class="p">[</span><span class="s">"Contents"</span><span class="p">]</span>
            <span class="k">except</span> <span class="nb">KeyError</span><span class="p">:</span>
                <span class="k">return</span>

            <span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">contents</span><span class="p">:</span>
                <span class="n">key</span> <span class="o">=</span> <span class="n">obj</span><span class="p">[</span><span class="s">"Key"</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">key</span><span class="p">.</span><span class="n">endswith</span><span class="p">(</span><span class="n">suffix</span><span class="p">):</span>
                    <span class="k">yield</span> <span class="n">obj</span>


<span class="k">def</span> <span class="nf">get_matching_s3_keys</span><span class="p">(</span><span class="n">bucket</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s">""</span><span class="p">,</span> <span class="n">suffix</span><span class="o">=</span><span class="s">""</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">get_matching_s3_objects</span><span class="p">(</span><span class="n">bucket</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">suffix</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">obj</span><span class="p">[</span><span class="s">"Key"</span><span class="p">]</span>

<span class="s">"""
Function 2: Connection string for RedShift
Its using a custom complied psycopg2
https://github.com/jkehler/awslambda-psycopg2
"""</span>    
     
<span class="k">def</span> <span class="nf">get_pg_con</span><span class="p">(</span>
    <span class="n">user</span><span class="o">=</span><span class="n">REDSHIFT_USER</span><span class="p">,</span>
    <span class="n">password</span><span class="o">=</span><span class="n">DE_PASS</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">"utf-8"</span><span class="p">),</span>
    <span class="n">host</span><span class="o">=</span><span class="n">REDSHIFT_ENDPOINT</span><span class="p">,</span>
    <span class="n">dbname</span><span class="o">=</span><span class="n">REDSHIFT_DATABASE</span><span class="p">,</span>
    <span class="n">port</span><span class="o">=</span><span class="n">REDSHIFT_PORT</span><span class="p">,</span>
    <span class="p">):</span>
    <span class="k">return</span> <span class="n">psycopg2</span><span class="p">.</span><span class="n">connect</span><span class="p">(</span><span class="n">dbname</span><span class="o">=</span><span class="n">dbname</span><span class="p">,</span> 
      <span class="n">host</span><span class="o">=</span><span class="n">host</span><span class="p">,</span> 
      <span class="n">user</span><span class="o">=</span><span class="n">user</span><span class="p">,</span>
      <span class="n">password</span><span class="o">=</span><span class="n">password</span><span class="p">,</span>
      <span class="n">port</span><span class="o">=</span><span class="n">port</span><span class="p">)</span>

<span class="s">"""
Function 3: Main function
"""</span>
<span class="k">def</span> <span class="nf">run_handler</span><span class="p">(</span><span class="n">handler</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
    <span class="n">all_files</span> <span class="o">=</span> <span class="n">get_matching_s3_keys</span><span class="p">(</span><span class="n">SOURCE_BUCKET</span><span class="p">,</span> <span class="n">SOURCE_PREFIX</span><span class="p">)</span>
    <span class="k">for</span> <span class="nb">file</span> <span class="ow">in</span> <span class="n">all_files</span><span class="p">:</span>
      <span class="n">source_file</span> <span class="o">=</span> <span class="p">{</span><span class="s">'Bucket'</span><span class="p">:</span> <span class="n">SOURCE_BUCKET</span><span class="p">,</span><span class="s">'Key'</span><span class="p">:</span> <span class="nb">file</span><span class="p">}</span>
      <span class="n">target_file</span> <span class="o">=</span> <span class="n">TARGET_PREFIX</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span> <span class="o">+</span> <span class="s">".done"</span>
      <span class="k">print</span> <span class="p">(</span><span class="n">SOURCE_BUCKET</span><span class="p">)</span>
      <span class="k">print</span> <span class="p">(</span><span class="n">SOURCE_PREFIX</span><span class="p">)</span>
      <span class="k">print</span> <span class="p">(</span><span class="nb">file</span><span class="p">)</span>
      <span class="c1">#Process starting here
</span>      <span class="n">start_time</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">datetime</span><span class="p">.</span><span class="n">now</span><span class="p">())</span>
      <span class="n">copy_command</span><span class="o">=</span><span class="s">"COPY "</span> <span class="o">+</span> <span class="n">REDSHIFT_TABLE</span> <span class="o">+</span> <span class="s">" FROM 's3://"</span> <span class="o">+</span> <span class="n">SOURCE_BUCKET</span> <span class="o">+</span> <span class="s">"/"</span> <span class="o">+</span> <span class="nb">file</span> <span class="o">+</span> <span class="s">"' iam_role '"</span> <span class="o">+</span> <span class="n">REDSHIFT_IAMROLE</span> <span class="o">+</span> <span class="s">"' MANIFEST json 'auto' GZIP;"</span>
      <span class="n">conn</span> <span class="o">=</span> <span class="n">get_pg_con</span><span class="p">()</span>
      <span class="n">cur</span> <span class="o">=</span> <span class="n">conn</span><span class="p">.</span><span class="n">cursor</span><span class="p">()</span>
      <span class="c1">#print (copy_command) - For debug 
</span>      <span class="n">cur</span><span class="p">.</span><span class="n">execute</span><span class="p">(</span><span class="n">copy_command</span><span class="p">)</span>
      
      <span class="c1">#Insert to History Table
</span>      <span class="n">end_time</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">datetime</span><span class="p">.</span><span class="n">now</span><span class="p">())</span>
      <span class="n">history_insert</span><span class="o">=</span><span class="s">"insert into error_copy_history (TRIGGER_TIME,start_time,end_time,db_name,table_name,file) values ( '"</span> <span class="o">+</span> <span class="n">TRIGGER_TIME</span> <span class="o">+</span> <span class="s">"','"</span>  <span class="o">+</span> <span class="n">start_time</span> <span class="o">+</span> <span class="s">"','"</span>  <span class="o">+</span> <span class="n">REDSHIFT_DATABASE</span> <span class="o">+</span> <span class="s">"','"</span>  <span class="o">+</span> <span class="n">db_name</span> <span class="o">+</span> <span class="s">"','"</span>  <span class="o">+</span> <span class="n">table_name</span> <span class="o">+</span> <span class="s">"','s3://floweraura-rawdata/"</span>  <span class="o">+</span> <span class="nb">file</span> <span class="o">+</span><span class="s">"');"</span>
      <span class="n">cur</span><span class="p">.</span><span class="n">execute</span><span class="p">(</span><span class="n">history_insert</span><span class="p">)</span>
      
      <span class="c1">#Commit and Close
</span>      <span class="n">conn</span><span class="p">.</span><span class="n">commit</span><span class="p">()</span>
      <span class="n">cur</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>
      <span class="n">conn</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>
      
      <span class="c1">#Move the files from Errors directory to processed directory
</span>      <span class="n">s3</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">source_file</span><span class="p">,</span> <span class="n">SOURCE_BUCKET</span><span class="p">,</span> <span class="n">target_file</span><span class="p">)</span>
      <span class="k">print</span> <span class="p">(</span><span class="s">"copied"</span><span class="p">,</span> <span class="nb">file</span><span class="p">)</span>
      <span class="n">s3</span><span class="p">.</span><span class="n">delete_object</span><span class="p">(</span><span class="n">Bucket</span><span class="o">=</span><span class="n">SOURCE_BUCKET</span><span class="p">,</span><span class="n">Key</span><span class="o">=</span><span class="nb">file</span><span class="p">)</span>
      <span class="k">print</span> <span class="p">(</span><span class="s">"deleted"</span><span class="p">,</span> <span class="nb">file</span><span class="p">)</span></code></pre></figure><h2 id="how-to-deploy-it">How to Deploy it?</h2><p>As I mentioned above, you need to use the custom complied psychopg2 which you can download from the below link.</p><p><a href="https://github.com/jkehler/awslambda-psycopg2" title="https://github.com/jkehler/awslambda-psycopg2">https://github.com/jkehler/awslambda-psycopg2</a></p><p>I’m using <code class="language-html highlighter-rouge">Python 3.6</code> on Lambda. So download this repo and rename the <code class="language-html highlighter-rouge">psycopg2-3.6</code> to <code class="language-html highlighter-rouge">psycopg2</code>. And then create a file with name <code class="language-html highlighter-rouge">pgcode.py</code> and paste the above python code.</p><p>Now create a zip file with the <code class="language-html highlighter-rouge">psycopg2</code> and <code class="language-html highlighter-rouge">pgcode.py</code> upload this file to lambda. In the <code class="language-html highlighter-rouge">lambda Handler</code> use <code class="language-html highlighter-rouge">pgcode.run_handler</code></p><p><img src="/assets/BackFill Failed Delivery From Kinesis To RedShift With Lambda2.jpg" alt="" /></p><p>That’s it, your lambda function is ready, not really ready to execute.</p><h2 id="create-the-history-table">Create the History Table:</h2><p>To maintain this import process in a table, we need to create a table in RedShift.</p><figure class="highlight"><pre><code class="language-sql" data-lang="sql">    <span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">error_copy_history</span> 
      <span class="p">(</span> 
         <span class="n">pid</span>          <span class="nb">INT</span> <span class="k">IDENTITY</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> 
         <span class="n">trigger_time</span> <span class="nb">DATETIME</span><span class="p">,</span> 
         <span class="n">start_time</span>   <span class="nb">DATETIME</span><span class="p">,</span> 
         <span class="n">end_time</span>     <span class="nb">DATETIME</span><span class="p">,</span> 
         <span class="n">db_name</span>      <span class="nb">VARCHAR</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> 
         <span class="k">table_name</span>   <span class="nb">VARCHAR</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> 
         <span class="n">FILE</span>         <span class="nb">VARCHAR</span><span class="p">(</span><span class="mi">65000</span><span class="p">)</span> 
      <span class="p">);</span> </code></pre></figure><h2 id="run-the-function">Run the Function:</h2><p>Im giving my s3 path and lambda environment variables here for your reference.</p><ul><li>S3 bucket - <code class="language-html highlighter-rouge">bhuvi-datalake</code> Here Im storing all the kinesis data.</li><li>S3 prefix - <code class="language-html highlighter-rouge">kinesis/errors/</code> Failed manifest files will go to this path (eg: <code class="language-html highlighter-rouge">kinesis/errors/2019/10/21/13/</code></li><li>Target prefix - <code class="language-html highlighter-rouge">kinesis/processed/</code> Once the data imported to Redshift the loaded manifest file will go to this location.</li></ul><p><img src="/assets/BackFill Failed Delivery From Kinesis To RedShift With Lambda1.jpg" alt="" /></p><p>Once the execution was done, you can see the load history from the History table.</p><figure class="highlight"><pre><code class="language-sql" data-lang="sql">    <span class="n">bhuvi</span><span class="o">=#</span> <span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">error_copy_history</span> <span class="k">limit</span> <span class="mi">1</span><span class="p">;</span>
    <span class="n">pid</span>          <span class="o">|</span> <span class="mi">260</span>
    <span class="n">trigger_time</span> <span class="o">|</span> <span class="mi">2019</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="mi">17</span> <span class="mi">08</span><span class="p">:</span><span class="mi">14</span><span class="p">:</span><span class="mi">23</span><span class="p">.</span><span class="mi">495213</span>
    <span class="n">start_time</span>   <span class="o">|</span> <span class="mi">2019</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="mi">17</span> <span class="mi">08</span><span class="p">:</span><span class="mi">14</span><span class="p">:</span><span class="mi">24</span><span class="p">.</span><span class="mi">59309</span>
    <span class="n">end_time</span>     <span class="o">|</span> <span class="mi">2019</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="mi">17</span> <span class="mi">08</span><span class="p">:</span><span class="mi">14</span><span class="p">:</span><span class="mi">24</span><span class="p">.</span><span class="mi">917248</span>
    <span class="n">db_name</span>      <span class="o">|</span> <span class="n">bhuvi</span>
    <span class="k">table_name</span>   <span class="o">|</span> <span class="n">s3cp</span>
    <span class="n">file</span>         <span class="o">|</span> <span class="n">s3</span><span class="p">:</span><span class="o">//</span><span class="n">bhuvi</span><span class="o">-</span><span class="n">datalake</span><span class="o">/</span><span class="n">errors</span><span class="o">/</span><span class="n">manifests</span><span class="o">/</span><span class="mi">2019</span><span class="o">/</span><span class="mi">09</span><span class="o">/</span><span class="mi">26</span><span class="o">/</span><span class="mi">13</span><span class="o">/</span><span class="n">collect</span><span class="o">-</span><span class="n">redshift</span><span class="o">-</span><span class="mi">2019</span><span class="o">-</span><span class="mi">09</span><span class="o">-</span><span class="mi">26</span><span class="o">-</span><span class="mi">13</span><span class="o">-</span><span class="mi">21</span><span class="o">-</span><span class="mi">49</span><span class="o">-</span><span class="mi">56371982</span><span class="o">-</span><span class="mi">2375</span><span class="o">-</span><span class="mi">4</span><span class="n">b28</span><span class="o">-</span><span class="mi">8</span><span class="n">d79</span><span class="o">-</span><span class="mi">45</span><span class="n">f01952667e</span></code></pre></figure><h2 id="further-customization">Further Customization:</h2><ol><li>I ran this on Ad-Hoc basis, but if you want to run this automatically, then use Cloudwatch triggers to trigger this on daily or any N internal.</li><li>I used KMS for encrypting the password, you can use IAM temporary credentials also.</li><li>My S3 data is compressed and JSON format. If you have different file format and compression then modify your COPY command in the python code.</li></ol><h2 id="related-interesting-reading">Related interesting Reading:</h2><ol><li><a href="">Get the email notification when kinesis failed to import the data into RedShift.</a></li><li><a href="https://alexwlchan.net/2019/07/listing-s3-keys/">List keys in S3 more than 1000 with list-objects-v2 include Prefix and Suffix.</a></li><li><a href="https://github.com/jkehler/awslambda-psycopg2">Psycopg2 - custom compiler for Python 2.7, 3.6, 3.7</a></li></ol><span class="meta"><time datetime="2019-10-17T16:26:00+00:00">October 17, 2019</time> &middot; <a href="/tags/#aws">aws</a>, <a href="/tags/#kinesis">kinesis</a>, <a href="/tags/#firehose">firehose</a>, <a href="/tags/#redshift">redshift</a>, <a href="/tags/#lambda">lambda</a>, <a href="/tags/#python">python</a></span><hr> <!--<span class="meta"><time datetime="2019-10-17T16:26:00+00:00">October 17, 2019</time> &middot; <a class="post" href="/tag/aws">aws</a>, <a class="post" href="/tag/kinesis">kinesis</a>, <a class="post" href="/tag/firehose">firehose</a>, <a class="post" href="/tag/redshift">redshift</a>, <a class="post" href="/tag/lambda">lambda</a>, <a class="post" href="/tag/python">python</a></span> --></section></main></body></html>
