<!DOCTYPE html><html lang="en" ><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><link href="/assets/css/syntax.css" rel="stylesheet"><title>RedShift Unload to S3 With Partitions - Stored Procedure Way | The Data Guy</title><meta name="generator" content="Jekyll v4.0.0" /><meta property="og:title" content="RedShift Unload to S3 With Partitions - Stored Procedure Way" /><meta name="author" content="Bhuvanesh" /><meta property="og:locale" content="en_US" /><meta name="description" content="We can use redshift stored procedure to execute unload command and save the data in S3 with partitions." /><meta property="og:description" content="We can use redshift stored procedure to execute unload command and save the data in S3 with partitions." /><meta property="og:site_name" content="The Data Guy" /><meta property="og:image" content="/assets/RedShift%20Unload%20to%20S3%20With%20Partitions%20-%20Stored%20Procedure%20Way.jpg" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2019-08-27T04:17:00+00:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:image" content="/assets/RedShift%20Unload%20to%20S3%20With%20Partitions%20-%20Stored%20Procedure%20Way.jpg" /><meta property="twitter:title" content="RedShift Unload to S3 With Partitions - Stored Procedure Way" /><meta name="twitter:site" content="@bhuvithedataguy" /><meta name="twitter:creator" content="@https://twitter.com/BhuviTheDataGuy" /> <script type="application/ld+json"> {"dateModified":"2019-08-27T04:17:00+00:00","datePublished":"2019-08-27T04:17:00+00:00","image":"/assets/RedShift%20Unload%20to%20S3%20With%20Partitions%20-%20Stored%20Procedure%20Way.jpg","url":"/2019/08/27/redshift-unload-to-s3-with-partitions-stored-procedure-way","mainEntityOfPage":{"@type":"WebPage","@id":"/2019/08/27/redshift-unload-to-s3-with-partitions-stored-procedure-way"},"@type":"BlogPosting","author":{"@type":"Person","name":"Bhuvanesh"},"description":"We can use redshift stored procedure to execute unload command and save the data in S3 with partitions.","headline":"RedShift Unload to S3 With Partitions - Stored Procedure Way","@context":"https://schema.org"}</script><style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui, sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.7;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:0.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:0.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}#share-bar{font-size:20px}#share-bar h4{margin-bottom:10px;font-weight:500}.share-button{margin:0px;margin-bottom:10px;margin-right:3px;border:1px solid #D3D6D2;padding:5px 10px 5px 10px}.share-button:hover{opacity:1;color:#ffffff}.fa-facebook-official{color:#3b5998}.fa-facebook-official:hover{background-color:#3b5998}.fa-twitter{color:#55acee}.fa-twitter:hover{background-color:#55acee}.fa-google-plus{color:#dd4b39}.fa-google-plus:hover{background-color:#dd4b39}.fa-pinterest-p{color:#cb2027}.fa-pinterest-p:hover{background-color:#cb2027}.fa-tumblr{color:#32506d}.fa-tumblr:hover{background-color:#32506d}.fa-reddit-alien{color:#ff4500}.fa-reddit-alien:hover{background-color:#ff4500}.fa-linkedin{color:#007bb5}.fa-linkedin:hover{background-color:#007bb5}.fa-envelope{color:#444444}.fa-envelope:hover{background-color:#444444}</style></head><body><main role="main"><header role="banner"><h1 class="logo">The Data Guy</h1><div class="page-author h-card p-author"><img src="/assets/circle-cropped.png" class="author-avatar u-photo" alt="Bhuvanesh"></div><nav role="navigation"><ul><li><a href="/" >Home</a></li><li><a href="/posts/" >Posts</a></li><li><a href="/categories/" >Categories</a></li><li><a href="/tags/" >Tags</a></li><li><a href="https://medium.com/@bhuvithedataguy" >Medium Blog</a></li><li><a href="/search" >Search Here</a></li></ul></nav></header><section class="post"><h2>RedShift Unload to S3 With Partitions - Stored Procedure Way</h2><p>Redshift unload is the fastest way to export the data from Redshift cluster. In BigData world, generally people use the data in S3 for DataLake. So its important that we need to make sure the data in S3 should be partitioned. So we can use Athena, RedShift Spectrum or EMR External tables to access that data in an optimized way. If you are dealing with multiple tables, then you can loop the table names in a shell script or Python code. But as a SQL guy, I choose stored procedures to do this. It made export process simple.</p><p>In this procedure I just used the options which are suitable for me, but you can use the same procedure and do whatever customization you want. Also you can track the activity of this unload in a metadata table.</p><figure class="highlight"><pre><code class="language-sql" data-lang="sql">    <span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">unload_meta</span> <span class="p">(</span>
    	<span class="n">id</span> <span class="nb">INT</span> <span class="k">IDENTITY</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    	<span class="p">,</span><span class="n">tablename</span> <span class="nb">VARCHAR</span><span class="p">(</span><span class="mi">500</span><span class="p">)</span>
    	<span class="p">,</span><span class="n">start_time</span> <span class="nb">DATETIME</span>
    	<span class="p">,</span><span class="n">end_time</span> <span class="nb">DATETIME</span>
    	<span class="p">,</span><span class="n">export_query</span> <span class="nb">VARCHAR</span><span class="p">(</span><span class="mi">65500</span><span class="p">)</span>
    	<span class="p">);</span></code></pre></figure><p>Define the parameters:</p><ul><li><code class="language-html highlighter-rouge">starttime</code> - timestamp when the process started</li><li><code class="language-html highlighter-rouge">endtime</code> - timestamp when the process end</li><li><code class="language-html highlighter-rouge">sql</code> - SQL Query that you want to export its results to S3.</li><li><code class="language-html highlighter-rouge">s3_path</code> - location of S3 with prefix. Make sure you have end this string with <code class="language-html highlighter-rouge">/</code>.</li><li><code class="language-html highlighter-rouge">iamrole</code> - IAM role ARN which has s3 write access.</li><li><code class="language-html highlighter-rouge">delimiter</code> - If you are exporting as CSV, you can define your delimiter.</li><li><code class="language-html highlighter-rouge">un_year</code> - Partition YEAR</li><li><code class="language-html highlighter-rouge">un_month</code> - Partition MONTH</li><li><code class="language-html highlighter-rouge">un_day</code> - Partition DAY</li></ul><p>In this procedure, I used <code class="language-html highlighter-rouge">GETDATE()</code> function to pass current day, month, year into partition variables. If you want custom one, you can get these variables from input in stored procedure.</p><figure class="highlight"><pre><code class="language-sql" data-lang="sql">    <span class="k">CREATE</span> <span class="k">OR</span> <span class="k">REPLACE</span> <span class="k">PROCEDURE</span> <span class="n">sp_unload</span><span class="p">(</span><span class="n">v_tablename</span> <span class="nb">varchar</span><span class="p">)</span>
    <span class="k">LANGUAGE</span> <span class="n">plpgsql</span> <span class="k">AS</span>
    <span class="err">$$</span>
    <span class="k">DECLARE</span>
       <span class="n">starttime</span> <span class="nb">datetime</span><span class="p">;</span>
       <span class="n">endtime</span> <span class="nb">datetime</span><span class="p">;</span>
       <span class="k">sql</span> <span class="nb">text</span><span class="p">;</span>
       <span class="n">s3_path</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">1000</span><span class="p">);</span>
       <span class="n">iamrole</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">100</span><span class="p">);</span>
       <span class="k">delimiter</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">10</span><span class="p">);</span>
       <span class="n">unload_query</span> <span class="nb">text</span><span class="p">;</span>
       <span class="n">max_filesize</span> <span class="nb">varchar</span><span class="p">(</span><span class="mi">100</span><span class="p">);</span>
       <span class="n">un_year</span> <span class="nb">int</span><span class="p">;</span>
       <span class="n">un_month</span> <span class="nb">int</span><span class="p">;</span>
       <span class="n">un_day</span> <span class="nb">int</span><span class="p">;</span>
    
    <span class="k">BEGIN</span>
       
       <span class="k">select</span> <span class="k">extract</span><span class="p">(</span><span class="nb">year</span> <span class="k">from</span>  <span class="n">GETDATE</span><span class="p">())</span> <span class="k">into</span> <span class="n">un_year</span><span class="p">;</span>
       <span class="k">select</span> <span class="k">extract</span><span class="p">(</span><span class="k">month</span> <span class="k">from</span>  <span class="n">GETDATE</span><span class="p">())</span> <span class="k">into</span> <span class="n">un_month</span><span class="p">;</span>
       <span class="k">select</span> <span class="k">extract</span><span class="p">(</span><span class="k">day</span> <span class="k">from</span>  <span class="n">GETDATE</span><span class="p">())</span> <span class="k">into</span> <span class="n">un_day</span><span class="p">;</span>
       <span class="k">select</span> <span class="n">GETDATE</span><span class="p">()</span> <span class="k">into</span> <span class="n">starttime</span><span class="p">;</span>
       
       <span class="k">sql</span><span class="p">:</span><span class="o">=</span><span class="s1">'select * from '</span><span class="o">||</span><span class="n">v_tablename</span><span class="o">||</span><span class="s1">''</span> <span class="p">;</span>
       <span class="n">s3_path</span><span class="p">:</span><span class="o">=</span><span class="s1">'s3://bhuvi-datalake/clicksteam/'</span><span class="p">;</span>
       <span class="n">iamrole</span><span class="p">:</span><span class="o">=</span><span class="s1">'arn:aws:iam::123123123123:role/myredshiftrole'</span><span class="p">;</span>
       <span class="k">delimiter</span><span class="p">:</span><span class="o">=</span><span class="s1">'|'</span><span class="p">;</span>
       <span class="n">unload_query</span> <span class="p">:</span><span class="o">=</span> <span class="s1">'unload (</span><span class="se">''</span><span class="s1">'</span><span class="o">||</span><span class="k">sql</span><span class="o">||</span><span class="s1">'</span><span class="se">''</span><span class="s1">) to </span><span class="se">''</span><span class="s1">'</span><span class="o">||</span><span class="n">s3_path</span><span class="o">||</span><span class="n">un_year</span><span class="o">||</span><span class="s1">'/'</span><span class="o">||</span><span class="n">un_month</span><span class="o">||</span><span class="s1">'/'</span><span class="o">||</span><span class="n">un_day</span><span class="o">||</span><span class="s1">'/'</span><span class="o">||</span><span class="n">v_tablename</span><span class="o">||</span><span class="s1">'_</span><span class="se">''</span><span class="s1"> iam_role </span><span class="se">''</span><span class="s1">'</span><span class="o">||</span><span class="n">iamrole</span><span class="o">||</span><span class="s1">'</span><span class="se">''</span><span class="s1"> delimiter </span><span class="se">''</span><span class="s1">'</span><span class="o">||</span><span class="k">delimiter</span><span class="o">||</span><span class="s1">'</span><span class="se">''</span><span class="s1"> MAXFILESIZE 100 MB PARALLEL ADDQUOTES HEADER GZIP'</span><span class="p">;</span>
      
       <span class="k">execute</span> <span class="n">unload_query</span><span class="p">;</span>
       <span class="k">select</span> <span class="n">GETDATE</span><span class="p">()</span> <span class="k">into</span> <span class="n">endtime</span><span class="p">;</span>
    
       <span class="k">Insert</span> <span class="k">into</span> <span class="n">unload_meta</span> <span class="p">(</span><span class="n">tablename</span><span class="p">,</span> <span class="n">start_time</span><span class="p">,</span> <span class="n">end_time</span><span class="p">,</span> <span class="n">export_query</span><span class="p">)</span> <span class="k">values</span> <span class="p">(</span><span class="n">v_tablename</span><span class="p">,</span><span class="n">starttime</span><span class="p">,</span><span class="n">endtime</span><span class="p">,</span><span class="n">unload_query</span><span class="p">);</span>
    
    <span class="k">END</span>
    <span class="err">$$</span><span class="p">;</span>  </code></pre></figure><p>Here, Im getting the table name from input. Then I used multiple options like parallel, max file size, include headers and compress. If you don’t want to use this, you can remove these options from the <code class="language-html highlighter-rouge">unload_query</code>. Also if you need you can get the s3 location and other parameters from the user input. You can do many customization here.</p><p>Lets try this.</p><figure class="highlight"><pre><code class="language-shell" data-lang="shell">    <span class="nb">test</span><span class="o">=</span><span class="c"># call sp_unload('bhuvi');</span>
    INFO:  UNLOAD completed, 400000 record<span class="o">(</span>s<span class="o">)</span> unloaded successfully.
    CALL
    <span class="nb">test</span><span class="o">=</span><span class="c">#</span></code></pre></figure><p><img src="/assets/RedShift Unload to S3 With Partitions.png" alt="" /></p><p>Get the unload History from Meta table:</p><div class="language-html highlighter-rouge"><div class="highlight"><pre class="syntax"><code>select * from unload_meta;

id           | 1
tablename    | bhuvi
start_time   | 2019-08-27 03:42:57
end_time     | 2019-08-27 03:43:03
export_query | unload ('select * from bhuvi') to 's3://bhuvi-datalake/clicksteam/2019/8/27/bhuvi_' iam_role 'arn:aws:iam::123123123:role/myredshiftrole' delimiter '|' MAXFILESIZE 100 MB PARALLEL ADDQUOTES HEADER GZIP
</code></pre></div></div><p>In my next blog, I’ll write about how to automate this Unload Process in AWS Glue and convert the CSV to Parquet format.</p><h2 id="update">Update:</h2><p>I have written the updated version of this stored procedure to unload all of the tables in a database to S3. You can read it here: <a href="https://thedataguy.in/redshift-unload-all-tables-to-s3/">https://thedataguy.in/redshift-unload-all-tables-to-s3/</a></p><span class="meta"><time datetime="2019-08-27T04:17:00+00:00">August 27, 2019</time> &middot; <a href="/tags/#aws">aws</a>, <a href="/tags/#redshift">redshift</a>, <a href="/tags/#s3">s3</a>, <a href="/tags/#sql">sql</a></span><hr> <!--<span class="meta"><time datetime="2019-08-27T04:17:00+00:00">August 27, 2019</time> &middot; <a class="post" href="/tag/aws">aws</a>, <a class="post" href="/tag/redshift">redshift</a>, <a class="post" href="/tag/s3">s3</a>, <a class="post" href="/tag/sql">sql</a></span> --></section></main></body></html>
