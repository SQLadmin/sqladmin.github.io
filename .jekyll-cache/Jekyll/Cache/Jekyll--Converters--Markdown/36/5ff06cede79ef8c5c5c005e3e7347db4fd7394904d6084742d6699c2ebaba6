I"ð<<p>AWS SCT Extraction Agents will help to pull the data from the various data sources and push it into the targets. S3 and Snowball also can be a target for SCT Agent. But if we run the SCT Agent with multiple tasks and multiple agents they will export the data into S3 or Snowball with some string folder structure. If you want to push it into the RedShift then it is very difficult to import from the complex random string(UUID kind of) path. Recently we had an experience with this where we have to import around 20TB+ data from Snowball(later exported to S3) to RedShift. In this blog, Im going to share my experience and script to generate RedShift copy command from SCT agent exported to S3 or Snowball with random string folders. 
<img src="/assets/RedShift Copy Command From SCT Agent Export In S3.png" alt="" /></p>
<h2 id="sct-agent-terminology">SCT Agent Terminology:</h2>

<p>Before explaining the solution, letâ€™s understand how the SCT agent works and its terminology.</p>

<ol>
  <li>Once we created a migration task, itâ€™ll split the data export between all the extraction agents.</li>
  <li>One tableâ€™s data will be exported in multiple chunks. This chunk is based on the Virtual Partition.</li>
  <li>The Virtual partition will be considered as Subtasks for the export.</li>
</ol>

<p>So the export folder structure will be formed based on the above terminology.</p>

<ul>
  <li>Top-level folder - The S3 prefix that you are going give while creating the task.</li>
  <li>2nd level - Extraction Agent ID (Each agent has a name as well as a unique ID).</li>
  <li>3rd level - The subtask ID. Each virtual partition will be exported in a subtask.</li>
  <li>4th level - The main task ID. The job you created to export, itâ€™ll get an ID.</li>
</ul>

<p><strong>Lets see an example</strong></p>

<ul>
  <li><strong>S3 Prefix</strong> - april-my-large-table/</li>
  <li><strong>Extraction Agent ID</strong> (you can have multiple agents, but here Iâ€™ll select the agent 1â€™s ID) - e03e92407fcf4ede86e1f4630409c48c</li>
  <li><strong>Subtask ID</strong> - 19f6ce57ec254053ab5eb72245dc2641</li>
  <li><strong>Task ID</strong> - 4c0233f054c840359d93bdae8c175bf8</li>
</ul>

<p>Now if you start exporting the data then itâ€™ll use the following folder structure.</p>

<p><code class="language-html highlighter-rouge">s3://bucketname/s3-prefix/agent-id/subtask-id/task-id/</code></p>

<p>Lets compare this with our example.</p>

<p><code class="language-html highlighter-rouge">s3://migration-bucket/april-my-large-table/e03e92407fcf4ede86e1f4630409c48c/19f6ce57ec254053ab5eb72245dc2641/4c0233f054c840359d93bdae8c175bf8/</code></p>

<h2 id="s3-export-terminology">S3 export Terminology:</h2>

<p>We understand the folder/directory structure, now letâ€™s see what kind of files weâ€™ll get inside these folders.</p>

<ul>
  <li><strong>unit directory</strong> - Your actual data in CSV format will be pushed here. Youâ€™ll get up to 8 files per unit folder. If your data size is huge then youâ€™ll get multiple unit folder like <code class="language-html highlighter-rouge">unit_1, unit_2, unit_n</code>.</li>
  <li>One good thing with this SCT is itâ€™ll generate the COPY command for the RedShift using Manifest file. So inside your unit directory if you have 8 files, then there is a file called <code class="language-html highlighter-rouge">unit.manifest</code> which is a manifest file that contains all the 8 files full path. Along with this, youâ€™ll get the SQL file that already refers to the manifest file.  So using this SQL we can load all the 8 CSV files.</li>
  <li><strong>Statistics file</strong> - This is a JSON file that contains the information about the Taskid, agent ID, source, and target table information and the where condition used to export this data(I virtual partition or chunk). This statistics file will have the name that is the same as your subtask ID.</li>
</ul>

<p>Refer to the below image which shows the complete illustration of the SCT Agent directory structure. And if you notice the subtask ID and the statistic fileâ€™s name, both are the same.</p>

<p><img src="/assets/RedShift Copy Command From SCT Agent Export In S3 snap.jpg" alt="RedShift Copy Command From SCT Agent Export In S3" title="RedShift Copy Command From SCT Agent Export In S3" />
<img src="/assets/RedShift Copy Command From SCT Agent Export In S3 snap1.jpg" alt="RedShift Copy Command From SCT Agent Export In S3" title="RedShift Copy Command From SCT Agent Export In S3" /></p>

<h2 id="challenges">Challenges:</h2>

<p>Here can notice one thing, itâ€™s already in good shape to load the CSV files to RedShift. Just get all the SQL files and run them, then what is the challenge here?</p>
<ul>
  <li>If you extract the SQL file, there we have an option for using Access and Secret keys. But we have to manually fill these values.</li>
  <li>I mean download all the SQL files(all are having the same name, so while downloading rename them). And edit them with the right Access and Secret key values.</li>
  <li>One SQL command refers to one manifest file that means a maximum of 8 files is there. If you have a pretty large cluster that has more than 8 slices then the performance will be slow.</li>
  <li>If you have a Terabyte of data for a single table, youâ€™ll get so many SQL files. Then you have to write a script to loop all these COPY SQL files one by one which may take a longer time.</li>
</ul>

<h2 id="optimized-approach">Optimized Approach:</h2>

<p>We can generate a custom SQL file for COPY command that refers to all the CSV files from the multiple subfolders. So in one SQL command, we can load all of our CSV files to RedShift. So how do we get that?</p>

<ol>
  <li>Get the list of all the <code class="language-html highlighter-rouge">unit.manifest</code> files.</li>
  <li>Download the manifest files to your system.</li>
  <li>Extract the CSV files location from all the manifest files. For this Im going to use <code class="language-html highlighter-rouge">jq</code> a command-line JSON parser for Linux. So just extract the <code class="language-html highlighter-rouge">entries[]</code> array from the <code class="language-html highlighter-rouge">unit.manifest</code> file.</li>
  <li>Create a new manifest file that contains all the CSV file locations that are extracted from the previous step. And upload this to the new S3 path.</li>
  <li>Generate a SQL command that refers to the newly generated manifest.</li>
  <li>If you have many tables instead of repeating this step for all the tables, pass the table names, and the S3 prefix in a CSV file, let the script to the complete work for you.</li>
</ol>

<h2 id="remember-how-did-you-export">Remember how did you export?</h2>

<ol>
  <li>From SCT we can create a local task to export a single table, so the S3 prefix contains the data only for that particular table.</li>
  <li>Or you can upload a CSV file to SCT that contains a bunch of tables list. In this case, your S3 prefix location contains all the tables files. But luckily one subtask will export a tableâ€™s data. So itâ€™ll not mix multiple tables data in one subtask.</li>
</ol>

<p>And make sure you install <code class="language-html highlighter-rouge">jq</code>.</p>

<figure class="highlight"><pre><code class="language-sh" data-lang="sh">apt <span class="nb">install </span>jq
yum <span class="nb">install </span>jq</code></pre></figure>

<p><strong>Replace the following things in my script</strong></p>

<ul>
  <li><strong>migration-bucket</strong> - your bucket name</li>
  <li><strong>your_schema</strong> - RedShift schema name</li>
  <li><strong>1231231231</strong> - AWS Account ID for the RedShift IAM role</li>
  <li><strong>s3-role</strong> - RedShift IAM role name to access S3</li>
  <li><strong>ap-south-1</strong> - s3 bucket region</li>
  <li><strong>merged_manifest</strong> - New s3 prefix where the new Manifest file will get uploaded</li>
</ul>

<h2 id="script-to-generate-sql-for-individually-exported-tables">Script to generate SQL for individually exported tables:</h2>

<p>Prepare a CSV file that contains the list of table names and their s3 Prefix.</p>

<p><strong>Eg Filename: source_generate_copy</strong></p>

<figure class="highlight"><pre><code class="language-sh" data-lang="sh">table1,apr-04-table1
table2,apr-04-table2
table3,apr-04-table3</code></pre></figure>

<p><strong>Create necessary folders</strong></p>

<figure class="highlight"><pre><code class="language-sh" data-lang="sh"><span class="nb">mkdir</span> /root/copy-sql  
<span class="nb">mkdir</span> /root/manifest-list  
<span class="nb">mkdir</span> /root/manifest_files</code></pre></figure>

<p><strong>Download the Manifest</strong></p>

<p>I want to download 20 files parallelly, so I used <code class="language-html highlighter-rouge"><span class="err">&amp;</span></code> and <code class="language-html highlighter-rouge">((++count % 20 == 0 )) <span class="err">&amp;&amp;</span> wait</code>. If you want to download more then use a higher number. But it better to keep less than 100.</p>

<figure class="highlight"><pre><code class="language-sh" data-lang="sh"><span class="nv">count</span><span class="o">=</span>0
<span class="k">while </span><span class="nb">read</span> <span class="nt">-r</span> s_table
<span class="k">do
</span><span class="nv">table</span><span class="o">=</span><span class="si">$(</span><span class="nb">echo</span> <span class="nv">$s_table</span> | <span class="nb">awk</span> <span class="nt">-F</span><span class="s1">','</span> <span class="s1">'{print $1}'</span><span class="si">)</span>
<span class="nv">s3_loc</span><span class="o">=</span><span class="si">$(</span><span class="nb">echo</span> <span class="nv">$s_table</span> | <span class="nb">awk</span> <span class="nt">-F</span><span class="s1">','</span> <span class="s1">'{print $2}'</span><span class="si">)</span>
aws s3 <span class="nb">ls</span> <span class="nt">--human-readable</span> <span class="nt">--recursive</span> s3://migration-bucket/<span class="nv">$s3_loc</span>/ | <span class="nb">grep</span> <span class="s2">"unit.manifest"</span> | <span class="nb">awk</span> <span class="nt">-F</span> <span class="s1">' '</span> <span class="s1">'{print $5}'</span> <span class="o">&gt;</span> /root/manifest-list/<span class="nv">$table</span>.manifest.list      
<span class="nb">mkdir</span> <span class="nt">-p</span> /root/manifest_files/<span class="nv">$table</span>/
<span class="nv">file_num</span><span class="o">=</span>0
<span class="k">while </span><span class="nb">read</span> <span class="nt">-r</span> r_manifest
<span class="k">do
</span>aws s3 <span class="nb">cp </span>s3://migration-bucket/<span class="nv">$r_manifest</span> /root/manifest_files/<span class="nv">$table</span>/unit.manifest.<span class="nv">$file_num</span> &amp;
<span class="nv">file_num</span><span class="o">=</span><span class="k">$((</span> <span class="nv">$file_num</span> <span class="o">+</span> <span class="m">1</span> <span class="k">))</span>
<span class="o">((</span>++count % 20 <span class="o">==</span> 0 <span class="o">))</span> <span class="o">&amp;&amp;</span> <span class="nb">wait
</span><span class="k">done</span> &lt; /root/manifest-list/<span class="nv">$table</span>.manifest.list
<span class="k">done</span> &lt; source_generate_copy</code></pre></figure>

<p><strong>Merge Manifest into one</strong></p>

<figure class="highlight"><pre><code class="language-sh" data-lang="sh"><span class="k">while </span><span class="nb">read</span> <span class="nt">-r</span> s_table
<span class="k">do
</span><span class="nv">table</span><span class="o">=</span><span class="si">$(</span><span class="nb">echo</span> <span class="nv">$s_table</span> | <span class="nb">awk</span> <span class="nt">-F</span><span class="s1">','</span> <span class="s1">'{print $1}'</span><span class="si">)</span>
<span class="nv">files</span><span class="o">=</span><span class="si">$(</span><span class="nb">ls</span> /root/manifest_files/<span class="nv">$table</span><span class="si">)</span>
<span class="k">for </span>file <span class="k">in</span> <span class="nv">$files</span>
<span class="k">do
</span><span class="nb">echo</span> <span class="nv">$file</span>
<span class="nb">cat</span> /root/manifest_files/<span class="nv">$table</span>/<span class="nv">$file</span> | jq <span class="s1">'.entries[]'</span>  <span class="o">&gt;&gt;</span> /root/manifest_files/<span class="nv">$table</span>/unit.merge
<span class="k">done
</span><span class="nb">cat</span> /root/manifest_files/<span class="nv">$table</span>/unit.merge | jq <span class="nt">-s</span> <span class="s1">''</span> <span class="o">&gt;</span> /root/manifest_files/<span class="nv">$table</span>/<span class="nv">$table</span>.manifest
<span class="nb">sed</span> <span class="nt">-i</span> <span class="s1">'1c\{"entries" : ['</span>  /root/manifest_files/<span class="nv">$table</span>/<span class="nv">$table</span>.manifest
<span class="nb">sed</span> <span class="nt">-i</span> <span class="nt">-e</span> <span class="s1">'$a\}'</span>  /root/manifest_files/<span class="nv">$table</span>/<span class="nv">$table</span>.manifest
<span class="k">done</span> &lt; source_generate_copy</code></pre></figure>

<p><strong>Upload Manifest</strong></p>

<figure class="highlight"><pre><code class="language-sh" data-lang="sh"><span class="k">while </span><span class="nb">read</span> <span class="nt">-r</span> s_table
<span class="k">do
</span><span class="nv">table</span><span class="o">=</span><span class="si">$(</span><span class="nb">echo</span> <span class="nv">$s_table</span> | <span class="nb">awk</span> <span class="nt">-F</span><span class="s1">','</span> <span class="s1">'{print $1}'</span><span class="si">)</span>
aws s3 <span class="nb">cp</span> /root/manifest_files/<span class="nv">$table</span>/<span class="nv">$table</span>.manifest s3://migration-bucket/merged_manifest/
<span class="k">done</span> &lt; source_generate_copy</code></pre></figure>

<p><strong>Generate COPY</strong></p>

<p>Change the options as per your need.</p>

<figure class="highlight"><pre><code class="language-sh" data-lang="sh"><span class="k">while </span><span class="nb">read</span> <span class="nt">-r</span> s_table
<span class="k">do
</span><span class="nv">table</span><span class="o">=</span><span class="si">$(</span><span class="nb">echo</span> <span class="nv">$s_table</span> | <span class="nb">awk</span> <span class="nt">-F</span><span class="s1">','</span> <span class="s1">'{print $1}'</span><span class="si">)</span>
<span class="nb">echo</span> <span class="s2">"COPY your_schema.</span><span class="nv">$table</span><span class="s2"> from 's3://migration-bucket/merged_manifest/</span><span class="nv">$table</span><span class="s2">.manifest' MANIFEST iam_role 'arn:aws:iam::1231231231:role/s3-role' REGION 'ap-south-1' REMOVEQUOTES IGNOREHEADER 1 ESCAPE DATEFORMAT 'auto' TIMEFORMAT 'auto' GZIP DELIMITER '|' ACCEPTINVCHARS '?' COMPUPDATE FALSE STATUPDATE FALSE MAXERROR 0 BLANKSASNULL EMPTYASNULL  EXPLICIT_IDS"</span>  <span class="o">&gt;</span> /root/copy-sql/copy-<span class="nv">$table</span>.sql
<span class="k">done</span> &lt; source_generate_copy</code></pre></figure>

<p>All tables COPY command will be generated and located on <code class="language-html highlighter-rouge">/root/copy-sql/</code> location.</p>

<p>I have written about how to generate the SQL command for bulk exported tables in <strong><a href="https://thedataguy.in/redshift-copy-script-from-sct-agent-export-s3-part1/">Part 2</a></strong>.</p>

:ET